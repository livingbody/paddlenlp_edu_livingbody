# åŸºäºé¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram å®ç°è¯­ä¹‰åŒ¹é…

æœ¬æ¡ˆä¾‹ä»‹ç» NLP æœ€åŸºæœ¬çš„ä»»åŠ¡ç±»å‹ä¹‹ä¸€ â€”â€” æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼Œå¹¶ä¸”åŸºäº PaddleNLP ä½¿ç”¨ç™¾åº¦å¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ ERNIE1.0 ä¸ºåŸºç¡€è®­ç»ƒæ•ˆæœä¼˜å¼‚çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹ï¼Œæ¥åˆ¤æ–­ 2 ä¸ªæ–‡æœ¬è¯­ä¹‰æ˜¯å¦ç›¸åŒã€‚

# ä¸€ã€ èƒŒæ™¯ä»‹ç»
æ–‡æœ¬è¯­ä¹‰åŒ¹é…ä»»åŠ¡ï¼Œç®€å•æ¥è¯´å°±æ˜¯ç»™å®šä¸¤æ®µæ–‡æœ¬çš„ç›¸ï¼Œè®©æ¨¡å‹æ¥åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬æ˜¯ä¸æ˜¯è¯­ä¹‰ç›¸ä¼¼ã€‚

åœ¨æœ¬æ¡ˆä¾‹ä¸­ä»¥æƒå¨çš„è¯­ä¹‰åŒ¹é…æ•°æ®é›† [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) ä¸ºä¾‹ï¼Œ[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ•°æ®é›†æ˜¯åŸºäºç™¾åº¦çŸ¥é“ç›¸ä¼¼é—®é¢˜æ¨èæ„é€ çš„é€šé—®å¥è¯­ä¹‰åŒ¹é…æ•°æ®é›†ã€‚è®­ç»ƒé›†ä¸­çš„æ¯ä¸¤æ®µæ–‡æœ¬éƒ½ä¼šè¢«æ ‡è®°ä¸º 1ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰ æˆ–è€… 0ï¼ˆè¯­ä¹‰ä¸ç›¸ä¼¼ï¼‰

ä¾‹å¦‚ç™¾åº¦çŸ¥é“åœºæ™¯ä¸‹ï¼Œç”¨æˆ·æœç´¢ä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ä¼šè®¡ç®—è¿™ä¸ªé—®é¢˜ä¸å€™é€‰é—®é¢˜æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼ï¼Œè¯­ä¹‰åŒ¹é…æ¨¡å‹ä¼šæ‰¾å‡ºä¸é—®é¢˜è¯­ä¹‰ç›¸ä¼¼çš„å€™é€‰é—®é¢˜è¿”å›ç»™ç”¨æˆ·ï¼Œé¿å…ç”¨æˆ·é‡å¤æé—®ã€‚ä¾‹å¦‚ï¼Œå½“æŸç”¨æˆ·åœ¨æœç´¢å¼•æ“ä¸­æœç´¢ â€œæ·±åº¦å­¦ä¹ çš„æ•™ææœ‰å“ªäº›ï¼Ÿâ€ï¼Œæ¨¡å‹å°±è‡ªåŠ¨æ‰¾åˆ°äº†ä¸€äº›è¯­ä¹‰ç›¸ä¼¼çš„é—®é¢˜å±•ç°ç»™ç”¨æˆ·:
![](https://ai-studio-static-online.cdn.bcebos.com/ecc1244685ec4476b869ce8a32d421c0ad530666e98d487da21fa4f61670544f)

# äºŒã€å¿«é€Ÿå®è·µ

ä»‹ç»å¦‚ä½•å‡†å¤‡æ•°æ®ï¼ŒåŸºäº ERNIE-Gram æ¨¡å‹è¿›è¡ŒåŒ¹é…ç½‘ç»œå¤§å®¶ï¼Œç„¶åå¿«é€Ÿè¿›è¡Œè¯­ä¹‰åŒ¹é…æ¨¡å‹çš„è®­ç»ƒã€è¯„ä¼°å’Œé¢„æµ‹ã€‚

## 1.æ•°æ®åŠ è½½
ä¸ºäº†è®­ç»ƒåŒ¹é…æ¨¡å‹ï¼Œä¸€èˆ¬éœ€è¦å‡†å¤‡ä¸‰ä¸ªæ•°æ®é›†ï¼šè®­ç»ƒé›† train.tsvã€éªŒè¯é›†dev.tsvã€æµ‹è¯•é›†test.tsvã€‚æ­¤æ¡ˆä¾‹æˆ‘ä»¬ä½¿ç”¨ PaddleNLP å†…ç½®çš„è¯­ä¹‰æ•°æ®é›† [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ¥è¿›è¡Œè®­ç»ƒã€è¯„ä¼°ã€é¢„æµ‹ã€‚

è®­ç»ƒé›†: ç”¨æ¥è®­ç»ƒæ¨¡å‹å‚æ•°çš„æ•°æ®é›†ï¼Œæ¨¡å‹ç›´æ¥æ ¹æ®è®­ç»ƒé›†æ¥è°ƒæ•´è‡ªèº«å‚æ•°ä»¥è·å¾—æ›´å¥½çš„åˆ†ç±»æ•ˆæœã€‚

éªŒè¯é›†: ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ£€éªŒæ¨¡å‹çš„çŠ¶æ€ï¼Œæ”¶æ•›æƒ…å†µã€‚éªŒè¯é›†é€šå¸¸ç”¨äºè°ƒæ•´è¶…å‚æ•°ï¼Œæ ¹æ®å‡ ç»„æ¨¡å‹éªŒè¯é›†ä¸Šçš„è¡¨ç°å†³å®šå“ªç»„è¶…å‚æ•°æ‹¥æœ‰æœ€å¥½çš„æ€§èƒ½ã€‚

æµ‹è¯•é›†: ç”¨æ¥è®¡ç®—æ¨¡å‹çš„å„é¡¹è¯„ä¼°æŒ‡æ ‡ï¼ŒéªŒè¯æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚

## 2.LCQMC	A Large-scale Chinese Question Matching Corpus è¯­ä¹‰åŒ¹é…æ•°æ®é›†
[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ•°æ®é›†æ˜¯å…¬å¼€çš„è¯­ä¹‰åŒ¹é…æƒå¨æ•°æ®é›†ã€‚PaddleNLP å·²ç»å†…ç½®è¯¥æ•°æ®é›†ï¼Œä¸€é”®å³å¯åŠ è½½ã€‚

## 3.åŒ…å¼•å…¥


```python
# æ­£å¼å¼€å§‹å®éªŒä¹‹å‰é¦–å…ˆé€šè¿‡å¦‚ä¸‹å‘½ä»¤å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ paddlenlp
!python -m pip install --upgrade paddlenlp==2.0.2 -i https://pypi.org/simple
```

    Collecting paddlenlp==2.0.2
    [?25l  Downloading https://files.pythonhosted.org/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 430kB 25kB/s eta 0:00:012
    [?25hRequirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.2) (2.1.1)
    Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.2) (4.1.0)
    Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.2) (0.70.11.1)
    Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.2) (2.9.0)
    Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.2) (0.4.4)
    Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.2) (0.42.1)
    Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.2) (1.2.2)
    Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (3.14.0)
    Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (3.8.2)
    Requirement already satisfied, skipping upgrade: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (1.20.3)
    Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (2.22.0)
    Requirement already satisfied, skipping upgrade: six>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (1.15.0)
    Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (1.1.1)
    Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (1.21.0)
    Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (7.1.2)
    Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (0.8.53)
    Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (0.7.1.1)
    Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.2) (1.0.0)
    Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp==2.0.2) (0.3.3)
    Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.0.2) (0.24.2)
    Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.2) (2.6.0)
    Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.2) (2.2.0)
    Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.2) (0.6.1)
    Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < "3.8" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.2) (0.23)
    Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.2) (3.0.4)
    Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.2) (2019.9.11)
    Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.2) (2.8)
    Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.2) (1.25.6)
    Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.2) (7.0)
    Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.2) (2.10.1)
    Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.2) (0.16.0)
    Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.2) (1.1.0)
    Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.2) (0.10.0)
    Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.2) (5.1.2)
    Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.2) (2.0.1)
    Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.2) (1.3.0)
    Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.2) (1.3.4)
    Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.2) (1.4.10)
    Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.2) (16.7.9)
    Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.2) (3.9.9)
    Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.2) (0.18.0)
    Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.2) (2.8.0)
    Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.2) (2019.3)
    Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.2) (2.1.0)
    Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.2) (1.6.3)
    Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.2) (0.14.1)
    Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < "3.8"->flake8>=3.7.9->visualdl->paddlenlp==2.0.2) (0.6.0)
    Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp==2.0.2) (1.1.1)
    Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < "3.8"->flake8>=3.7.9->visualdl->paddlenlp==2.0.2) (7.2.0)
    Installing collected packages: paddlenlp
      Found existing installation: paddlenlp 2.0.1
        Uninstalling paddlenlp-2.0.1:
          Successfully uninstalled paddlenlp-2.0.1
    Successfully installed paddlenlp-2.0.2



```python
import time
import os
import numpy as np

import paddle
import paddle.nn.functional as F
from paddlenlp.datasets import load_dataset
import paddlenlp

# ä¸€é”®åŠ è½½ Lcqmc çš„è®­ç»ƒé›†ã€éªŒè¯é›†
train_ds, dev_ds = load_dataset("lcqmc", splits=["train", "dev"])
```

## 4.æ•°æ®ä¸‹è½½


```python
# paddlenlp ä¼šè‡ªåŠ¨ä¸‹è½½ lcqmc æ•°æ®é›†è§£å‹åˆ° "${HOME}/.paddlenlp/datasets/LCQMC/lcqmc/lcqmc/" ç›®å½•ä¸‹
! ls ${HOME}/.paddlenlp/datasets/LCQMC/lcqmc/lcqmc
print(paddlenlp.__version__)
```

## 5.æ•°æ®æŸ¥çœ‹


```python
# è¾“å‡ºè®­ç»ƒé›†çš„å‰ 20 æ¡æ ·æœ¬
for idx, example in enumerate(train_ds):
    if idx <= 20:
        print(example)
```

##  6.æ•°æ®é¢„å¤„ç†
é€šè¿‡ paddlenlp åŠ è½½è¿›æ¥çš„ [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ•°æ®é›†æ˜¯åŸå§‹çš„æ˜æ–‡æ•°æ®é›†ï¼Œè¿™éƒ¨åˆ†æˆ‘ä»¬æ¥å®ç°ç»„ batchã€tokenize ç­‰é¢„å¤„ç†é€»è¾‘ï¼Œå°†åŸå§‹æ˜æ–‡æ•°æ®è½¬æ¢æˆç½‘ç»œè®­ç»ƒçš„è¾“å…¥æ•°æ® 

### 6.1å®šä¹‰æ ·æœ¬è½¬æ¢å‡½æ•°


```python
# å› ä¸ºæ˜¯åŸºäºé¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram æ¥è¿›è¡Œï¼Œæ‰€ä»¥éœ€è¦é¦–å…ˆåŠ è½½ ERNIE-Gram çš„ tokenizerï¼Œ
# åç»­æ ·æœ¬è½¬æ¢å‡½æ•°åŸºäº tokenizer å¯¹æ–‡æœ¬è¿›è¡Œåˆ‡åˆ†

tokenizer = paddlenlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')
```


```python
# å°† 1 æ¡æ˜æ–‡æ•°æ®çš„ queryã€title æ‹¼æ¥èµ·æ¥ï¼Œæ ¹æ®é¢„è®­ç»ƒæ¨¡å‹çš„ tokenizer å°†æ˜æ–‡è½¬æ¢ä¸º ID æ•°æ®
# è¿”å› input_ids å’Œ token_type_ids

def convert_example(example, tokenizer, max_seq_length=512, is_test=False):

    query, title = example["query"], example["title"]

    encoded_inputs = tokenizer(
        text=query, text_pair=title, max_seq_len=max_seq_length)

    input_ids = encoded_inputs["input_ids"]
    token_type_ids = encoded_inputs["token_type_ids"]

    if not is_test:
        label = np.array([example["label"]], dtype="int64")
        return input_ids, token_type_ids, label
    # åœ¨é¢„æµ‹æˆ–è€…è¯„ä¼°é˜¶æ®µï¼Œä¸è¿”å› label å­—æ®µ
    else:
        return input_ids, token_type_ids
```


```python
### å¯¹è®­ç»ƒé›†çš„ç¬¬ 1 æ¡æ•°æ®è¿›è¡Œè½¬æ¢
input_ids, token_type_ids, label = convert_example(train_ds[0], tokenizer)
```


```python
print(input_ids)
```


```python
print(token_type_ids)
```


```python
print(label)
```


```python
# ä¸ºäº†åç»­æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬ç»™ convert_example èµ‹äºˆä¸€äº›é»˜è®¤å‚æ•°
from functools import partial

# è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ ·æœ¬è½¬æ¢å‡½æ•°
trans_func = partial(
    convert_example,
    tokenizer=tokenizer,
    max_seq_length=512)
```

### 6.2 ç»„è£… Batch æ•°æ® & Padding

ä¸Šä¸€å°èŠ‚ï¼Œæˆ‘ä»¬å®Œæˆäº†å¯¹å•æ¡æ ·æœ¬çš„è½¬æ¢ï¼Œæœ¬èŠ‚æˆ‘ä»¬éœ€è¦å°†æ ·æœ¬ç»„åˆæˆ Batch æ•°æ®ï¼Œå¯¹äºä¸ç­‰é•¿çš„æ•°æ®è¿˜éœ€è¦è¿›è¡Œ Padding æ“ä½œï¼Œä¾¿äº GPU è®­ç»ƒã€‚

PaddleNLP æä¾›äº†è®¸å¤šå…³äº NLP ä»»åŠ¡ä¸­æ„å»ºæœ‰æ•ˆçš„æ•°æ® pipeline çš„å¸¸ç”¨ API

| API                             | ç®€ä»‹                                       |
| ------------------------------- | :----------------------------------------- |
| `paddlenlp.data.Stack`          | å †å Nä¸ªå…·æœ‰ç›¸åŒshapeçš„è¾“å…¥æ•°æ®æ¥æ„å»ºä¸€ä¸ªbatch |
| `paddlenlp.data.Pad`            | å°†é•¿åº¦ä¸åŒçš„å¤šä¸ªå¥å­paddingåˆ°ç»Ÿä¸€é•¿åº¦ï¼Œå–Nä¸ªè¾“å…¥æ•°æ®ä¸­çš„æœ€å¤§é•¿åº¦ |
| `paddlenlp.data.Tuple`          | å°†å¤šä¸ªbatchifyå‡½æ•°åŒ…è£…åœ¨ä¸€èµ· |

æ›´å¤šæ•°æ®å¤„ç†æ“ä½œè¯¦è§ï¼š https://github.com/PaddlePaddle/models/blob/release/2.0-beta/PaddleNLP/docs/data.md


```python
from paddlenlp.data import Stack, Pad, Tuple
a = [1, 2, 3, 4]
b = [3, 4, 5, 6]
c = [5, 6, 7, 8]
result = Stack()([a, b, c])
print("Stacked Data: \n", result)
print()

a = [1, 2, 3, 4]
b = [5, 6, 7]
c = [8, 9]
result = Pad(pad_val=0)([a, b, c])
print("Padded Data: \n", result)
print()

data = [
        [[1, 2, 3, 4], [1]],
        [[5, 6, 7], [0]],
        [[8, 9], [1]],
       ]
batchify_fn = Tuple(Pad(pad_val=0), Stack())
ids, labels = batchify_fn(data)
print("ids: \n", ids)
print()
print("labels: \n", labels)
print()
```


```python
# æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ä¼šè¿”å› input_ids, token_type_ids, labels 3 ä¸ªå­—æ®µ
# å› æ­¤é’ˆå¯¹è¿™ 3 ä¸ªå­—æ®µéœ€è¦åˆ†åˆ«å®šä¹‰ 3 ä¸ªç»„ batch æ“ä½œ
batchify_fn = lambda samples, fn=Tuple(
    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids
    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids
    Stack(dtype="int64")  # label
): [data for data in fn(samples)]
```

### 6.3å®šä¹‰ Dataloader
ä¸‹é¢æˆ‘ä»¬åŸºäºç»„ batchify_fn å‡½æ•°å’Œæ ·æœ¬è½¬æ¢å‡½æ•° trans_func æ¥æ„é€ è®­ç»ƒé›†çš„ DataLoader, æ”¯æŒå¤šå¡è®­ç»ƒ



```python

# å®šä¹‰åˆ†å¸ƒå¼ Sampler: è‡ªåŠ¨å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ‡åˆ†ï¼Œæ”¯æŒå¤šå¡å¹¶è¡Œè®­ç»ƒ
batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=32, shuffle=True)

# åŸºäº train_ds å®šä¹‰ train_data_loader
# å› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†åˆ†å¸ƒå¼çš„ DistributedBatchSampler, train_data_loader ä¼šè‡ªåŠ¨å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ‡åˆ†
train_data_loader = paddle.io.DataLoader(
        dataset=train_ds.map(trans_func),
        batch_sampler=batch_sampler,
        collate_fn=batchify_fn,
        return_list=True)

# é’ˆå¯¹éªŒè¯é›†æ•°æ®åŠ è½½ï¼Œæˆ‘ä»¬ä½¿ç”¨å•å¡è¿›è¡Œè¯„ä¼°ï¼Œæ‰€ä»¥é‡‡ç”¨ paddle.io.BatchSampler å³å¯
# å®šä¹‰ dev_data_loader
batch_sampler = paddle.io.BatchSampler(dev_ds, batch_size=32, shuffle=False)
dev_data_loader = paddle.io.DataLoader(
        dataset=dev_ds.map(trans_func),
        batch_sampler=batch_sampler,
        collate_fn=batchify_fn,
        return_list=True)
```

## 7.æ¨¡å‹æ­å»º

è‡ªä» 2018 å¹´ 10 æœˆä»¥æ¥ï¼ŒNLP ä¸ªé¢†åŸŸçš„ä»»åŠ¡éƒ½é€šè¿‡ Pretrain + Finetune çš„æ¨¡å¼ç›¸æ¯”ä¼ ç»Ÿ DNN æ–¹æ³•åœ¨æ•ˆæœä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œæœ¬èŠ‚æˆ‘ä»¬ä»¥ç™¾åº¦å¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram ä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨æ­¤ä¹‹ä¸Šæ„å»º Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œã€‚

é¦–å…ˆæˆ‘ä»¬æ¥å®šä¹‰ç½‘ç»œç»“æ„:


```python
import paddle.nn as nn

# æˆ‘ä»¬åŸºäº ERNIE-Gram æ¨¡å‹ç»“æ„æ­å»º Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œ
# æ‰€ä»¥æ­¤å¤„å…ˆå®šä¹‰ ERNIE-Gram çš„ pretrained_model
pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')
#pretrained_model = paddlenlp.transformers.ErnieModel.from_pretrained('ernie-1.0')


class PointwiseMatching(nn.Layer):
   
    # æ­¤å¤„çš„ pretained_model åœ¨æœ¬ä¾‹ä¸­ä¼šè¢« ERNIE-Gram é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–
    def __init__(self, pretrained_model, dropout=None):
        super().__init__()
        self.ptm = pretrained_model
        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)

        # è¯­ä¹‰åŒ¹é…ä»»åŠ¡: ç›¸ä¼¼ã€ä¸ç›¸ä¼¼ 2 åˆ†ç±»ä»»åŠ¡
        self.classifier = nn.Linear(self.ptm.config["hidden_size"], 2)

    def forward(self,
                input_ids,
                token_type_ids=None,
                position_ids=None,
                attention_mask=None):

        # æ­¤å¤„çš„ Input_ids ç”±ä¸¤æ¡æ–‡æœ¬çš„ token ids æ‹¼æ¥è€Œæˆ
        # token_type_ids è¡¨ç¤ºä¸¤æ®µæ–‡æœ¬çš„ç±»å‹ç¼–ç 
        # è¿”å›çš„ cls_embedding å°±è¡¨ç¤ºè¿™ä¸¤æ®µæ–‡æœ¬ç»è¿‡æ¨¡å‹çš„è®¡ç®—ä¹‹åè€Œå¾—åˆ°çš„è¯­ä¹‰è¡¨ç¤ºå‘é‡
        _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids,
                                    attention_mask)

        cls_embedding = self.dropout(cls_embedding)

        # åŸºäºæ–‡æœ¬å¯¹çš„è¯­ä¹‰è¡¨ç¤ºå‘é‡è¿›è¡Œ 2 åˆ†ç±»ä»»åŠ¡
        logits = self.classifier(cls_embedding)
        probs = F.softmax(logits)

        return probs

# å®šä¹‰ Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œ
model = PointwiseMatching(pretrained_model)
```

## 8. æ¨¡å‹è®­ç»ƒ & è¯„ä¼°


```python
from paddlenlp.transformers import LinearDecayWithWarmup

epochs = 3
num_training_steps = len(train_data_loader) * epochs

# å®šä¹‰ learning_rate_schedulerï¼Œè´Ÿè´£åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ lr è¿›è¡Œè°ƒåº¦
lr_scheduler = LinearDecayWithWarmup(5E-5, num_training_steps, 0.0)

# Generate parameter names needed to perform weight decay.
# All bias and LayerNorm parameters are excluded.
decay_params = [
    p.name for n, p in model.named_parameters()
    if not any(nd in n for nd in ["bias", "norm"])
]

# å®šä¹‰ Optimizer
optimizer = paddle.optimizer.AdamW(
    learning_rate=lr_scheduler,
    parameters=model.parameters(),
    weight_decay=0.0,
    apply_decay_param_fun=lambda x: x in decay_params)

# é‡‡ç”¨äº¤å‰ç†µ æŸå¤±å‡½æ•°
criterion = paddle.nn.loss.CrossEntropyLoss()

# è¯„ä¼°çš„æ—¶å€™é‡‡ç”¨å‡†ç¡®ç‡æŒ‡æ ‡
metric = paddle.metric.Accuracy()
```


```python
# å› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶è¦åœ¨éªŒè¯é›†è¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼Œå› æ­¤æˆ‘ä»¬å…ˆå®šä¹‰è¯„ä¼°å‡½æ•°

@paddle.no_grad()
def evaluate(model, criterion, metric, data_loader, phase="dev"):
    model.eval()
    metric.reset()
    losses = []
    for batch in data_loader:
        input_ids, token_type_ids, labels = batch
        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)
        loss = criterion(probs, labels)
        losses.append(loss.numpy())
        correct = metric.compute(probs, labels)
        metric.update(correct)
        accu = metric.accumulate()
    print("eval {} loss: {:.5}, accu: {:.5}".format(phase,
                                                    np.mean(losses), accu))
    model.train()
    metric.reset()
```


```python
# æ¥ä¸‹æ¥ï¼Œå¼€å§‹æ­£å¼è®­ç»ƒæ¨¡å‹

global_step = 0
tic_train = time.time()

for epoch in range(1, epochs + 1):
    for step, batch in enumerate(train_data_loader, start=1):

        input_ids, token_type_ids, labels = batch
        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)
        loss = criterion(probs, labels)
        correct = metric.compute(probs, labels)
        metric.update(correct)
        acc = metric.accumulate()

        global_step += 1
        
        # æ¯é—´éš” 10 step è¾“å‡ºè®­ç»ƒæŒ‡æ ‡
        if global_step % 10 == 0:
            print(
                "global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s"
                % (global_step, epoch, step, loss, acc,
                    10 / (time.time() - tic_train)))
            tic_train = time.time()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.clear_grad()

        # æ¯é—´éš” 100 step åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°
        if global_step % 100 == 0:
            evaluate(model, criterion, metric, dev_data_loader, "dev")
            
# è®­ç»ƒç»“æŸåï¼Œå­˜å‚¨æ¨¡å‹å‚æ•°
save_dir = os.path.join("checkpoint", "model_%d" % global_step)
os.makedirs(save_dir)

save_param_path = os.path.join(save_dir, 'model_state.pdparams')
paddle.save(model.state_dict(), save_param_path)
tokenizer.save_pretrained(save_dir)
```

æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºå¦‚ä¸‹æ—¥å¿—:
```
global step 5310, epoch: 3, batch: 1578, loss: 0.31671, accu: 0.95000, speed: 0.63 step/s
global step 5320, epoch: 3, batch: 1588, loss: 0.36240, accu: 0.94063, speed: 6.98 step/s
global step 5330, epoch: 3, batch: 1598, loss: 0.41451, accu: 0.93854, speed: 7.40 step/s
global step 5340, epoch: 3, batch: 1608, loss: 0.31327, accu: 0.94063, speed: 7.01 step/s
global step 5350, epoch: 3, batch: 1618, loss: 0.40664, accu: 0.93563, speed: 7.83 step/s
global step 5360, epoch: 3, batch: 1628, loss: 0.33064, accu: 0.93958, speed: 7.34 step/s
global step 5370, epoch: 3, batch: 1638, loss: 0.38411, accu: 0.93795, speed: 7.72 step/s
global step 5380, epoch: 3, batch: 1648, loss: 0.35376, accu: 0.93906, speed: 7.92 step/s
global step 5390, epoch: 3, batch: 1658, loss: 0.39706, accu: 0.93924, speed: 7.47 step/s
global step 5400, epoch: 3, batch: 1668, loss: 0.41198, accu: 0.93781, speed: 7.41 step/s
eval dev loss: 0.4177, accu: 0.89082
global step 5410, epoch: 3, batch: 1678, loss: 0.34453, accu: 0.93125, speed: 0.63 step/s
global step 5420, epoch: 3, batch: 1688, loss: 0.34569, accu: 0.93906, speed: 7.75 step/s
global step 5430, epoch: 3, batch: 1698, loss: 0.39160, accu: 0.92917, speed: 7.54 step/s
global step 5440, epoch: 3, batch: 1708, loss: 0.46002, accu: 0.93125, speed: 7.05 step/s
global step 5450, epoch: 3, batch: 1718, loss: 0.32302, accu: 0.93188, speed: 7.14 step/s
global step 5460, epoch: 3, batch: 1728, loss: 0.40802, accu: 0.93281, speed: 7.22 step/s
global step 5470, epoch: 3, batch: 1738, loss: 0.34607, accu: 0.93348, speed: 7.44 step/s
global step 5480, epoch: 3, batch: 1748, loss: 0.34709, accu: 0.93398, speed: 7.38 step/s
global step 5490, epoch: 3, batch: 1758, loss: 0.31814, accu: 0.93437, speed: 7.39 step/s
global step 5500, epoch: 3, batch: 1768, loss: 0.42689, accu: 0.93125, speed: 7.74 step/s
eval dev loss: 0.41789, accu: 0.88968
```

åŸºäºé»˜è®¤å‚æ•°é…ç½®è¿›è¡Œå•å¡è®­ç»ƒå¤§æ¦‚è¦æŒç»­ 4 ä¸ªå°æ—¶å·¦å³ï¼Œä¼šè®­ç»ƒå®Œæˆ 3 ä¸ª Epoch, æ¨¡å‹æœ€ç»ˆçš„æ”¶æ•›æŒ‡æ ‡ç»“æœå¦‚ä¸‹:


| æ•°æ®é›† | Accuracy |
| -------- | -------- |
| dev.tsv     | 89.62  |

å¯ä»¥çœ‹åˆ°: æˆ‘ä»¬åŸºäº PaddleNLP ï¼Œåˆ©ç”¨ ERNIE-Gram é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨éå¸¸ç®€æ´çš„ä»£ç ï¼Œå°±åœ¨æƒå¨è¯­ä¹‰åŒ¹é…æ•°æ®é›†ä¸Šå–å¾—äº†å¾ˆä¸é”™çš„æ•ˆæœ.

## 9.æ¨¡å‹é¢„æµ‹

æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹å¯¹ä¸€äº›é¢„æµ‹æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚å¾…é¢„æµ‹æ•°æ®ä¸ºæ¯è¡Œéƒ½æ˜¯æ–‡æœ¬å¯¹çš„ tsv æ–‡ä»¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ Lcqmc æ•°æ®é›†çš„æµ‹è¯•é›†ä½œä¸ºæˆ‘ä»¬çš„é¢„æµ‹æ•°æ®ï¼Œè¿›è¡Œé¢„æµ‹å¹¶æäº¤é¢„æµ‹ç»“æœåˆ° [åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›](https://aistudio.baidu.com/aistudio/competition/detail/45)

ä¸‹è½½æˆ‘ä»¬å·²ç»è®­ç»ƒå¥½çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹, å¹¶è§£å‹


```python
# ä¸‹è½½æˆ‘ä»¬åŸºäº Lcqmc äº‹å…ˆè®­ç»ƒå¥½çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹å¹¶è§£å‹
! wget https://paddlenlp.bj.bcebos.com/models/text_matching/ernie_gram_zh_pointwise_matching_model.tar
! tar -xvf ernie_gram_zh_pointwise_matching_model.tar
```


```python
# æµ‹è¯•æ•°æ®ç”± 2 åˆ—æ–‡æœ¬æ„æˆ tab åˆ†éš”
# Lcqmc é»˜è®¤ä¸‹è½½åˆ°å¦‚ä¸‹è·¯å¾„
! head -n10 "${HOME}/.paddlenlp/datasets/LCQMC/lcqmc/lcqmc/test.tsv"
```

### 9.1å®šä¹‰é¢„æµ‹å‡½æ•°


```python

def predict(model, data_loader):
    
    batch_probs = []

    # é¢„æµ‹é˜¶æ®µæ‰“å¼€ eval æ¨¡å¼ï¼Œæ¨¡å‹ä¸­çš„ dropout ç­‰æ“ä½œä¼šå…³æ‰
    model.eval()

    with paddle.no_grad():
        for batch_data in data_loader:
            input_ids, token_type_ids = batch_data
            input_ids = paddle.to_tensor(input_ids)
            token_type_ids = paddle.to_tensor(token_type_ids)
            
            # è·å–æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡: [batch_size, 2] çš„çŸ©é˜µ
            batch_prob = model(
                input_ids=input_ids, token_type_ids=token_type_ids).numpy()

            batch_probs.append(batch_prob)
        batch_probs = np.concatenate(batch_probs, axis=0)

        return batch_probs
```

### 9.2 å®šä¹‰é¢„æµ‹æ•°æ®çš„ data_loader


```python
# é¢„æµ‹æ•°æ®çš„è½¬æ¢å‡½æ•°
# predict æ•°æ®æ²¡æœ‰ label, å› æ­¤ convert_exmaple çš„ is_test å‚æ•°è®¾ä¸º True
trans_func = partial(
    convert_example,
    tokenizer=tokenizer,
    max_seq_length=512,
    is_test=True)

# é¢„æµ‹æ•°æ®çš„ç»„ batch æ“ä½œ
# predict æ•°æ®åªè¿”å› input_ids å’Œ token_type_idsï¼Œå› æ­¤åªéœ€è¦ 2 ä¸ª Pad å¯¹è±¡ä½œä¸º batchify_fn
batchify_fn = lambda samples, fn=Tuple(
    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids
    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment_ids
): [data for data in fn(samples)]

# åŠ è½½é¢„æµ‹æ•°æ®
test_ds = load_dataset("lcqmc", splits=["test"])
```


```python
batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=32, shuffle=False)

# ç”Ÿæˆé¢„æµ‹æ•°æ® data_loader
predict_data_loader =paddle.io.DataLoader(
        dataset=test_ds.map(trans_func),
        batch_sampler=batch_sampler,
        collate_fn=batchify_fn,
        return_list=True)
```

### 9.3 å®šä¹‰é¢„æµ‹æ¨¡å‹


```python
pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')

model = PointwiseMatching(pretrained_model)
```

### 9.4 åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°


```python
# åˆšæ‰ä¸‹è½½çš„æ¨¡å‹è§£å‹ä¹‹åå­˜å‚¨è·¯å¾„ä¸º ./ernie_gram_zh_pointwise_matching_model/model_state.pdparams
state_dict = paddle.load("./ernie_gram_zh_pointwise_matching_model/model_state.pdparams")

# åˆšæ‰ä¸‹è½½çš„æ¨¡å‹è§£å‹ä¹‹åå­˜å‚¨è·¯å¾„ä¸º ./pointwise_matching_model/ernie1.0_base_pointwise_matching.pdparams
# state_dict = paddle.load("pointwise_matching_model/ernie1.0_base_pointwise_matching.pdparams")
model.set_dict(state_dict)
```

### 9.5 å¼€å§‹é¢„æµ‹


```python
for idx, batch in enumerate(predict_data_loader):
    if idx < 1:
        print(batch)
```


```python
# æ‰§è¡Œé¢„æµ‹å‡½æ•°
y_probs = predict(model, predict_data_loader)

# æ ¹æ®é¢„æµ‹æ¦‚ç‡è·å–é¢„æµ‹ label
y_preds = np.argmax(y_probs, axis=1)
```

### 9.6è¾“å‡ºé¢„æµ‹ç»“æœ


```python
# æˆ‘ä»¬æŒ‰ç…§åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›çš„æäº¤æ ¼å¼å°†é¢„æµ‹ç»“æœå­˜å‚¨åœ¨ lcqmc.tsv ä¸­ï¼Œç”¨æ¥åç»­æäº¤
# åŒæ—¶å°†é¢„æµ‹ç»“æœè¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¾¿äºå¤§å®¶ç›´è§‚æ„Ÿå—æ¨¡å‹é¢„æµ‹æ•ˆæœ

test_ds = load_dataset("lcqmc", splits=["test"])

with open("lcqmc.tsv", 'w', encoding="utf-8") as f:
    f.write("index\tprediction\n")    
    for idx, y_pred in enumerate(y_preds):
        f.write("{}\t{}\n".format(idx, y_pred))
        text_pair = test_ds[idx]
        text_pair["label"] = y_pred
        print(text_pair)
```

### 9.7æäº¤ lcqmc é¢„æµ‹ç»“æœ[åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›](https://aistudio.baidu.com/aistudio/competition/detail/45)

åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›ä¸€å…±æœ‰ 3 ä¸ªæ•°æ®é›†: lcqmcã€bq_corpusã€paws-x, æˆ‘ä»¬åˆšæ‰ç”Ÿæˆäº† lcqmc çš„é¢„æµ‹ç»“æœ lcqmc.tsv, åŒæ—¶æˆ‘ä»¬åœ¨é¡¹ç›®å†…æä¾›äº† bq_corpusã€paw-x æ•°æ®é›†çš„ç©ºé¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬å°†è¿™ 3 ä¸ªæ–‡ä»¶æ‰“åŒ…æäº¤åˆ°åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›ï¼Œå³å¯çœ‹åˆ°è‡ªå·±çš„æ¨¡å‹åœ¨ Lcqmc æ•°æ®é›†ä¸Šçš„ç«èµ›æˆç»©ã€‚




```python
# æ‰“åŒ…é¢„æµ‹ç»“æœ
!zip submit.zip lcqmc.tsv paws-x.tsv bq_corpus.tsv
```

##### æäº¤é¢„æµ‹ç»“æœ submit.zip åˆ° [åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›](https://aistudio.baidu.com/aistudio/competition/detail/45)
### åŸºçº¿ç«Ÿç„¶å¦‚æ­¤ç®€å•

# ä¸‰ã€å†…ç½® PaddleNLP Datasets API(å¦ˆå¦ˆè¯´æˆ‘å†ä¹Ÿä¸ç¼ºæ•°æ®é›†äº†)

PaddleNLPæä¾›äº†ä»¥ä¸‹æ•°æ®é›†çš„å¿«é€Ÿè¯»å–APIï¼Œå®é™…ä½¿ç”¨æ—¶è¯·æ ¹æ®éœ€è¦**æ·»åŠ splitsä¿¡æ¯**ï¼š

## 1.é˜…è¯»ç†è§£

|  æ•°æ®é›†åç§°   | ç®€ä»‹ | è°ƒç”¨æ–¹æ³• |
|  ----  | ----- | ------ |
|  [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) | æ–¯å¦ç¦é—®ç­”æ•°æ®é›†ï¼ŒåŒ…æ‹¬SQuAD1.1å’ŒSQuAD2.0|`paddlenlp.datasets.load_dataset('squad')` |
|  [DuReader-yesno](https://aistudio.baidu.com/aistudio/competition/detail/49) | åƒè¨€æ•°æ®é›†ï¼šé˜…è¯»ç†è§£ï¼Œåˆ¤æ–­ç­”æ¡ˆææ€§|`paddlenlp.datasets.load_dataset('dureader_yesno')` |
|  [DuReader-robust](https://aistudio.baidu.com/aistudio/competition/detail/49) | åƒè¨€æ•°æ®é›†ï¼šé˜…è¯»ç†è§£ï¼Œç­”æ¡ˆåŸæ–‡æŠ½å–|`paddlenlp.datasets.load_dataset('dureader_robust')` |
|  [CMRC2018](http://hfl-rc.com/cmrc2018/) | ç¬¬äºŒå±Šâ€œè®¯é£æ¯â€ä¸­æ–‡æœºå™¨é˜…è¯»ç†è§£è¯„æµ‹æ•°æ®é›†|`paddlenlp.datasets.load_dataset('cmrc2018')` |
|  [DRCD](https://github.com/DRCKnowledgeTeam/DRCD) | å°é”é–±è®€ç†è§£è³‡æ–™é›†|`paddlenlp.datasets.load_dataset('drcd')` |

##  2.æ–‡æœ¬åˆ†ç±»

| æ•°æ®é›†åç§°  | ç®€ä»‹ | è°ƒç”¨æ–¹æ³• |
| ----  | --------- | ------ |
|  [CoLA](https://nyu-mll.github.io/CoLA/) | å•å¥åˆ†ç±»ä»»åŠ¡ï¼ŒäºŒåˆ†ç±»ï¼Œåˆ¤æ–­å¥å­æ˜¯å¦åˆæ³•| `paddlenlp.datasets.load_dataset('glue','cola')`|
|  [SST-2](https://nlp.stanford.edu/sentiment/index.html) | å•å¥åˆ†ç±»ä»»åŠ¡ï¼ŒäºŒåˆ†ç±»ï¼Œåˆ¤æ–­å¥å­æƒ…æ„Ÿææ€§| `paddlenlp.datasets.load_dataset('glue','sst-2')`|
|  [MRPC](https://microsoft.com/en-us/download/details.aspx?id=52398) | å¥å¯¹åŒ¹é…ä»»åŠ¡ï¼ŒäºŒåˆ†ç±»ï¼Œåˆ¤æ–­å¥å­å¯¹æ˜¯å¦æ˜¯ç›¸åŒæ„æ€| `paddlenlp.datasets.load_dataset('glue','mrpc')`|
|  [STSB](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) | è®¡ç®—å¥å­å¯¹ç›¸ä¼¼æ€§ï¼Œåˆ†æ•°ä¸º1~5| `paddlenlp.datasets.load_dataset('glue','sts-b')`|
|  [QQP](https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs) | åˆ¤å®šå¥å­å¯¹æ˜¯å¦ç­‰æ•ˆï¼Œç­‰æ•ˆã€ä¸ç­‰æ•ˆä¸¤ç§æƒ…å†µï¼ŒäºŒåˆ†ç±»ä»»åŠ¡| `paddlenlp.datasets.load_dataset('glue','qqp')`|
|  [MNLI](http://www.nyu.edu/projects/bowman/multinli/) | å¥å­å¯¹ï¼Œä¸€ä¸ªå‰æï¼Œä¸€ä¸ªæ˜¯å‡è®¾ã€‚å‰æå’Œå‡è®¾çš„å…³ç³»æœ‰ä¸‰ç§æƒ…å†µï¼šè•´å«ï¼ˆentailmentï¼‰ï¼ŒçŸ›ç›¾ï¼ˆcontradictionï¼‰ï¼Œä¸­ç«‹ï¼ˆneutralï¼‰ã€‚å¥å­å¯¹ä¸‰åˆ†ç±»é—®é¢˜| `paddlenlp.datasets.load_dataset('glue','mnli')`|
|  [QNLI](https://rajpurkar.github.io/SQuAD-explorer/) | åˆ¤æ–­é—®é¢˜ï¼ˆquestionï¼‰å’Œå¥å­ï¼ˆsentenceï¼‰æ˜¯å¦è•´å«ï¼Œè•´å«å’Œä¸è•´å«ï¼ŒäºŒåˆ†ç±»| `paddlenlp.datasets.load_dataset('glue','qnli')`|
|  [RTE](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) | åˆ¤æ–­å¥å¯¹æ˜¯å¦è•´å«ï¼Œå¥å­1å’Œå¥å­2æ˜¯å¦äº’ä¸ºè•´å«ï¼ŒäºŒåˆ†ç±»ä»»åŠ¡| `paddlenlp.datasets.load_dataset('glue','rte')`|
|  [WNLI](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html) | åˆ¤æ–­å¥å­å¯¹æ˜¯å¦ç›¸å…³ï¼Œç›¸å…³æˆ–ä¸ç›¸å…³ï¼ŒäºŒåˆ†ç±»ä»»åŠ¡| `paddlenlp.datasets.load_dataset('glue','wnli')`|
|  [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) | A Large-scale Chinese Question Matching Corpus è¯­ä¹‰åŒ¹é…æ•°æ®é›†| `paddlenlp.datasets.load_dataset('lcqmc')`|
|  [ChnSentiCorp](https://github.com/SophonPlus/ChineseNlpCorpus/blob/master/datasets/ChnSentiCorp_htl_all/intro.ipynb) | ä¸­æ–‡è¯„è®ºæƒ…æ„Ÿåˆ†æè¯­æ–™| `paddlenlp.datasets.load_dataset('chnsenticorp')`|


## 3.åºåˆ—æ ‡æ³¨

|  æ•°æ®é›†åç§°   | ç®€ä»‹ | è°ƒç”¨æ–¹æ³• |
|  ----  | --------- | ------ |
|  [MSRA_NER](https://github.com/lemonhu/NER-BERT-pytorch/tree/master/data/msra) | MSRA å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†| `paddlenlp.datasets.load_dataset('msra_ner')`|
|  [People's Daily](https://github.com/OYE93/Chinese-NLP-Corpus/tree/master/NER/People's%20Daily) | äººæ°‘æ—¥æŠ¥å‘½åå®ä½“è¯†åˆ«æ•°æ®é›†| `paddlenlp.datasets.load_dataset('peoples_daily_ner')`|


## 4.æœºå™¨ç¿»è¯‘

| æ•°æ®é›†åç§°  | ç®€ä»‹ | è°ƒç”¨æ–¹æ³• |
| ----  | --------- | ------ |
|  [IWSLT15](https://workshop2015.iwslt.org/) | IWSLT'15 English-Vietnamese data è‹±è¯­-è¶Šå—è¯­ç¿»è¯‘æ•°æ®é›†| `paddlenlp.datasets.load_dataset('iwslt15')`|
|  [WMT14ENDE](http://www.statmt.org/wmt14/translation-task.html) | WMT14 EN-DE ç»è¿‡BPEåˆ†è¯çš„è‹±è¯­-å¾·è¯­ç¿»è¯‘æ•°æ®é›†| `paddlenlp.datasets.load_dataset('wmt14ende')`|


## 5.æœºå™¨åŒä¼ 

| æ•°æ®é›†åç§°  | ç®€ä»‹ | è°ƒç”¨æ–¹æ³• |
| ----  | --------- | ------ |
|  [BSTC](https://aistudio.baidu.com/aistudio/competition/detail/44/) | åƒè¨€æ•°æ®é›†ï¼šæœºå™¨åŒä¼ ï¼ŒåŒ…æ‹¬transcription_translationå’Œasr | `paddlenlp.datasets.load_dataset('bstc', 'asr')`|


## 6.æ–‡æœ¬ç”Ÿæˆ

| æ•°æ®é›†åç§°  | ç®€ä»‹ | è°ƒç”¨æ–¹æ³• |
| ----  | --------- | ------ |
|  [Poetry](https://github.com/chinese-poetry/chinese-poetry) | ä¸­æ–‡è¯—æ­Œå¤å…¸æ–‡é›†æ•°æ®| `paddlenlp.datasets.load_dataset('poetry')`|
|  [Couplet](https://github.com/v-zich/couplet-clean-dataset) | ä¸­æ–‡å¯¹è”æ•°æ®é›†| `paddlenlp.datasets.load_dataset('couplet')`|

## 7.è¯­æ–™åº“

| æ•°æ®é›†åç§°  | ç®€ä»‹ | è°ƒç”¨æ–¹æ³• |
| ----  | --------- | ------ |
|  [PTB](http://www.fit.vutbr.cz/~imikolov/rnnlm/) | Penn Treebank Dataset | `paddlenlp.datasets.load_dataset('ptb')`|
|  [Yahoo Answer 100k](https://arxiv.org/pdf/1702.08139.pdf)  | ä»Yahoo Answeré‡‡æ ·100K| `paddlenlp.datasets.load_dataset('yahoo_answer_100k')`|


# å››ã€PaddleNLP data API
## 1.APIlåˆ—è¡¨

| API                             | ç®€ä»‹                                       |
| ------------------------------- | :----------------------------------------- |
| `paddlenlp.data.Stack`          | å †å Nä¸ªå…·æœ‰ç›¸åŒshapeçš„è¾“å…¥æ•°æ®æ¥æ„å»ºä¸€ä¸ªbatch |
| `paddlenlp.data.Pad`            | å †å Nä¸ªè¾“å…¥æ•°æ®æ¥æ„å»ºä¸€ä¸ªbatchï¼Œæ¯ä¸ªè¾“å…¥æ•°æ®å°†ä¼šè¢«paddingåˆ°Nä¸ªè¾“å…¥æ•°æ®ä¸­æœ€å¤§çš„é•¿åº¦ |
| `paddlenlp.data.Tuple`          | å°†å¤šä¸ªbatchifyå‡½æ•°åŒ…è£…åœ¨ä¸€èµ·ï¼Œç»„æˆtuple      |
| `paddlenlp.data.Dict`           | å°†å¤šä¸ªbatchifyå‡½æ•°åŒ…è£…åœ¨ä¸€èµ·ï¼Œç»„æˆdict       |
| `paddlenlp.data.SamplerHelper`  | æ„å»ºç”¨äº`Dataloader`çš„å¯è¿­ä»£sampler         |
| `paddlenlp.data.Vocab`          | ç”¨äºæ–‡æœ¬tokenå’ŒIDä¹‹é—´çš„æ˜ å°„                  |
| `paddlenlp.data.JiebaTokenizer` | Jiebaåˆ†è¯                                  |

## 2.APIä½¿ç”¨æ–¹æ³•

ä»¥ä¸ŠAPIéƒ½æ˜¯ç”¨æ¥è¾…åŠ©æ„å»º`DataLoader`ï¼Œ`DataLoader`æ¯”è¾ƒé‡è¦çš„ä¸‰ä¸ªåˆå§‹åŒ–å‚æ•°æ˜¯`dataset`ã€`batch_sampler`å’Œ`collate_fn`ã€‚

`paddlenlp.data.Vocab`å’Œ`paddlenlp.data.JiebaTokenizer`ç”¨åœ¨æ„å»º`dataset`æ—¶å¤„ç†æ–‡æœ¬tokenåˆ°IDçš„æ˜ å°„ã€‚

`paddlenlp.data.SamplerHelper`ç”¨äºæ„å»ºå¯è¿­ä»£çš„`batch_sampler`ã€‚

`paddlenlp.data.Stack`ã€`paddlenlp.data.Pad`ã€`paddlenlp.data.Tuple`å’Œ`paddlenlp.data.Dict`ç”¨äºæ„å»ºç”Ÿæˆmini-batchçš„`collate_fn`å‡½æ•°ã€‚

### 2.1æ•°æ®é¢„å¤„ç†

#### 2.1.1 `paddlenlp.data.Vocab`

`paddlenlp.data.Vocab`è¯è¡¨ç±»ï¼Œé›†åˆäº†ä¸€ç³»åˆ—æ–‡æœ¬tokenä¸idsä¹‹é—´æ˜ å°„çš„ä¸€ç³»åˆ—æ–¹æ³•ï¼Œæ”¯æŒä»æ–‡ä»¶ã€å­—å…¸ã€jsonç­‰ä¸€ç³»æ–¹å¼æ„å»ºè¯è¡¨ã€‚

```python
from paddlenlp.data import Vocab
# ä»æ–‡ä»¶æ„å»º
vocab1 = Vocab.load_vocabulary(vocab_file_path)
# ä»å­—å…¸æ„å»º
# dic = {'unk':0, 'pad':1, 'bos':2, 'eos':3, ...}
vocab2 = Vocab.from_dict(dic)
# ä»jsonæ„å»ºï¼Œä¸€èˆ¬æ˜¯å·²æ„å»ºå¥½çš„Vocabå¯¹è±¡å…ˆä¿å­˜ä¸ºjson_stræˆ–jsonæ–‡ä»¶åå†è¿›è¡Œæ¢å¤
# json_stræ–¹å¼
json_str = vocab1.to_json()
vocab3 = Vocab.from_json(json_str)
# jsonæ–‡ä»¶æ–¹å¼
vocab1.to_json(json_file_path)
vocab4 = Vocab.from_json(json_file_path)
```

#### 2.1.2 `paddlenlp.data.JiebaTokenizer`

`paddlenlp.data.JiebaTokenizer`åˆå§‹åŒ–éœ€ä¼ å…¥`paddlenlp.data.Vocab`ç±»ï¼ŒåŒ…å«`cut`åˆ†è¯æ–¹æ³•å’Œå°†å¥å­æ˜æ–‡è½¬æ¢ä¸ºidsçš„`encode`æ–¹æ³•ã€‚

```python
from paddlenlp.data import Vocab, JiebaTokenizer
# è¯è¡¨æ–‡ä»¶è·¯å¾„ï¼Œè¿è¡Œç¤ºä¾‹ç¨‹åºå¯å…ˆä¸‹è½½è¯è¡¨æ–‡ä»¶
# wget https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt
vocab_file_path = './senta_word_dict.txt'
# æ„å»ºè¯è¡¨
vocab = Vocab.load_vocabulary(
    vocab_file_path,
    unk_token='[UNK]',
    pad_token='[PAD]')
tokenizer = JiebaTokenizer(vocab)
tokens = tokenizer.cut('æˆ‘çˆ±ä½ ä¸­å›½') # ['æˆ‘çˆ±ä½ ', 'ä¸­å›½']
ids = tokenizer.encode('æˆ‘çˆ±ä½ ä¸­å›½') # [1170578, 575565]
```

### 2.2 æ„å»º`Sampler`

#### 2.2.1 `paddlenlp.data.SamplerHelper`

`paddlenlp.data.SamplerHelper`çš„ä½œç”¨æ˜¯æ„å»ºç”¨äº`DataLoader`çš„å¯è¿­ä»£é‡‡æ ·å™¨ï¼Œå®ƒåŒ…å«`shuffle`ã€`sort`ã€`batch`ã€`shard`ç­‰ä¸€ç³»åˆ—æ–¹æ³•ï¼Œæ–¹ä¾¿ç”¨æˆ·çµæ´»ä½¿ç”¨ã€‚

```python
from paddlenlp.data import SamplerHelper
from paddle.io import Dataset

class MyDataset(Dataset):
    def __init__(self):
        super(MyDataset, self).__init__()
        self.data = [
            [[1, 2, 3, 4], [1]],
            [[5, 6, 7], [0]],
            [[8, 9], [1]],
        ]

    def __getitem__(self, index):
        data = self.data[index][0]
        label = self.data[index][1]
        return data, label

    def __len__(self):
        return len(self.data)

dataset = MyDataset()
# SamplerHelperè¿”å›çš„æ˜¯æ•°æ®ç´¢å¼•çš„å¯è¿­ä»£å¯¹è±¡ï¼Œäº§ç”Ÿçš„è¿­ä»£çš„ç´¢å¼•ä¸ºï¼š[0, 1, 2]
sampler = SamplerHelper(dataset)
# `shuffle()`çš„ä½œç”¨æ˜¯éšæœºæ‰“ä¹±ç´¢å¼•é¡ºåºï¼Œäº§ç”Ÿçš„è¿­ä»£çš„ç´¢å¼•ä¸ºï¼š[0, 2, 1]
sampler = sampler.shuffle()
# sort()çš„ä½œç”¨æ˜¯æŒ‰ç…§æŒ‡å®škeyä¸ºæ’åºæ–¹å¼å¹¶åœ¨buffer_sizeå¤§å°ä¸ªæ ·æœ¬ä¸­æ’åº
# ç¤ºä¾‹ä¸­ä»¥æ ·æœ¬ç¬¬ä¸€ä¸ªå­—æ®µçš„é•¿åº¦è¿›è¡Œå‡åºæ’åºï¼Œäº§ç”Ÿçš„è¿­ä»£çš„ç´¢å¼•ä¸ºï¼š[2, 0, 1]
key = (lambda x, data_source: len(data_source[x][0]))
sampler = sampler.sort(key=key, buffer_size=2)
# batch()çš„ä½œç”¨æ˜¯æŒ‰ç…§batch_sizeç»„å»ºmini-batchï¼Œäº§ç”Ÿçš„è¿­ä»£çš„ç´¢å¼•ä¸ºï¼š[[2, 0], [1]]
sampler = sampler.batch(batch_size=2)
# shard()çš„ä½œç”¨æ˜¯ä¸ºå¤šå¡è®­ç»ƒåˆ‡åˆ†æ•°æ®é›†ï¼Œå½“å‰å¡äº§ç”Ÿçš„è¿­ä»£çš„ç´¢å¼•ä¸ºï¼š[[2, 0]]
sampler = sampler.shard(num_replicas=2)
```

### 2.3 æ„å»º`collate_fn`

#### 2.3.1 `paddlenlp.data.Stack`

`paddlenlp.data.Stack`ç”¨æ¥ç»„å»ºbatchï¼Œå…¶è¾“å…¥å¿…é¡»å…·æœ‰ç›¸åŒçš„shapeï¼Œè¾“å‡ºä¾¿æ˜¯è¿™äº›è¾“å…¥çš„å †å ç»„æˆçš„batchæ•°æ®ã€‚

```python
from paddlenlp.data import Stack
a = [1, 2, 3, 4]
b = [3, 4, 5, 6]
c = [5, 6, 7, 8]
result = Stack()([a, b, c])
"""
[[1, 2, 3, 4],
 [3, 4, 5, 6],
 [5, 6, 7, 8]]
"""
```

#### 2.3.2  `paddlenlp.data.Pad`

`paddlenlp.data.Pad`ç”¨æ¥ç»„å»ºbatchï¼Œå®ƒçš„è¾“å…¥é•¿åº¦ä¸åŒï¼Œå®ƒé¦–å…ˆä¼šå°†è¾“å…¥æ•°æ®å…¨éƒ¨paddingåˆ°æœ€å¤§é•¿åº¦ï¼Œç„¶åå†å †å ç»„æˆbatchæ•°æ®è¾“å‡ºã€‚

```python
from paddlenlp.data import Pad
a = [1, 2, 3, 4]
b = [5, 6, 7]
c = [8, 9]
result = Pad(pad_val=0)([a, b, c])
"""
[[1, 2, 3, 4],
 [5, 6, 7, 0],
 [8, 9, 0, 0]]
"""
```

#### 2.3.3 `paddlenlp.data.Tuple`

`paddlenlp.data.Tuple`ä¼šå°†å¤šä¸ªç»„batchçš„å‡½æ•°åŒ…è£…åœ¨ä¸€èµ·ï¼Œç»„æˆtupleã€‚

```python
from paddlenlp.data import Stack, Pad, Tuple
data = [
        [[1, 2, 3, 4], [1]],
        [[5, 6, 7], [0]],
        [[8, 9], [1]],
       ]
batchify_fn = Tuple(Pad(pad_val=0), Stack())
ids, label = batchify_fn(data)
"""
ids:
[[1, 2, 3, 4],
 [5, 6, 7, 0],
 [8, 9, 0, 0]]
label: [[1], [0], [1]]
"""
```

#### 2.3.4 `paddlenlp.data.Dict`

`paddlenlp.data.Dict`ä¼šå°†å¤šä¸ªç»„batchçš„å‡½æ•°åŒ…è£…åœ¨ä¸€èµ·ï¼Œç»„æˆdictã€‚

```python
from paddlenlp.data import Stack, Pad, Dict
data = [
        {'labels':[1], 'token_ids':[1, 2, 3, 4]},
        {'labels':[0], 'token_ids':[5, 6, 7]},
        {'labels':[1], 'token_ids':[8, 9]},
       ]
batchify_fn = Dict({'token_ids':Pad(pad_val=0), 'labels':Stack()})
ids, label = batchify_fn(data)
"""
ids:
[[1, 2, 3, 4],
 [5, 6, 7, 0],
 [8, 9, 0, 0]]
label: [[1], [0], [1]]
"""
```

### 2.4 ç»¼åˆç¤ºä¾‹

```python
from paddlenlp.data import Vocab, JiebaTokenizer, Stack, Pad, Tuple, SamplerHelper
from paddlenlp.datasets import load_dataset
from paddle.io import DataLoader

# è¯è¡¨æ–‡ä»¶è·¯å¾„ï¼Œè¿è¡Œç¤ºä¾‹ç¨‹åºå¯å…ˆä¸‹è½½è¯è¡¨æ–‡ä»¶
# wget https://paddlenlp.bj.bcebos.com/data/senta_word_dict.txt
vocab_file_path = './senta_word_dict.txt'
# æ„å»ºè¯è¡¨
vocab = Vocab.load_vocabulary(
    vocab_file_path,
    unk_token='[UNK]',
    pad_token='[PAD]')
# åˆå§‹åŒ–åˆ†è¯å™¨
tokenizer = JiebaTokenizer(vocab)

def convert_example(example):
    text, label = example['text'], example['label']
    ids = tokenizer.encode(text)
    label = [label]
    return ids, label

dataset = load_dataset('chnsenticorp', splits='train')
dataset = dataset.map(convert_example, lazy=True)

pad_id = vocab.token_to_idx[vocab.pad_token]
batchify_fn = Tuple(
    Pad(axis=0, pad_val=pad_id),  # ids
    Stack(dtype='int64')  # label
)

batch_sampler = SamplerHelper(dataset).shuffle().batch(batch_size=16)
data_loader = DataLoader(
    dataset,
    batch_sampler=batch_sampler,
    collate_fn=batchify_fn,
    return_list=True)

# æµ‹è¯•æ•°æ®é›†
for batch in data_loader:
    ids, label = batch
    print(ids.shape, label.shape)
    print(ids)
    print(label)
    break
```



```python

```
