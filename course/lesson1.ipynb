{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# æˆ‘çš„PaddleNLPå­¦ä¹ ç¬”è®°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ä¸€ã€PaddleNLPåŠ è½½é¢„è®­ç»ƒè¯å‘é‡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "6.7æ—¥NLPç›´æ’­æ‰“å¡è¯¾å¼€å§‹å•¦\n",
    "\n",
    "**[ç›´æ’­é“¾æ¥è¯·æˆ³è¿™é‡Œï¼Œæ¯æ™š20:00-21:30ğŸ‘ˆ](http://live.bilibili.com/21689802)**\n",
    "\n",
    "**[è¯¾ç¨‹åœ°å€è¯·æˆ³è¿™é‡ŒğŸ‘ˆ](https://aistudio.baidu.com/aistudio/course/introduce/24177)**\n",
    "\n",
    "æ¬¢è¿æ¥è¯¾ç¨‹**QQç¾¤**ï¼ˆç¾¤å·:618354318ï¼‰äº¤æµå§~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "è¯å‘é‡ï¼ˆWord embeddingï¼‰ï¼Œå³æŠŠè¯è¯­è¡¨ç¤ºæˆå®æ•°å‘é‡ã€‚â€œå¥½â€çš„è¯å‘é‡èƒ½ä½“ç°è¯è¯­ç›´æ¥çš„ç›¸è¿‘å…³ç³»ã€‚è¯å‘é‡å·²ç»è¢«è¯æ˜å¯ä»¥æé«˜NLPä»»åŠ¡çš„æ€§èƒ½ï¼Œä¾‹å¦‚è¯­æ³•åˆ†æå’Œæƒ…æ„Ÿåˆ†æã€‚\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/54878855b1df42f9ab50b280d76906b1e0175f280b0f4a2193a542c72634a9bf\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p>\n",
    "<br><center>å›¾1ï¼šè¯å‘é‡ç¤ºæ„å›¾</center></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "PaddleNLPå·²é¢„ç½®å¤šä¸ªå…¬å¼€çš„é¢„è®­ç»ƒEmbeddingï¼Œæ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨`paddlenlp.embeddings.TokenEmbedding`æ¥å£åŠ è½½é¢„è®­ç»ƒEmbeddingï¼Œä»è€Œæå‡è®­ç»ƒæ•ˆæœã€‚æœ¬ç¯‡æ•™ç¨‹å°†ä¾æ¬¡ä»‹ç»`paddlenlp.embeddings.TokenEmbedding`çš„åˆå§‹åŒ–å’Œæ–‡æœ¬è¡¨ç¤ºæ•ˆæœï¼Œå¹¶é€šè¿‡æ–‡æœ¬åˆ†ç±»è®­ç»ƒçš„ä¾‹å­å±•ç¤ºå…¶å¯¹è®­ç»ƒæå‡çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# äºŒã€PaddleNLPå®‰è£…\n",
    "AI Studioå¹³å°åç»­ä¼šé»˜è®¤å®‰è£…PaddleNLPï¼Œåœ¨æ­¤ä¹‹å‰å¯ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å®‰è£…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "from paddlenlp.data import Pad, Stack, Tuple, Vocab\n",
    "\n",
    "import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ä¸‰ã€PaddleNLPä½¿ç”¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## 1. TokenEmbeddingåˆå§‹åŒ–\n",
    "- `embedding_name`\n",
    "å°†æ¨¡å‹åç§°ä»¥å‚æ•°å½¢å¼ä¼ å…¥TokenEmbeddingï¼ŒåŠ è½½å¯¹åº”çš„æ¨¡å‹ã€‚é»˜è®¤ä¸º`w2v.baidu_encyclopedia.target.word-word.dim300`çš„è¯å‘é‡ã€‚\n",
    "- `unknown_token`\n",
    "æœªçŸ¥tokençš„è¡¨ç¤ºï¼Œé»˜è®¤ä¸º[UNK]ã€‚\n",
    "- `unknown_token_vector`\n",
    "æœªçŸ¥tokençš„å‘é‡è¡¨ç¤ºï¼Œé»˜è®¤ç”Ÿæˆå’Œembeddingç»´æ•°ä¸€è‡´ï¼Œæ•°å€¼å‡å€¼ä¸º0çš„æ­£æ€åˆ†å¸ƒå‘é‡ã€‚\n",
    "- `extended_vocab_path`\n",
    "æ‰©å±•è¯æ±‡åˆ—è¡¨æ–‡ä»¶è·¯å¾„ï¼Œè¯è¡¨æ ¼å¼ä¸ºä¸€è¡Œä¸€ä¸ªè¯ã€‚å¦‚å¼•å…¥æ‰©å±•è¯æ±‡åˆ—è¡¨ï¼Œtrainable=Trueã€‚\n",
    "- `trainable`\n",
    "Embeddingå±‚æ˜¯å¦å¯è¢«è®­ç»ƒã€‚Trueè¡¨ç¤ºEmbeddingå¯ä»¥æ›´æ–°å‚æ•°ï¼ŒFalseä¸ºä¸å¯æ›´æ–°ã€‚é»˜è®¤ä¸ºTrueã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 694483/694483 [00:10<00:00, 69402.57it/s]\n",
      "[2021-06-06 23:03:00,552] [    INFO] - Loading token embedding...\n",
      "[2021-06-06 23:03:10,475] [    INFO] - Finish loading embedding vector.\n",
      "[2021-06-06 23:03:10,478] [    INFO] - Token Embedding info:             \n",
      "Unknown index: 635963             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 635964             \n",
      "Padding token: [PAD]             \n",
      "Shape :[635965, 300]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object   type: TokenEmbedding(635965, 300, padding_idx=635964, sparse=False)             \n",
      "Unknown index: 635963             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 635964             \n",
      "Padding token: [PAD]             \n",
      "Parameter containing:\n",
      "Tensor(shape=[635965, 300], dtype=float32, place=CPUPlace, stop_gradient=False,\n",
      "       [[-0.24200200,  0.13931701,  0.07378800, ...,  0.14103900,  0.05592300, -0.08004800],\n",
      "        [-0.08671700,  0.07770800,  0.09515300, ...,  0.11196400,  0.03082200, -0.12893000],\n",
      "        [-0.11436500,  0.12201900,  0.02833000, ...,  0.11068700,  0.03607300, -0.13763499],\n",
      "        ...,\n",
      "        [ 0.02628800, -0.00008300, -0.00393500, ...,  0.00654000,  0.00024600, -0.00662600],\n",
      "        [ 0.00743478, -0.00040147,  0.00931276, ..., -0.01128159, -0.00069775, -0.00615075],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,  0.        ,  0.        ]])\n"
     ]
    }
   ],
   "source": [
    "# åˆå§‹åŒ–TokenEmbeddingï¼Œ é¢„è®­ç»ƒembeddingæœªä¸‹è½½æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½å¹¶åŠ è½½æ•°æ®\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")\n",
    "\n",
    "# æŸ¥çœ‹token_embeddingè¯¦æƒ…\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. TokenEmbedding.search\n",
    "è·å¾—æŒ‡å®šè¯æ±‡çš„è¯å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.260801  0.1047    0.129453 -0.257317 -0.16152   0.19567  -0.074868\n",
      "   0.361168  0.245882 -0.219141 -0.388083  0.235189  0.029316  0.154215\n",
      "  -0.354343  0.017746  0.009028  0.01197  -0.121429  0.096542  0.009255\n",
      "   0.039721  0.363704 -0.239497 -0.41168   0.16958   0.261758  0.022383\n",
      "  -0.053248 -0.000994 -0.209913 -0.208296  0.197332 -0.3426   -0.162112\n",
      "   0.134557 -0.250201  0.431298  0.303116  0.517221  0.243843  0.022219\n",
      "  -0.136554 -0.189223  0.148563 -0.042963 -0.456198  0.14546  -0.041207\n",
      "   0.049685  0.20294   0.147355 -0.206953 -0.302796 -0.111834  0.128183\n",
      "   0.289539 -0.298934 -0.096412  0.063079  0.324821 -0.144471  0.052456\n",
      "   0.088761 -0.040925 -0.103281 -0.216065 -0.200878 -0.100664  0.170614\n",
      "  -0.355546 -0.062115 -0.52595  -0.235442  0.300866 -0.521523 -0.070713\n",
      "  -0.331768  0.023021  0.309111 -0.125696  0.016723 -0.0321   -0.200611\n",
      "   0.057294 -0.128891 -0.392886  0.423002  0.282569 -0.212836  0.450132\n",
      "   0.067604 -0.124928 -0.294086  0.136479  0.091505 -0.061723 -0.577495\n",
      "   0.293856 -0.401198  0.302559 -0.467656  0.021708 -0.088507  0.088322\n",
      "  -0.015567  0.136594  0.112152  0.005394  0.133818  0.071278 -0.198807\n",
      "   0.043538  0.116647 -0.210486 -0.217972 -0.320675  0.293977  0.277564\n",
      "   0.09591  -0.359836  0.473573  0.083847  0.240604  0.441624  0.087959\n",
      "   0.064355 -0.108271  0.055709  0.380487 -0.045262  0.04014  -0.259215\n",
      "  -0.398335  0.52712  -0.181298  0.448978 -0.114245 -0.028225 -0.146037\n",
      "   0.347414 -0.076505  0.461865 -0.105099  0.131892  0.079946  0.32422\n",
      "  -0.258629  0.05225   0.566337  0.348371  0.124111  0.229154  0.075039\n",
      "  -0.139532 -0.08839  -0.026703 -0.222828 -0.106018  0.324477  0.128269\n",
      "  -0.045624  0.071815 -0.135702  0.261474  0.297334 -0.031481  0.18959\n",
      "   0.128716  0.090022  0.037609 -0.049669  0.092909  0.0564   -0.347994\n",
      "  -0.367187 -0.292187  0.021649 -0.102004 -0.398568 -0.278248 -0.082361\n",
      "  -0.161823  0.044846  0.212597 -0.013164  0.005527 -0.004024  0.176243\n",
      "   0.237274 -0.174856 -0.197214  0.150825 -0.164427 -0.244255 -0.14897\n",
      "   0.098907 -0.295891 -0.013408 -0.146875 -0.126049  0.033235 -0.133444\n",
      "  -0.003258  0.082053 -0.162569  0.283657  0.315608 -0.171281 -0.276051\n",
      "   0.258458  0.214045 -0.129798 -0.511728  0.198481 -0.35632  -0.186253\n",
      "  -0.203719  0.22004  -0.016474  0.080321 -0.463004  0.290794 -0.003445\n",
      "   0.061247 -0.069157 -0.022525  0.13514   0.001354  0.011079  0.014223\n",
      "  -0.079145 -0.41402  -0.404242 -0.301509  0.036712  0.037076 -0.061683\n",
      "  -0.202429  0.130216  0.054355  0.140883 -0.030627 -0.281293 -0.28059\n",
      "  -0.214048 -0.467033  0.203632 -0.541544  0.183898 -0.129535 -0.286422\n",
      "  -0.162222  0.262487  0.450505  0.11551  -0.247965 -0.15837   0.060613\n",
      "  -0.285358  0.498203  0.025008 -0.256397  0.207582  0.166383  0.669677\n",
      "  -0.067961 -0.049835 -0.444369  0.369306  0.134493 -0.080478 -0.304565\n",
      "  -0.091756  0.053657  0.114497 -0.076645 -0.123933  0.168645  0.018987\n",
      "  -0.260592 -0.019668 -0.063312 -0.094939  0.657352  0.247547 -0.161621\n",
      "   0.289043 -0.284084  0.205076  0.059885  0.055871  0.159309  0.062181\n",
      "   0.123634  0.282932  0.140399 -0.076253 -0.087103  0.07262 ]]\n"
     ]
    }
   ],
   "source": [
    "test_token_embedding = token_embedding.search(\"ä¸­å›½\")\n",
    "print(test_token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 300)\n"
     ]
    }
   ],
   "source": [
    "print(test_token_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.TokenEmbedding.cosine_sim\n",
    "è®¡ç®—è¯å‘é‡é—´ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¯­ä¹‰ç›¸è¿‘çš„è¯è¯­ä½™å¼¦ç›¸ä¼¼åº¦æ›´é«˜ï¼Œè¯´æ˜é¢„è®­ç»ƒå¥½çš„è¯å‘é‡ç©ºé—´æœ‰å¾ˆå¥½çš„è¯­ä¹‰è¡¨ç¤ºèƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score1: 0.7017183\n",
      "score2: 0.19189896\n",
      "score3: 0.7981167\n",
      "score4: 0.49586025\n",
      "score5: 8.611071\n"
     ]
    }
   ],
   "source": [
    "score1 = token_embedding.cosine_sim(\"å¥³å­©\", \"å¥³äºº\")\n",
    "score2 = token_embedding.cosine_sim(\"å¥³å­©\", \"ä¹¦ç±\")\n",
    "score3 = token_embedding.cosine_sim(\"å¥³å­©\", \"ç”·å­©\")\n",
    "score4 = token_embedding.cosine_sim(\"ä¸­å›½\", \"ç¾å›½\")\n",
    "score5 = token_embedding.dot(\"ä¸­å›½\", \"ç¾å›½\")\n",
    "print('score1:',score1)\n",
    "print('score2:',score2)\n",
    "print('score3:',score3)\n",
    "print('score4:',score4)\n",
    "print('score5:',score5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.è¯å‘é‡æ˜ å°„åˆ°ä½ç»´ç©ºé—´\n",
    "\n",
    "ä½¿ç”¨æ·±åº¦å­¦ä¹ å¯è§†åŒ–å·¥å…·[VisualDL](https://github.com/PaddlePaddle/VisualDL)çš„[High Dimensional](https://github.com/PaddlePaddle/VisualDL/blob/develop/docs/components/README_CN.md#High-Dimensional--%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4%E7%BB%84%E4%BB%B6)ç»„ä»¶å¯ä»¥å¯¹embeddingç»“æœè¿›è¡Œå¯è§†åŒ–å±•ç¤ºï¼Œä¾¿äºå¯¹å…¶ç›´è§‚åˆ†æï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "\n",
    "1. ç”±äºAI Studioå½“å‰æ”¯æŒçš„æ˜¯VisualDL 2.1ç‰ˆæœ¬ï¼Œå› æ­¤éœ€è¦å‡çº§åˆ°2.2ç‰ˆæœ¬ä½“éªŒæœ€æ–°çš„æ•°æ®é™ç»´åŠŸèƒ½\n",
    "\n",
    "`pip install --upgrade visualdl`\n",
    "\n",
    "2. åˆ›å»ºLogWriterå¹¶å°†è®°å½•è¯å‘é‡\n",
    "3. ç‚¹å‡»å·¦ä¾§é¢æ¿ä¸­çš„å¯è§†åŒ–tabï¼Œé€‰æ‹©â€˜hidiâ€™ä½œä¸ºæ–‡ä»¶å¹¶å¯åŠ¨VisualDLå¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade visualdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# è·å–è¯è¡¨ä¸­å‰1000ä¸ªå•è¯\n",
    "labels = token_embedding.vocab.to_tokens(list(range(0,1000)))\n",
    "test_token_embedding = token_embedding.search(labels)\n",
    "\n",
    "# å¼•å…¥VisualDLçš„LogWriterè®°å½•æ—¥å¿—\n",
    "from visualdl import LogWriter\n",
    "\n",
    "with LogWriter(logdir='./hidi') as writer:\n",
    "    #writer.add_embeddings(tag='test', mat=test_token_embedding, metadata=labels)\n",
    "    writer.add_embeddings(tag='test', mat=[i for i in test_token_embedding], metadata=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### å¯åŠ¨VisualDLæŸ¥çœ‹è¯å‘é‡é™ç»´æ•ˆæœ\n",
    "å¯åŠ¨æ­¥éª¤ï¼š\n",
    "- 1ã€åˆ‡æ¢åˆ°ã€Œå¯è§†åŒ–ã€æŒ‡å®šå¯è§†åŒ–æ—¥å¿—\n",
    "- 2ã€æ—¥å¿—æ–‡ä»¶é€‰æ‹© 'hidi'\n",
    "- 3ã€ç‚¹å‡»ã€Œå¯åŠ¨VisualDLã€åç‚¹å‡»ã€Œæ‰“å¼€VisualDLã€ï¼Œé€‰æ‹©ã€Œé«˜ç»´æ•°æ®æ˜ å°„ã€ï¼Œå³å¯æŸ¥çœ‹è¯è¡¨ä¸­å‰1000è¯UMAPæ–¹æ³•ä¸‹æ˜ å°„åˆ°ä¸‰ç»´ç©ºé—´çš„å¯è§†åŒ–ç»“æœ:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://user-images.githubusercontent.com/48054808/120594172-1fe02b00-c473-11eb-9df1-c0206b07e948.gif)\n",
    "\n",
    "å¯ä»¥çœ‹å‡ºï¼Œè¯­ä¹‰ç›¸è¿‘çš„è¯åœ¨è¯å‘é‡ç©ºé—´ä¸­èšé›†(å¦‚æ•°å­—ã€ç« èŠ‚ç­‰)ï¼Œè¯´æ˜é¢„è®­ç»ƒå¥½çš„è¯å‘é‡æœ‰å¾ˆå¥½çš„æ–‡æœ¬è¡¨ç¤ºèƒ½åŠ›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "ä½¿ç”¨VisualDLé™¤å¯è§†åŒ–embeddingç»“æœå¤–ï¼Œè¿˜å¯ä»¥å¯¹æ ‡é‡ã€å›¾ç‰‡ã€éŸ³é¢‘ç­‰è¿›è¡Œå¯è§†åŒ–ï¼Œæœ‰æ•ˆæå‡è®­ç»ƒè°ƒå‚æ•ˆç‡ã€‚å…³äºVisualDLæ›´å¤šåŠŸèƒ½å’Œè¯¦ç»†ä»‹ç»ï¼Œå¯å‚è€ƒ[VisualDLä½¿ç”¨æ–‡æ¡£](https://github.com/PaddlePaddle/VisualDL/tree/develop/docs)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# å››ã€æ–‡æœ¬åˆ†ç±»ä»»åŠ¡\n",
    "ä»¥ä¸‹é€šè¿‡æ–‡æœ¬åˆ†ç±»è®­ç»ƒçš„ä¾‹å­å±•ç¤ºpaddlenlp.embeddings.TokenEmbeddingå¯¹è®­ç»ƒæå‡çš„æ•ˆæœã€‚\n",
    "## 1.æ•°æ®å‡†å¤‡\n",
    "### 1.1ä¸‹è½½è¯æ±‡è¡¨æ–‡ä»¶dict.txtï¼Œç”¨äºæ„é€ è¯-idæ˜ å°„å…³ç³»ã€‚\n",
    "\n",
    "data.py\n",
    "```\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "import jieba\n",
    "tokenizer = jieba\n",
    "\n",
    "\n",
    "def set_tokenizer(vocab):\n",
    "    global tokenizer\n",
    "    if vocab is not None:\n",
    "        tokenizer = JiebaTokenizer(vocab=vocab)\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = {}\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\").split(\"\\t\")[0]\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def convert_tokens_to_ids(tokens, vocab):\n",
    "    \"\"\" Converts a token id (or a sequence of id) in a token string\n",
    "        (or a sequence of tokens), using the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    ids = []\n",
    "    unk_id = vocab.get('[UNK]', None)\n",
    "    for token in tokens:\n",
    "        wid = vocab.get(token, unk_id)\n",
    "        if wid:\n",
    "            ids.append(wid)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def convert_example(example, vocab, unk_token_id=1, is_test=False):\n",
    "    \"\"\"\n",
    "    Builds model inputs from a sequence for sequence classification tasks. \n",
    "    It use `jieba.cut` to tokenize text.\n",
    "    Args:\n",
    "        example(obj:`list[str]`): List of input data, containing text and label if it have label.\n",
    "        vocab(obj:`dict`): The vocabulary.\n",
    "        unk_token_id(obj:`int`, defaults to 1): The unknown token id.\n",
    "        is_test(obj:`False`, defaults to `False`): Whether the example contains label or not.\n",
    "    Returns:\n",
    "        input_ids(obj:`list[int]`): The list of token ids.s\n",
    "        valid_length(obj:`int`): The input sequence valid length.\n",
    "        label(obj:`numpy.array`, data type of int64, optional): The input label if not is_test.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    for token in tokenizer.cut(example['text']):\n",
    "        token_id = vocab.get(token, unk_token_id)\n",
    "        input_ids.append(token_id)\n",
    "    valid_length = len(input_ids)\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array(example['label'], dtype=\"int64\")\n",
    "        return input_ids, valid_length, label\n",
    "    else:\n",
    "        return input_ids, valid_length\n",
    "\n",
    "\n",
    "def pad_texts_to_max_seq_len(texts, max_seq_len, pad_token_id=0):\n",
    "    \"\"\"\n",
    "    Padded the texts to the max sequence length if the length of text is lower than it.\n",
    "    Unless it truncates the text.\n",
    "    Args:\n",
    "        texts(obj:`list`): Texts which contrains a sequence of word ids.\n",
    "        max_seq_len(obj:`int`): Max sequence length.\n",
    "        pad_token_id(obj:`int`, optinal, defaults to 0) : The pad token index.\n",
    "    \"\"\"\n",
    "    for index, text in enumerate(texts):\n",
    "        seq_len = len(text)\n",
    "        if seq_len < max_seq_len:\n",
    "            padded_tokens = [pad_token_id for _ in range(max_seq_len - seq_len)]\n",
    "            new_text = text + padded_tokens\n",
    "            texts[index] = new_text\n",
    "        elif seq_len > max_seq_len:\n",
    "            new_text = text[:max_seq_len]\n",
    "            texts[index] = new_text\n",
    "\n",
    "def preprocess_prediction_data(data, vocab):\n",
    "    \"\"\"\n",
    "    It process the prediction data as the format used as training.\n",
    "    Args:\n",
    "        data (obj:`List[str]`): The prediction data whose each element is  a tokenized text.\n",
    "    Returns:\n",
    "        examples (obj:`List(Example)`): The processed data whose each element is a Example (numedtuple) object.\n",
    "            A Example object contains `text`(word_ids) and `se_len`(sequence length).\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        tokens = \" \".join(tokenizer.cut(text)).split(' ')\n",
    "        ids = convert_tokens_to_ids(tokens, vocab)\n",
    "        examples.append([ids, len(ids)])\n",
    "    return examples\n",
    "\n",
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1,\n",
    "                      use_gpu=False,\n",
    "                      pad_token_id=0):\n",
    "    \"\"\"\n",
    "    Creats dataloader.\n",
    "    Args:\n",
    "        dataset(obj:`paddle.io.Dataset`): Dataset instance.\n",
    "        mode(obj:`str`, optional, defaults to obj:`train`): If mode is 'train', it will shuffle the dataset randomly.\n",
    "        batch_size(obj:`int`, optional, defaults to 1): The sample number of a mini-batch.\n",
    "        use_gpu(obj:`bool`, optional, defaults to obj:`False`): Whether to use gpu to run.\n",
    "        pad_token_id(obj:`int`, optional, defaults to 0): The pad token index.\n",
    "    Returns:\n",
    "        dataloader(obj:`paddle.io.DataLoader`): The dataloader which generates batches.\n",
    "    \"\"\"\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn, lazy=True)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    sampler = paddle.io.BatchSampler(\n",
    "        dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=pad_token_id),  # input_ids\n",
    "        Stack(dtype=\"int64\"),  # seq len\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=sampler,\n",
    "        return_list=True,\n",
    "        collate_fn=batchify_fn)\n",
    "    return dataloader\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget https://paddlenlp.bj.bcebos.com/data/dict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 åŠ è½½è¯è¡¨å’Œæ•°æ®é›†ï¼Œæ•°æ®é›†æ¥è‡ªPaddleNLPå†…ç½®çš„å…¬å¼€ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†ChnSenticorpï¼Œä¸€é”®å³å¯åŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1909/1909 [00:00<00:00, 53805.65it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loads vocab.\n",
    "vocab_path='./dict.txt'  \n",
    "vocab = data.load_vocab(vocab_path)\n",
    "if '[PAD]' not in vocab:\n",
    "    vocab['[PAD]'] = len(vocab)\n",
    "# Loads dataset.\n",
    "train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3 åˆ›å»ºè¿è¡Œå’Œé¢„æµ‹æ—¶æ‰€éœ€è¦çš„DataLoaderå¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reads data and generates mini-batches.\n",
    "trans_fn = partial(\n",
    "    data.convert_example,\n",
    "    vocab=vocab,\n",
    "    unk_token_id=vocab['[UNK]'],\n",
    "    is_test=False)\n",
    "train_loader = data.create_dataloader(\n",
    "    train_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=64,\n",
    "    mode='train',\n",
    "    pad_token_id=vocab['[PAD]'])\n",
    "dev_loader = data.create_dataloader(\n",
    "    dev_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=64,\n",
    "    mode='validation',\n",
    "    pad_token_id=vocab['[PAD]'])\n",
    "test_loader = data.create_dataloader(\n",
    "    test_ds,\n",
    "    trans_fn=trans_fn,\n",
    "    batch_size=64,\n",
    "    mode='test',\n",
    "    pad_token_id=vocab['[PAD]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.æ¨¡å‹æ­å»º\n",
    "ä½¿ç”¨`BOWencoder`æ­å»ºä¸€ä¸ªBOWæ¨¡å‹ç”¨äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚`BOWencoder`è¾“å…¥åºåˆ—çš„å…¨éƒ¨å‘é‡ï¼Œè¾“å‡ºä¸€ä¸ªåºåˆ—å‘é‡çš„ç®€å•åŠ å’Œåçš„å‘é‡æ¥è¡¨ç¤ºåºåˆ—è¯­ä¹‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BoWModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 vocab_path,\n",
    "                 emb_dim=300,\n",
    "                 hidden_size=128,\n",
    "                 fc_hidden_size=96,\n",
    "                 use_token_embedding=True):\n",
    "        super().__init__()\n",
    "        if use_token_embedding:\n",
    "            self.embedder = TokenEmbedding(\n",
    "                \"w2v.baidu_encyclopedia.target.word-word.dim300\", extended_vocab_path=vocab_path)\n",
    "            emb_dim = self.embedder.embedding_dim\n",
    "        else:\n",
    "            padding_idx = vocab_size - 1\n",
    "            self.embedder = nn.Embedding(\n",
    "                vocab_size, emb_dim, padding_idx=padding_idx)\n",
    "        self.bow_encoder = paddlenlp.seq2vec.BoWEncoder(emb_dim)\n",
    "        self.fc1 = nn.Linear(self.bow_encoder.get_output_dim(), hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, fc_hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.3, axis=1)\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, text, seq_len=None):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "\n",
    "        # Shape: (batch_size, embedding_dim)\n",
    "        summed = self.bow_encoder(embedded_text)\n",
    "        summed = self.dropout(summed)\n",
    "        encoded_text = paddle.tanh(summed)\n",
    "\n",
    "        # Shape: (batch_size, hidden_size)\n",
    "        fc1_out = paddle.tanh(self.fc1(encoded_text))\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc2_out = paddle.tanh(self.fc2(fc1_out))\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc2_out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1è·å¾—æ ·æœ¬æ•°æ®ç±»åˆ«æ ‡ç­¾æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_classes = len(train_ds.label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 å®šä¹‰æ¨¡å‹\n",
    "å®šä¹‰ä¸€ä¸ªä½¿ç”¨é¢„è®­ç»ƒå¥½çš„Tokenembeddingçš„BOWæ¨¡å‹ï¼Œå¯é€šè¿‡è®¾ç½®è¶…å‚æ•°use_token_embedding=Falseæ¥å®šä¹‰ä¸€ä¸ªéšæœºåˆå§‹åŒ–embeddingå‘é‡çš„BOWæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-06 00:19:57,125] [    INFO] - Loading token embedding...\n",
      "[2021-06-06 00:19:58,623] [    INFO] - Start extending vocab.\n",
      "[2021-06-06 00:20:08,291] [    INFO] - Finish extending vocab.\n",
      "[2021-06-06 00:20:10,966] [    INFO] - Finish loading embedding vector.\n",
      "[2021-06-06 00:20:10,969] [    INFO] - Token Embedding info:             \n",
      "Unknown index: 0             \n",
      "Unknown token: [UNK]             \n",
      "Padding index: 750906             \n",
      "Padding token: [PAD]             \n",
      "Shape :[750907, 300]\n"
     ]
    }
   ],
   "source": [
    "use_token_embedding=True\n",
    "learning_rate=5e-4\n",
    "\n",
    "model = BoWModel(\n",
    "    vocab_size=len(vocab),\n",
    "    num_classes=num_classes,\n",
    "    vocab_path=vocab_path,\n",
    "    use_token_embedding=use_token_embedding)\n",
    "if use_token_embedding:\n",
    "    vocab = model.embedder.vocab\n",
    "    data.set_tokenizer(vocab)\n",
    "    vocab = vocab.token_to_idx\n",
    "else:\n",
    "    v = Vocab.from_dict(vocab, unk_token=\"[UNK]\", pad_token=\"[PAD]\")\n",
    "    data.set_tokenizer(v)\n",
    "\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3æ¨¡å‹é…ç½®\n",
    "è°ƒç”¨`model.prepare()`é…ç½®æ¨¡å‹ï¼Œå¦‚æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=learning_rate)\n",
    "\n",
    "# Defines loss and metric.\n",
    "criterion = paddle.nn.CrossEntropyLoss()\n",
    "metric = paddle.metric.Accuracy()\n",
    "\n",
    "model.prepare(optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.æ¨¡å‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.1 ä½¿ç”¨VisualDLè§‚æµ‹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ¨¡å‹ç»“æœ\n",
    "Paddle2.0ä¸­åˆ©ç”¨model.prepare(), model.fit()ç­‰é«˜å±‚æ¥å£å¯ä»¥è½»æ¾å®ç°è®­ç»ƒæ¨¡å‹çš„ä»£ç éƒ¨åˆ†ï¼ŒåŒæ—¶åœ¨Paddle.Model()æ¥å£ä¸­æä¾›äº†Callbackç±»ï¼Œè¿™é‡Œé€šè¿‡VisualDLå¯è§†åŒ–æ¥ç›´è§‚çš„å¯¹æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å’Œæ¨¡å‹ç»“æœè¿›è¡Œè¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# è®¾ç½®VisualDLè·¯å¾„\n",
    "log_dir = './use_normal_embedding'\n",
    "if use_token_embedding:\n",
    "    log_dir = './use_token_embedding'\n",
    "callback = paddle.callbacks.VisualDL(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.2 è°ƒç”¨`model.fit()`ä¸€é”®è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_loader,\n",
    "    dev_loader,\n",
    "    epochs=20,\n",
    "    save_dir='./checkpoints',\n",
    "    callbacks=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "Epoch 20/20\n",
    "step  10/150 - loss: 0.2043 - acc: 0.9187 - 63ms/step\n",
    "step  20/150 - loss: 0.0673 - acc: 0.9375 - 56ms/step\n",
    "step  30/150 - loss: 0.0824 - acc: 0.9510 - 54ms/step\n",
    "step  40/150 - loss: 0.1066 - acc: 0.9496 - 53ms/step\n",
    "step  50/150 - loss: 0.1181 - acc: 0.9531 - 52ms/step\n",
    "step  60/150 - loss: 0.0444 - acc: 0.9560 - 52ms/step\n",
    "step  70/150 - loss: 0.0328 - acc: 0.9567 - 51ms/step\n",
    "step  80/150 - loss: 0.2206 - acc: 0.9574 - 51ms/step\n",
    "step  90/150 - loss: 0.1328 - acc: 0.9580 - 51ms/step\n",
    "step 100/150 - loss: 0.0814 - acc: 0.9563 - 51ms/step\n",
    "step 110/150 - loss: 0.0616 - acc: 0.9572 - 51ms/step\n",
    "step 120/150 - loss: 0.1595 - acc: 0.9573 - 51ms/step\n",
    "step 130/150 - loss: 0.0690 - acc: 0.9556 - 51ms/step\n",
    "step 140/150 - loss: 0.1511 - acc: 0.9561 - 51ms/step\n",
    "step 150/150 - loss: 0.0406 - acc: 0.9563 - 50ms/step\n",
    "save checkpoint at /home/aistudio/checkpoints/19\n",
    "Eval begin...\n",
    "step 10/19 - loss: 0.5991 - acc: 0.8922 - 56ms/step\n",
    "step 19/19 - loss: 0.5061 - acc: 0.8867 - 37ms/step\n",
    "Eval samples: 1200\n",
    "save checkpoint at /home/aistudio/checkpoints/final\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.è®­ç»ƒç»“æœ\n",
    "å¯åŠ¨VisualDLæ­¥éª¤åŒä¸Šï¼Œæ—¥å¿—æ–‡ä»¶é€‰æ‹© 'use_token_embedding'å’Œ'use_normal_embedding'è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–ç»“æœå¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<div align=\"center\">\n",
    "\t<img src=\"https://ai-studio-static-online.cdn.bcebos.com/290af9c8fc2442c1b29e94b6232926e3922f6669bb7944409014299a807cdb8c\" width=\"95%\">\n",
    "</div>                                                                                                                                     \n",
    "å›¾ä¸­ç»¿è‰²æ˜¯ä½¿ç”¨paddlenlp.embeddings.TokenEmbeddingè¿›è¡Œçš„å®éªŒï¼Œè“è‰²æ˜¯ä½¿ç”¨æ²¡æœ‰åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„Embeddingè¿›è¡Œçš„å®éªŒã€‚å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨paddlenlp.embeddings.TokenEmbeddingçš„è®­ç»ƒï¼Œå…¶éªŒè¯accå˜åŒ–è¶‹åŠ¿ä¸Šå‡ï¼Œå¹¶æ”¶æ•›äº0.90å·¦å³ï¼Œlosså˜åŒ–è¶‹åŠ¿ä¸‹é™ï¼Œæ”¶æ•›åç›¸å¯¹å¹³ç¨³ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆã€‚è€Œæ²¡æœ‰ä½¿ç”¨paddlenlp.embeddings.TokenEmbeddingçš„è®­ç»ƒï¼Œå…¶éªŒè¯accå˜åŒ–è¶‹åŠ¿å‘ä¸‹ï¼Œå¹¶æ”¶æ•›äº0.88å·¦å³ã€‚ä»ç¤ºä¾‹å®éªŒå¯ä»¥è§‚å¯Ÿåˆ°ï¼Œä½¿ç”¨paddlenlp.embedding.TokenEmbeddingèƒ½æå‡è®­ç»ƒæ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# äº”ã€åˆ‡è¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ä¸­å›½äºº', 'æ°‘']\n",
      "[12530, 1334]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.data import JiebaTokenizer\r\n",
    "tokenizer = JiebaTokenizer(vocab=token_embedding.vocab)\r\n",
    "words = tokenizer.cut(\"ä¸­å›½äººæ°‘\")\r\n",
    "print(words) # ['ä¸­å›½äºº', 'æ°‘']\r\n",
    "\r\n",
    "tokens = tokenizer.encode(\"ä¸­å›½äººæ°‘\")\r\n",
    "print(tokens) # [12530, 1334]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# å…­ã€embedding constant.pyæºç åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. 62ç§embedding name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "è¯¥æ–‡ä»¶ä¸­å¯è§Embedding name listï¼Œä¸€å †ä¸€å †çš„åç§°ï¼Œå¥½å¤šå•Šå•Šå•Šã€‚æˆ‘æ•°æ•°å…ˆ\n",
    "\n",
    "```\n",
    "from enum import Enum\n",
    "import os.path as osp\n",
    "\n",
    "URL_ROOT = \"https://paddlenlp.bj.bcebos.com\"\n",
    "EMBEDDING_URL_ROOT = URL_ROOT + \"/models/embeddings\"\n",
    "\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "\n",
    "EMBEDDING_NAME_LIST = [\n",
    "    # Word2Vec\n",
    "    # baidu_encyclopedia\n",
    "    \"w2v.baidu_encyclopedia.target.word-word.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-character.char1-1.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-character.char1-2.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-character.char1-4.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-ngram.1-2.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-ngram.1-3.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-ngram.2-2.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-wordLR.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.word-wordPosition.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.target.bigram-char.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-word.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-character.char1-1.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-character.char1-2.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-character.char1-4.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-ngram.1-2.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-ngram.1-3.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-ngram.2-2.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-wordLR.dim300\",\n",
    "    \"w2v.baidu_encyclopedia.context.word-wordPosition.dim300\",\n",
    "    # wikipedia\n",
    "    \"w2v.wiki.target.bigram-char.dim300\",\n",
    "    \"w2v.wiki.target.word-char.dim300\",\n",
    "    \"w2v.wiki.target.word-word.dim300\",\n",
    "    \"w2v.wiki.target.word-bigram.dim300\",\n",
    "    # people_daily\n",
    "    \"w2v.people_daily.target.bigram-char.dim300\",\n",
    "    \"w2v.people_daily.target.word-char.dim300\",\n",
    "    \"w2v.people_daily.target.word-word.dim300\",\n",
    "    \"w2v.people_daily.target.word-bigram.dim300\",\n",
    "    # weibo\n",
    "    \"w2v.weibo.target.bigram-char.dim300\",\n",
    "    \"w2v.weibo.target.word-char.dim300\",\n",
    "    \"w2v.weibo.target.word-word.dim300\",\n",
    "    \"w2v.weibo.target.word-bigram.dim300\",\n",
    "    # sogou\n",
    "    \"w2v.sogou.target.bigram-char.dim300\",\n",
    "    \"w2v.sogou.target.word-char.dim300\",\n",
    "    \"w2v.sogou.target.word-word.dim300\",\n",
    "    \"w2v.sogou.target.word-bigram.dim300\",\n",
    "    # zhihu\n",
    "    \"w2v.zhihu.target.bigram-char.dim300\",\n",
    "    \"w2v.zhihu.target.word-char.dim300\",\n",
    "    \"w2v.zhihu.target.word-word.dim300\",\n",
    "    \"w2v.zhihu.target.word-bigram.dim300\",\n",
    "    # finacial\n",
    "    \"w2v.financial.target.bigram-char.dim300\",\n",
    "    \"w2v.financial.target.word-char.dim300\",\n",
    "    \"w2v.financial.target.word-word.dim300\",\n",
    "    \"w2v.financial.target.word-bigram.dim300\",\n",
    "    # literature\n",
    "    \"w2v.literature.target.bigram-char.dim300\",\n",
    "    \"w2v.literature.target.word-char.dim300\",\n",
    "    \"w2v.literature.target.word-word.dim300\",\n",
    "    \"w2v.literature.target.word-bigram.dim300\",\n",
    "    # siku\n",
    "    \"w2v.sikuquanshu.target.word-word.dim300\",\n",
    "    \"w2v.sikuquanshu.target.word-bigram.dim300\",\n",
    "    # Mix-large\n",
    "    \"w2v.mixed-large.target.word-char.dim300\",\n",
    "    \"w2v.mixed-large.target.word-word.dim300\",\n",
    "    # GOOGLE NEWS\n",
    "    \"w2v.google_news.target.word-word.dim300.en\",\n",
    "    # GloVe\n",
    "    \"glove.wiki2014-gigaword.target.word-word.dim50.en\",\n",
    "    \"glove.wiki2014-gigaword.target.word-word.dim100.en\",\n",
    "    \"glove.wiki2014-gigaword.target.word-word.dim200.en\",\n",
    "    \"glove.wiki2014-gigaword.target.word-word.dim300.en\",\n",
    "    \"glove.twitter.target.word-word.dim25.en\",\n",
    "    \"glove.twitter.target.word-word.dim50.en\",\n",
    "    \"glove.twitter.target.word-word.dim100.en\",\n",
    "    \"glove.twitter.target.word-word.dim200.en\",\n",
    "    # FastText\n",
    "    \"fasttext.wiki-news.target.word-word.dim300.en\",\n",
    "    \"fasttext.crawl.target.word-word.dim300.en\"\n",
    "]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "w2v.baidu_encyclopedia.target.word-word.dim300\n",
      "w2v.baidu_encyclopedia.target.word-character.char1-1.dim300\n",
      "w2v.baidu_encyclopedia.target.word-character.char1-2.dim300\n",
      "w2v.baidu_encyclopedia.target.word-character.char1-4.dim300\n",
      "w2v.baidu_encyclopedia.target.word-ngram.1-2.dim300\n",
      "w2v.baidu_encyclopedia.target.word-ngram.1-3.dim300\n",
      "w2v.baidu_encyclopedia.target.word-ngram.2-2.dim300\n",
      "w2v.baidu_encyclopedia.target.word-wordLR.dim300\n",
      "w2v.baidu_encyclopedia.target.word-wordPosition.dim300\n",
      "w2v.baidu_encyclopedia.target.bigram-char.dim300\n",
      "w2v.baidu_encyclopedia.context.word-word.dim300\n",
      "w2v.baidu_encyclopedia.context.word-character.char1-1.dim300\n",
      "w2v.baidu_encyclopedia.context.word-character.char1-2.dim300\n",
      "w2v.baidu_encyclopedia.context.word-character.char1-4.dim300\n",
      "w2v.baidu_encyclopedia.context.word-ngram.1-2.dim300\n",
      "w2v.baidu_encyclopedia.context.word-ngram.1-3.dim300\n",
      "w2v.baidu_encyclopedia.context.word-ngram.2-2.dim300\n",
      "w2v.baidu_encyclopedia.context.word-wordLR.dim300\n",
      "w2v.baidu_encyclopedia.context.word-wordPosition.dim300\n",
      "w2v.wiki.target.bigram-char.dim300\n",
      "w2v.wiki.target.word-char.dim300\n",
      "w2v.wiki.target.word-word.dim300\n",
      "w2v.wiki.target.word-bigram.dim300\n",
      "w2v.people_daily.target.bigram-char.dim300\n",
      "w2v.people_daily.target.word-char.dim300\n",
      "w2v.people_daily.target.word-word.dim300\n",
      "w2v.people_daily.target.word-bigram.dim300\n",
      "w2v.weibo.target.bigram-char.dim300\n",
      "w2v.weibo.target.word-char.dim300\n",
      "w2v.weibo.target.word-word.dim300\n",
      "w2v.weibo.target.word-bigram.dim300\n",
      "w2v.sogou.target.bigram-char.dim300\n",
      "w2v.sogou.target.word-char.dim300\n",
      "w2v.sogou.target.word-word.dim300\n",
      "w2v.sogou.target.word-bigram.dim300\n",
      "w2v.zhihu.target.bigram-char.dim300\n",
      "w2v.zhihu.target.word-char.dim300\n",
      "w2v.zhihu.target.word-word.dim300\n",
      "w2v.zhihu.target.word-bigram.dim300\n",
      "w2v.financial.target.bigram-char.dim300\n",
      "w2v.financial.target.word-char.dim300\n",
      "w2v.financial.target.word-word.dim300\n",
      "w2v.financial.target.word-bigram.dim300\n",
      "w2v.literature.target.bigram-char.dim300\n",
      "w2v.literature.target.word-char.dim300\n",
      "w2v.literature.target.word-word.dim300\n",
      "w2v.literature.target.word-bigram.dim300\n",
      "w2v.sikuquanshu.target.word-word.dim300\n",
      "w2v.sikuquanshu.target.word-bigram.dim300\n",
      "w2v.mixed-large.target.word-char.dim300\n",
      "w2v.mixed-large.target.word-word.dim300\n",
      "w2v.google_news.target.word-word.dim300.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim50.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim100.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim200.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim300.en\n",
      "glove.twitter.target.word-word.dim25.en\n",
      "glove.twitter.target.word-word.dim50.en\n",
      "glove.twitter.target.word-word.dim100.en\n",
      "glove.twitter.target.word-word.dim200.en\n",
      "fasttext.wiki-news.target.word-word.dim300.en\n",
      "fasttext.crawl.target.word-word.dim300.en\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.embeddings import constant\r\n",
    "\r\n",
    "print(len(constant.EMBEDDING_NAME_LIST))\r\n",
    "for item in constant.EMBEDDING_NAME_LIST:\r\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.token_embedding\n",
    "\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "import os\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.utils.download import get_path_from_url\n",
    "from paddlenlp.utils.env import _get_sub_home, MODEL_HOME\n",
    "from paddlenlp.utils.log import logger\n",
    "from paddlenlp.data import Vocab, get_idx_from_word\n",
    "from .constant import EMBEDDING_URL_ROOT, PAD_TOKEN, UNK_TOKEN,\\\n",
    "                      EMBEDDING_NAME_LIST\n",
    "\n",
    "EMBEDDING_HOME = _get_sub_home('embeddings', parent_home=MODEL_HOME)\n",
    "\n",
    "__all__ = ['list_embedding_name', 'TokenEmbedding']\n",
    "\n",
    "\n",
    "def list_embedding_name():\n",
    "    \"\"\"\n",
    "    Lists all names of pretrained embedding models paddlenlp provides.\n",
    "    \"\"\"\n",
    "    return list(EMBEDDING_NAME_LIST)\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    A `TokenEmbedding` can load pre-trained embedding model which paddlenlp provides by\n",
    "    specifying embedding name. Furthermore, a `TokenEmbedding` can load extended vocabulary\n",
    "    by specifying extended_vocab_path.\n",
    "\n",
    "    Args:\n",
    "        embedding_name (`str`, optional):\n",
    "            The pre-trained embedding model name. Use `paddlenlp.embeddings.list_embedding_name()` to\n",
    "            list the names of all embedding models that we provide. \n",
    "            Defaults to `w2v.baidu_encyclopedia.target.word-word.dim300`.\n",
    "        unknown_token (`str`, optional):\n",
    "            Specifies unknown token.\n",
    "            Defaults to `[UNK]`.\n",
    "        unknown_token_vector (`list`, optional):\n",
    "            To initialize the vector of unknown token. If it's none, use normal distribution to\n",
    "            initialize the vector of unknown token.\n",
    "            Defaults to `None`.\n",
    "        extended_vocab_path (`str`, optional):\n",
    "            The file path of extended vocabulary.\n",
    "            Defaults to `None`.\n",
    "        trainable (`bool`, optional):\n",
    "            Whether the weight of embedding can be trained.\n",
    "            Defaults to True.\n",
    "        keep_extended_vocab_only (`bool`, optional):\n",
    "            Whether to keep the extended vocabulary only, will be effective only if provides extended_vocab_path.\n",
    "            Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_name=EMBEDDING_NAME_LIST[0],\n",
    "                 unknown_token=UNK_TOKEN,\n",
    "                 unknown_token_vector=None,\n",
    "                 extended_vocab_path=None,\n",
    "                 trainable=True,\n",
    "                 keep_extended_vocab_only=False):\n",
    "        vector_path = osp.join(EMBEDDING_HOME, embedding_name + \".npz\")\n",
    "        if not osp.exists(vector_path):\n",
    "            # download\n",
    "            url = EMBEDDING_URL_ROOT + \"/\" + embedding_name + \".tar.gz\"\n",
    "            get_path_from_url(url, EMBEDDING_HOME)\n",
    "\n",
    "        logger.info(\"Loading token embedding...\")\n",
    "        vector_np = np.load(vector_path)\n",
    "        self.embedding_dim = vector_np['embedding'].shape[1]\n",
    "        self.unknown_token = unknown_token\n",
    "        if unknown_token_vector is not None:\n",
    "            unk_vector = np.array(unknown_token_vector).astype(\n",
    "                paddle.get_default_dtype())\n",
    "        else:\n",
    "            unk_vector = np.random.normal(\n",
    "                scale=0.02,\n",
    "                size=self.embedding_dim).astype(paddle.get_default_dtype())\n",
    "        pad_vector = np.array(\n",
    "            [0] * self.embedding_dim).astype(paddle.get_default_dtype())\n",
    "        if extended_vocab_path is not None:\n",
    "            embedding_table = self._extend_vocab(extended_vocab_path, vector_np,\n",
    "                                                 pad_vector, unk_vector,\n",
    "                                                 keep_extended_vocab_only)\n",
    "            trainable = True\n",
    "        else:\n",
    "            embedding_table = self._init_without_extend_vocab(\n",
    "                vector_np, pad_vector, unk_vector)\n",
    "\n",
    "        self.vocab = Vocab.from_dict(\n",
    "            self._word_to_idx, unk_token=unknown_token, pad_token=PAD_TOKEN)\n",
    "        self.num_embeddings = embedding_table.shape[0]\n",
    "        # import embedding\n",
    "        super(TokenEmbedding, self).__init__(\n",
    "            self.num_embeddings,\n",
    "            self.embedding_dim,\n",
    "            padding_idx=self._word_to_idx[PAD_TOKEN])\n",
    "        self.weight.set_value(embedding_table)\n",
    "        self.set_trainable(trainable)\n",
    "        logger.info(\"Finish loading embedding vector.\")\n",
    "        s = \"Token Embedding info:\\\n",
    "             \\nUnknown index: {}\\\n",
    "             \\nUnknown token: {}\\\n",
    "             \\nPadding index: {}\\\n",
    "             \\nPadding token: {}\\\n",
    "             \\nShape :{}\".format(\n",
    "            self._word_to_idx[self.unknown_token], self.unknown_token,\n",
    "            self._word_to_idx[PAD_TOKEN], PAD_TOKEN, self.weight.shape)\n",
    "        logger.info(s)\n",
    "\n",
    "    def _init_without_extend_vocab(self, vector_np, pad_vector, unk_vector):\n",
    "        \"\"\"\n",
    "        Constructs index to word list, word to index dict and embedding weight.\n",
    "        \"\"\"\n",
    "        self._idx_to_word = list(vector_np['vocab'])\n",
    "        self._idx_to_word.append(self.unknown_token)\n",
    "        self._idx_to_word.append(PAD_TOKEN)\n",
    "        self._word_to_idx = self._construct_word_to_idx(self._idx_to_word)\n",
    "        # insert unk, pad embedding\n",
    "        embedding_table = np.append(\n",
    "            vector_np['embedding'], [unk_vector, pad_vector], axis=0)\n",
    "\n",
    "        return embedding_table\n",
    "\n",
    "    def _read_vocab_list_from_file(self, extended_vocab_path):\n",
    "        # load new vocab table from file\n",
    "        vocab_list = []\n",
    "        with open(extended_vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f.readlines():\n",
    "                vocab = line.rstrip(\"\\n\").split(\"\\t\")[0]\n",
    "                vocab_list.append(vocab)\n",
    "        return vocab_list\n",
    "\n",
    "    def _extend_vocab(self, extended_vocab_path, vector_np, pad_vector,\n",
    "                      unk_vector, keep_extended_vocab_only):\n",
    "        \"\"\"\n",
    "        Constructs index to word list, word to index dict and embedding weight using\n",
    "        extended vocab.\n",
    "        \"\"\"\n",
    "        logger.info(\"Start extending vocab.\")\n",
    "        extend_vocab_list = self._read_vocab_list_from_file(extended_vocab_path)\n",
    "        extend_vocab_set = set(extend_vocab_list)\n",
    "        # update idx_to_word\n",
    "        self._idx_to_word = extend_vocab_list\n",
    "        self._word_to_idx = self._construct_word_to_idx(self._idx_to_word)\n",
    "\n",
    "        # use the Xavier init the embedding\n",
    "        xavier_scale = np.sqrt(\n",
    "            6.0 / float(len(self._idx_to_word) + self.embedding_dim))\n",
    "        embedding_table = np.random.uniform(\n",
    "            low=-1.0 * xavier_scale,\n",
    "            high=xavier_scale,\n",
    "            size=(len(self._idx_to_word),\n",
    "                  self.embedding_dim)).astype(paddle.get_default_dtype())\n",
    "\n",
    "        pretrained_idx_to_word = list(vector_np['vocab'])\n",
    "        pretrained_word_to_idx = self._construct_word_to_idx(\n",
    "            pretrained_idx_to_word)\n",
    "        pretrained_embedding_table = np.array(vector_np['embedding'])\n",
    "\n",
    "        pretrained_vocab_set = set(pretrained_idx_to_word)\n",
    "        extend_vocab_set = set(self._idx_to_word)\n",
    "        vocab_intersection = pretrained_vocab_set & extend_vocab_set\n",
    "        vocab_subtraction = pretrained_vocab_set - extend_vocab_set\n",
    "\n",
    "        # assignment from pretrained_vocab_embedding to extend_vocab_embedding\n",
    "        pretrained_vocab_intersect_index = [\n",
    "            pretrained_word_to_idx[word] for word in vocab_intersection\n",
    "        ]\n",
    "        pretrained_vocab_subtract_index = [\n",
    "            pretrained_word_to_idx[word] for word in vocab_subtraction\n",
    "        ]\n",
    "        extend_vocab_intersect_index = [\n",
    "            self._word_to_idx[word] for word in vocab_intersection\n",
    "        ]\n",
    "        embedding_table[\n",
    "            extend_vocab_intersect_index] = pretrained_embedding_table[\n",
    "                pretrained_vocab_intersect_index]\n",
    "        if not keep_extended_vocab_only:\n",
    "            for idx in pretrained_vocab_subtract_index:\n",
    "                word = pretrained_idx_to_word[idx]\n",
    "                self._idx_to_word.append(word)\n",
    "                self._word_to_idx[word] = len(self._idx_to_word) - 1\n",
    "\n",
    "            embedding_table = np.append(\n",
    "                embedding_table,\n",
    "                pretrained_embedding_table[pretrained_vocab_subtract_index],\n",
    "                axis=0)\n",
    "\n",
    "        if self.unknown_token not in extend_vocab_set:\n",
    "            self._idx_to_word.append(self.unknown_token)\n",
    "            self._word_to_idx[self.unknown_token] = len(self._idx_to_word) - 1\n",
    "            embedding_table = np.append(embedding_table, [unk_vector], axis=0)\n",
    "        else:\n",
    "            unk_idx = self._word_to_idx[self.unknown_token]\n",
    "            embedding_table[unk_idx] = unk_vector\n",
    "\n",
    "        if PAD_TOKEN not in extend_vocab_set:\n",
    "            self._idx_to_word.append(PAD_TOKEN)\n",
    "            self._word_to_idx[PAD_TOKEN] = len(self._idx_to_word) - 1\n",
    "            embedding_table = np.append(embedding_table, [pad_vector], axis=0)\n",
    "        else:\n",
    "            embedding_table[self._word_to_idx[PAD_TOKEN]] = pad_vector\n",
    "\n",
    "        logger.info(\"Finish extending vocab.\")\n",
    "        return embedding_table\n",
    "\n",
    "    def set_trainable(self, trainable):\n",
    "        \"\"\"\n",
    "        Whether or not to set the weights of token embedding to be trainable.\n",
    "\n",
    "        Args:\n",
    "            trainable (`bool`):\n",
    "                The weights can be trained if trainable is set to True, or the weights are fixed if trainable is False.\n",
    "\n",
    "        \"\"\"\n",
    "        self.weight.stop_gradient = not trainable\n",
    "\n",
    "    def search(self, words):\n",
    "        \"\"\"\n",
    "        Gets the vectors of specifying words.\n",
    "\n",
    "        Args:\n",
    "            words (`list` or `str` or `int`): The words which need to be searched.\n",
    "\n",
    "        Returns:\n",
    "            `numpy.array`: The vectors of specifying words.\n",
    "\n",
    "        \"\"\"\n",
    "        idx_list = self.get_idx_list_from_words(words)\n",
    "        idx_tensor = paddle.to_tensor(idx_list)\n",
    "        return self(idx_tensor).numpy()\n",
    "\n",
    "    def get_idx_from_word(self, word):\n",
    "        \"\"\"\n",
    "        Gets the index of specifying word by searching word_to_idx dict. \n",
    "\n",
    "        Args:\n",
    "            word (`list` or `str` or `int`): The input token word which we want to get the token index converted from.\n",
    "\n",
    "        Returns:\n",
    "            `int`: The index of specifying word.\n",
    "\n",
    "        \"\"\"\n",
    "        return get_idx_from_word(word, self.vocab.token_to_idx,\n",
    "                                 self.unknown_token)\n",
    "\n",
    "    def get_idx_list_from_words(self, words):\n",
    "        \"\"\"\n",
    "        Gets the index list of specifying words by searching word_to_idx dict.\n",
    "\n",
    "        Args:\n",
    "            words (`list` or `str` or `int`): The input token words which we want to get the token indices converted from.\n",
    "\n",
    "        Returns:\n",
    "            `list`: The indexes list of specifying words.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(words, str):\n",
    "            idx_list = [self.get_idx_from_word(words)]\n",
    "        elif isinstance(words, int):\n",
    "            idx_list = [words]\n",
    "        elif isinstance(words, list) or isinstance(words, tuple):\n",
    "            idx_list = [\n",
    "                self.get_idx_from_word(word) if isinstance(word, str) else word\n",
    "                for word in words\n",
    "            ]\n",
    "        else:\n",
    "            raise TypeError\n",
    "        return idx_list\n",
    "\n",
    "    def _dot_np(self, array_a, array_b):\n",
    "        return np.sum(array_a * array_b)\n",
    "\n",
    "    def _calc_word(self, word_a, word_b, calc_kernel):\n",
    "        embeddings = self.search([word_a, word_b])\n",
    "        embedding_a = embeddings[0]\n",
    "        embedding_b = embeddings[1]\n",
    "        return calc_kernel(embedding_a, embedding_b)\n",
    "\n",
    "    def dot(self, word_a, word_b):\n",
    "        \"\"\"\n",
    "        Calculates the dot product of 2 words. Dot product or scalar product is an\n",
    "        algebraic operation that takes two equal-length sequences of numbers (usually\n",
    "        coordinate vectors), and returns a single number.\n",
    "\n",
    "        Args:\n",
    "            word_a (`str`): The first word string.\n",
    "            word_b (`str`): The second word string.\n",
    "\n",
    "        Returns:\n",
    "            `Float`: The dot product of 2 words.\n",
    "\n",
    "        \"\"\"\n",
    "        dot = self._dot_np\n",
    "        return self._calc_word(word_a, word_b, lambda x, y: dot(x, y))\n",
    "\n",
    "    def cosine_sim(self, word_a, word_b):\n",
    "        \"\"\"\n",
    "        Calculates the cosine similarity of 2 word vectors. Cosine similarity is the\n",
    "        cosine of the angle between two n-dimensional vectors in an n-dimensional space.\n",
    "\n",
    "        Args:\n",
    "            word_a (`str`): The first word string.\n",
    "            word_b (`str`): The second word string.\n",
    "\n",
    "        Returns:\n",
    "            `Float`: The cosine similarity of 2 words.\n",
    "\n",
    "        \"\"\"\n",
    "        dot = self._dot_np\n",
    "        return self._calc_word(\n",
    "            word_a, word_b,\n",
    "            lambda x, y: dot(x, y) / (np.sqrt(dot(x, x)) * np.sqrt(dot(y, y))))\n",
    "\n",
    "    def _construct_word_to_idx(self, idx_to_word):\n",
    "        \"\"\"\n",
    "        Constructs word to index dict.\n",
    "\n",
    "        Args:\n",
    "            idx_to_word ('list'):\n",
    "\n",
    "        Returns:\n",
    "            `Dict`: The word to index dict constructed by idx_to_word.\n",
    "\n",
    "        \"\"\"\n",
    "        word_to_idx = {}\n",
    "        for i, word in enumerate(idx_to_word):\n",
    "            word_to_idx[word] = i\n",
    "        return word_to_idx\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            `Str`: The token embedding infomation.\n",
    "\n",
    "        \"\"\"\n",
    "        info = \"Object   type: {}\\\n",
    "             \\nUnknown index: {}\\\n",
    "             \\nUnknown token: {}\\\n",
    "             \\nPadding index: {}\\\n",
    "             \\nPadding token: {}\\\n",
    "             \\n{}\".format(\n",
    "            super(TokenEmbedding, self).__repr__(),\n",
    "            self._word_to_idx[self.unknown_token], self.unknown_token,\n",
    "            self._word_to_idx[PAD_TOKEN], PAD_TOKEN, self.weight)\n",
    "        return info\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.TokenEmbeddingæ–¹æ³•\n",
    "### 3.1 list_embedding_name\n",
    "### 3.2 TokenEmbeddingå„ç±»æ–¹æ³•\n",
    "\n",
    "#### 3.2.1 set_trainable æ˜¯å¦å°†tokençš„æƒé‡è®¾ç½®ä¸ºå¯è®­ç»ƒçš„\n",
    "#### 3.2.2 search è·å–æŒ‡å®šå•è¯çš„å‘é‡\n",
    "#### 3.2.3 get_idx_from_word è·å–é€šè¿‡æœç´¢å•è¯\\uåˆ°\\u idx dictæ¥æŒ‡å®šå•è¯çš„ç´¢å¼•\n",
    "#### 3.2.4 get_idx_list_from_words Gets the index list of specifying words by searching word_to_idx dict.\n",
    "#### 3.2.5 dot  è®¡ç®—ä¸¤ä¸ªå•è¯çš„ç‚¹ç§¯ã€‚ç‚¹ç§¯æˆ–æ ‡é‡ç§¯æ˜¯ä¸€ç§ä»£æ•°è¿ç®—ï¼Œå®ƒé‡‡ç”¨ä¸¤ä¸ªç­‰é•¿çš„æ•°å­—åºåˆ—ï¼ˆé€šå¸¸æ˜¯åæ ‡å‘é‡ï¼‰ï¼Œå¹¶è¿”å›å•ä¸ªæ•°å­—\n",
    "#### 3.2.6 cosine_sim è®¡ç®—ä¸¤ä¸ªè¯å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ã€‚ä½™å¼¦ç›¸ä¼¼æ€§æ˜¯nç»´ç©ºé—´ä¸­ä¸¤ä¸ªnç»´å‘é‡ä¹‹é—´å¤¹è§’çš„ä½™å¼¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w2v.baidu_encyclopedia.target.word-word.dim300\n",
      "w2v.baidu_encyclopedia.target.word-character.char1-1.dim300\n",
      "w2v.baidu_encyclopedia.target.word-character.char1-2.dim300\n",
      "w2v.baidu_encyclopedia.target.word-character.char1-4.dim300\n",
      "w2v.baidu_encyclopedia.target.word-ngram.1-2.dim300\n",
      "w2v.baidu_encyclopedia.target.word-ngram.1-3.dim300\n",
      "w2v.baidu_encyclopedia.target.word-ngram.2-2.dim300\n",
      "w2v.baidu_encyclopedia.target.word-wordLR.dim300\n",
      "w2v.baidu_encyclopedia.target.word-wordPosition.dim300\n",
      "w2v.baidu_encyclopedia.target.bigram-char.dim300\n",
      "w2v.baidu_encyclopedia.context.word-word.dim300\n",
      "w2v.baidu_encyclopedia.context.word-character.char1-1.dim300\n",
      "w2v.baidu_encyclopedia.context.word-character.char1-2.dim300\n",
      "w2v.baidu_encyclopedia.context.word-character.char1-4.dim300\n",
      "w2v.baidu_encyclopedia.context.word-ngram.1-2.dim300\n",
      "w2v.baidu_encyclopedia.context.word-ngram.1-3.dim300\n",
      "w2v.baidu_encyclopedia.context.word-ngram.2-2.dim300\n",
      "w2v.baidu_encyclopedia.context.word-wordLR.dim300\n",
      "w2v.baidu_encyclopedia.context.word-wordPosition.dim300\n",
      "w2v.wiki.target.bigram-char.dim300\n",
      "w2v.wiki.target.word-char.dim300\n",
      "w2v.wiki.target.word-word.dim300\n",
      "w2v.wiki.target.word-bigram.dim300\n",
      "w2v.people_daily.target.bigram-char.dim300\n",
      "w2v.people_daily.target.word-char.dim300\n",
      "w2v.people_daily.target.word-word.dim300\n",
      "w2v.people_daily.target.word-bigram.dim300\n",
      "w2v.weibo.target.bigram-char.dim300\n",
      "w2v.weibo.target.word-char.dim300\n",
      "w2v.weibo.target.word-word.dim300\n",
      "w2v.weibo.target.word-bigram.dim300\n",
      "w2v.sogou.target.bigram-char.dim300\n",
      "w2v.sogou.target.word-char.dim300\n",
      "w2v.sogou.target.word-word.dim300\n",
      "w2v.sogou.target.word-bigram.dim300\n",
      "w2v.zhihu.target.bigram-char.dim300\n",
      "w2v.zhihu.target.word-char.dim300\n",
      "w2v.zhihu.target.word-word.dim300\n",
      "w2v.zhihu.target.word-bigram.dim300\n",
      "w2v.financial.target.bigram-char.dim300\n",
      "w2v.financial.target.word-char.dim300\n",
      "w2v.financial.target.word-word.dim300\n",
      "w2v.financial.target.word-bigram.dim300\n",
      "w2v.literature.target.bigram-char.dim300\n",
      "w2v.literature.target.word-char.dim300\n",
      "w2v.literature.target.word-word.dim300\n",
      "w2v.literature.target.word-bigram.dim300\n",
      "w2v.sikuquanshu.target.word-word.dim300\n",
      "w2v.sikuquanshu.target.word-bigram.dim300\n",
      "w2v.mixed-large.target.word-char.dim300\n",
      "w2v.mixed-large.target.word-word.dim300\n",
      "w2v.google_news.target.word-word.dim300.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim50.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim100.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim200.en\n",
      "glove.wiki2014-gigaword.target.word-word.dim300.en\n",
      "glove.twitter.target.word-word.dim25.en\n",
      "glove.twitter.target.word-word.dim50.en\n",
      "glove.twitter.target.word-word.dim100.en\n",
      "glove.twitter.target.word-word.dim200.en\n",
      "fasttext.wiki-news.target.word-word.dim300.en\n",
      "fasttext.crawl.target.word-word.dim300.en\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.embeddings import token_embedding\r\n",
    "\r\n",
    "# list_embedding_name()è·å–embeddingåç§°\r\n",
    "embedding_list=token_embedding.list_embedding_name()\r\n",
    "for item in embedding_list:\r\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ä¸ƒã€PaddleNLPæ›´å¤šé¢„è®­ç»ƒè¯å‘é‡\n",
    "PaddleNLPæä¾›61ç§å¯ç›´æ¥åŠ è½½çš„é¢„è®­ç»ƒè¯å‘é‡ï¼Œè®­ç»ƒè‡ªå¤šé¢†åŸŸä¸­è‹±æ–‡è¯­æ–™ã€è¦†ç›–å¤šç§ç»å…¸è¯å‘é‡æ¨¡å‹ï¼ˆword2vecã€gloveã€fastTextï¼‰ã€æ¶µç›–ä¸åŒç»´åº¦ã€ä¸åŒè¯­æ–™åº“å¤§å°ï¼Œè¯¦è§[PaddleNLP Embedding API](https://github.com/PaddlePaddle/PaddleNLP/blob/develop/docs/embeddings.md)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# å…«ã€PaddleNLP æ›´å¤šé¡¹ç›®\n",
    " - [seq2vecæ˜¯ä»€ä¹ˆ? ç§ç§æ€ä¹ˆç”¨å®ƒåšæƒ…æ„Ÿåˆ†æ](https://aistudio.baidu.com/aistudio/projectdetail/1283423)\n",
    " - [å¦‚ä½•é€šè¿‡é¢„è®­ç»ƒæ¨¡å‹Fine-tuneä¸‹æ¸¸ä»»åŠ¡](https://aistudio.baidu.com/aistudio/projectdetail/1294333)\n",
    " - [ä½¿ç”¨BiGRU-CRFæ¨¡å‹å®Œæˆå¿«é€’å•ä¿¡æ¯æŠ½å–](https://aistudio.baidu.com/aistudio/projectdetail/1317771)\n",
    " - [ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ERNIEä¼˜åŒ–å¿«é€’å•ä¿¡æ¯æŠ½å–](https://aistudio.baidu.com/aistudio/projectdetail/1329361)\n",
    " - [ä½¿ç”¨Seq2Seqæ¨¡å‹å®Œæˆè‡ªåŠ¨å¯¹è”](https://aistudio.baidu.com/aistudio/projectdetail/1321118)\n",
    " - [ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ERNIE-GENå®ç°æ™ºèƒ½å†™è¯—](https://aistudio.baidu.com/aistudio/projectdetail/1339888)\n",
    " - [ä½¿ç”¨PaddleNLPé¢„æµ‹æ–°å† ç–«æƒ…ç—…ä¾‹æ•°](https://aistudio.baidu.com/aistudio/projectdetail/1515548)\n",
    " - [ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å®Œæˆé˜…è¯»ç†è§£](https://aistudio.baidu.com/aistudio/projectdetail/1339612)\n",
    " - [è‡ªå®šä¹‰æ•°æ®é›†å®ç°æ–‡æœ¬å¤šåˆ†ç±»ä»»åŠ¡](https://aistudio.baidu.com/aistudio/projectdetail/1468469)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
