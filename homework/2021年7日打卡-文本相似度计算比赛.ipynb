{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2021年7日打卡营大作业\n",
    "\n",
    "大家好，这里是2021年7日打卡营大作业，本次作业内容为**实现文本相似度任务**，通过课上所学知识，实现文本相似度任务的代码。**目前已经给出了基于SimilarityNet的相似度任务的实现代码，同学们可以基于本代码进行修改或者重写，实现更高的准确率**，其中数据集已经上传，如果有误操，可以联系qq群助教，获得数据集。\n",
    "\n",
    "## 【作业内容】\n",
    "\n",
    "按照本项目给出的模块完成代码并跑通。\n",
    "\n",
    "## 【作业提交】\n",
    "我们会给出一个数据集压缩包，包含`baidu_train.tsv`,`baidu_dev.tsv`,`vocab.txt`和`test_forstu.tsv`。其中`baidu_train.tsv`,`baidu_dev.tsv`,`vocab.txt`用来训练模型。最后大家根据`test_forstu.tsv`测试数据输出预测结果。命名为`result.tsv`的文件并提交。\n",
    ">note:遇到问题请及时向qq群的助教反馈。\n",
    "\n",
    "## 【评分标准】\n",
    "\n",
    "**优秀学员：**\n",
    "\n",
    "条件：\n",
    "1. 完成每天的课后作业；\n",
    "2. 大作业代码运行成功且有结果；\n",
    "3. 准确率排名前26名。\n",
    "\n",
    "奖品设置：\n",
    "|等级|奖品|名额|\n",
    "|---|---|---|\n",
    "|一等奖|小度在家|1|\n",
    "|二等奖|耳机 |5|\n",
    "|三等奖|书 | 10|\n",
    "|四等奖|健身包|10|\n",
    "|鼓励奖|招财熊|170|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. 任务介绍\n",
    "## 1.1 任务内容\n",
    "\n",
    "文本语义匹配是自然语言处理中一个重要的基础问题，NLP领域的很多任务都可以抽象为文本匹配任务。例如，信息检索可以归结为查询项和文档的匹配，问答系统可以归结为问题和候选答案的匹配，对话系统可以归结为对话和回复的匹配。语义匹配在搜索优化、推荐系统、快速检索排序、智能客服上都有广泛的应用。如何提升文本匹配的准确度，是自然语言处理领域的一个重要挑战。\n",
    "\n",
    "+ 信息检索：在信息检索领域的很多应用中，都需要根据原文本来检索与其相似的其他文本，使用场景非常普遍。\n",
    "+ 新闻推荐：通过用户刚刚浏览过的新闻标题，自动检索出其他的相似新闻，个性化地为用户做推荐，从而增强用户粘性，提升产品体验。\n",
    "+ 智能客服：用户输入一个问题后，自动为用户检索出相似的问题和答案，节约人工客服的成本，提高效率。\n",
    "\n",
    "## 1.2 什么是文本匹配？\n",
    "\n",
    "让我们来看一个简单的例子，比较各候选句子哪句和原句语义更相近\n",
    "\n",
    "原句：“车头如何放置车牌”\n",
    "\n",
    "+ 比较句1：“前牌照怎么装”\n",
    "+ 比较句2：“如何办理北京车牌”\n",
    "+ 比较句3：“后牌照怎么装”\n",
    "\n",
    "（1）比较句1与原句，虽然句式和语序等存在较大差异，但是所表述的含义几乎相同\n",
    "\n",
    "（2）比较句2与原句，虽然存在“如何” 、“车牌”等共现词，但是所表述的含义完全不同\n",
    "\n",
    "（3）比较句3与原句，二者讨论的都是如何放置车牌的问题，只不过一个是前牌照，另一个是后牌照。二者间存在一定的语义相关性。\n",
    "\n",
    "所以语义相关性，句1大于句3，句3大于句2.这就是语义匹配。\n",
    "\n",
    "\n",
    "## 1.3 短文本语义匹配网络\n",
    "\n",
    "短文本语义匹配(SimilarityNet, SimNet)是一个计算短文本相似度的框架，可以根据用户输入的两个文本，计算出相似度得分。主要包括BOW、CNN、RNN、MMDNN等核心网络结构形式，提供语义相似度计算训练和预测框架，适用于信息检索、新闻推荐、智能客服等多个应用场景，帮助企业解决语义匹配问题。\n",
    "\n",
    "SimNet模型结构如图所示，包括输入层、表示层以及匹配层。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/953b573d100b46fe84f026c99a3a2664fbecbe680fa748eda6337548a2f4b4af\" width=50%></center>\n",
    "<center><br>图 SimilarityNet 框架 </br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.4 实验设计\n",
    "\n",
    "本任务给出一个文本相似度任务实现方法，模型框架结构图如下图所示，其中query和title是数据集经过处理后的待匹配的文本，然后经过分词处理，编码成id，经过SimilarityNet处理，得到输出，训练的损失函数使用的是交叉熵损失。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/df8c743f9b3742e4a99ae6b066c133c9b119f7499e3b4addb5ce755ec3faae7e\" width='600px'></center>\n",
    "<center><br>图 SimilarityNet实验流程 </br></center>\n",
    "<br></br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. 实验详细实现\n",
    "文本相似度实验流程包含如下6个步骤：\n",
    "1. **数据读取**：根据网络接收的数据格式，完成相应的预处理操作，保证模型正常读取；\n",
    "2. **模型构建**：设计基于GRU的文本相似度模型，判断两句话是否是相似；\n",
    "3. **训练配置**：实例化模型，选择模型计算资源（CPU或者GPU），指定模型迭代的优化算法；\n",
    "4. **模型训练与保存**：执行多轮训练不断调整参数，以达到较好的效果，保存模型；\n",
    "5. **模型评估**：训练好的模型在测试集合上测试；\n",
    "6. **模型预测**：加载训练好的模型，并进行预测；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1数据读取\n",
    "\n",
    "本实验共计需要读取四份数据: 训练集 **train.tsv**、验证集 **dev.tsv**、测试集 **test.tsv** 和 词汇表 **vocab.txt**。加载数据的代码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 加载第三方库，paddle和paddlenlp相关的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "from functools import partial\n",
    "import random\n",
    "import time\n",
    "import inspect\n",
    "import importlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle.io import IterableDataset\n",
    "from paddle.utils.download import get_path_from_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 本实验需要依赖与paddlenlp，aistudio上的paddlenlp版本过低，所以需要首先升级paddlenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple/\n",
      "Collecting paddlenlp\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)\n",
      "\u001b[K     |████████████████████████████████| 430kB 12.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess (from paddlenlp)\n",
      "\u001b[?25l  Downloading https://mirror.baidu.com/pypi/packages/db/20/458ac043a57322365ac2ed86a911bf7598fc2e49bccb3f94ea810fbb6b9b/multiprocess-0.70.11.1-py37-none-any.whl (108kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 52.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->visualdl->paddlenlp) (7.2.0)\n",
      "Installing collected packages: multiprocess, paddlenlp\n",
      "  Found existing installation: paddlenlp 2.0.0rc7\n",
      "    Uninstalling paddlenlp-2.0.0rc7:\n",
      "      Successfully uninstalled paddlenlp-2.0.0rc7\n",
      "Successfully installed multiprocess-0.70.11.1 paddlenlp-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install paddlenlp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 导入paddlenlp相关的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "# from utils import convert_example\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddle.dataset.common import md5file\n",
    "from paddlenlp.datasets import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_example(example, tokenizer, is_test=False):\n",
    "    \"\"\"\n",
    "    为序列分类任务从序列生成模型输入\n",
    "    \"\"\"\n",
    "    \n",
    "    query, title = example[\"query\"], example[\"title\"]\n",
    "    # query id生成\n",
    "    query_ids = np.array(tokenizer.encode(query), dtype=\"int64\")\n",
    "    query_seq_len = np.array(len(query_ids), dtype=\"int64\")\n",
    "\n",
    "    # title id 生成\n",
    "    title_ids = np.array(tokenizer.encode(title), dtype=\"int64\")\n",
    "    title_seq_len = np.array(len(title_ids), dtype=\"int64\")\n",
    "    \n",
    "    # 训练模式\n",
    "    if not is_test:\n",
    "        label = np.array(example[\"label\"], dtype=\"int64\")\n",
    "        return query_ids, title_ids, query_seq_len, title_seq_len, label\n",
    "    else:\n",
    "        # 测试模式\n",
    "        return query_ids, title_ids, query_seq_len, title_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BAIDUData(DatasetBuilder):\n",
    "\n",
    "    SPLITS = {\n",
    "        'train':os.path.join( 'baidu_train.tsv'),\n",
    "        'dev': os.path.join('baidu_dev.tsv'),\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        filename = self.SPLITS[mode]\n",
    "        return filename\n",
    "\n",
    "    def _read(self, filename):\n",
    "        \"\"\"读取数据\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            head = None\n",
    "            for line in f:\n",
    "                data = line.strip().split(\"\\t\")\n",
    "                if not head:\n",
    "                    head = data\n",
    "                else:\n",
    "                    query, title, label = data\n",
    "                    yield {\"query\": query, \"title\": title, \"label\": label}\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_dataset(name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = BAIDUData\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 加载词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "vocab_path=\"vocab.txt\"\n",
    "\n",
    "vocab = Vocab.load_vocabulary(vocab_path, unk_token='[UNK]', pad_token='[PAD]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 加载训练集，验证集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.BAIDUData'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 19:08:47,165 - INFO - unique_endpoints {''}\n"
     ]
    }
   ],
   "source": [
    "# Loads dataset.\n",
    "train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 创建dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_dataloader(dataset,\n",
    "                      trans_fn=None,\n",
    "                      mode='train',\n",
    "                      batch_size=1, #1\n",
    "                      batchify_fn=None):\n",
    "    if trans_fn:\n",
    "        dataset = dataset.map(trans_fn)\n",
    "\n",
    "    shuffle = True if mode == 'train' else False\n",
    "    if mode == \"train\":\n",
    "        sampler = paddle.io.DistributedBatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        sampler = paddle.io.BatchSampler(\n",
    "            dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    dataloader = paddle.io.DataLoader(\n",
    "        dataset,\n",
    "        batch_sampler=sampler,\n",
    "        return_list=True,\n",
    "        collate_fn=batchify_fn)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reads data and generates mini-batches.\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # query_ids\n",
    "        Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # title_ids\n",
    "        Stack(dtype=\"int64\"),  # query_seq_lens\n",
    "        Stack(dtype=\"int64\"),  # title_seq_lens\n",
    "        Stack(dtype=\"int64\")  # label\n",
    "    ): [data for data in fn(samples)]\n",
    "tokenizer = ppnlp.data.JiebaTokenizer(vocab)\n",
    "\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, is_test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 模型构建\n",
    "\n",
    "本节实验构建了基于GRU的SimilarityNet网络结构，如图所示。模型得输入是query和title，然后分别经过embedding层，GRU层，把GRU输出得向量拼接（concat操作），最后经过FC(全连接)层。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/645678222ebb4c87bcf8b53e4e4ecbdc64340506839e498c8a3ed12fe57c0686\" width='500px'></center>\n",
    "<center><br>图 基于GRU的SimNet网络结构 </br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUEncoder(nn.Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 hidden_size,\n",
    "                 num_layers=1,\n",
    "                 direction=\"forward\",\n",
    "                 dropout=0.0,\n",
    "                 pooling_type=None,\n",
    "                 **kwargs):\n",
    "        super().__init__()\n",
    "        # 输入的大小\n",
    "        self._input_size = input_size\n",
    "        # 隐藏层的大小\n",
    "        self._hidden_size = hidden_size\n",
    "        self._direction = direction\n",
    "        self._pooling_type = pooling_type\n",
    "        # 实例化GRU层\n",
    "        self.gru_layer = nn.GRU(input_size=input_size,\n",
    "                                hidden_size=hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                direction=direction,\n",
    "                                dropout=dropout,\n",
    "                                **kwargs)\n",
    "\n",
    "    def get_input_dim(self):\n",
    "        \"\"\"\n",
    "        Returns the dimension of the vector input for each element in the sequence input\n",
    "        to a `GRUEncoder`. This is not the shape of the input tensor, but the\n",
    "        last element of that shape.\n",
    "        \"\"\"\n",
    "        return self._input_size\n",
    "\n",
    "    def get_output_dim(self):\n",
    "        \"\"\"\n",
    "        Returns the dimension of the final vector output by this `GRUEncoder`.  This is not\n",
    "        the shape of the returned tensor, but the last element of that shape.\n",
    "        \"\"\"\n",
    "        if self._direction == \"bidirect\":\n",
    "            return self._hidden_size * 2\n",
    "        else:\n",
    "            return self._hidden_size\n",
    "\n",
    "    def forward(self, inputs, sequence_length):\n",
    "        \n",
    "        encoded_text, last_hidden = self.gru_layer(\n",
    "            inputs, sequence_length=sequence_length)\n",
    "        if not self._pooling_type:\n",
    "            # We exploit the `last_hidden` (the hidden state at the last time step for every layer)\n",
    "            # to create a single vector.\n",
    "            # If gru is not bidirection, then output is the hidden state of the last time step \n",
    "            # at last layer. Output is shape of `(batch_size, hidden_size)`.\n",
    "            # If gru is bidirection, then output is concatenation of the forward and backward hidden state \n",
    "            # of the last time step at last layer. Output is shape of `(batch_size, hidden_size*2)`.\n",
    "            if self._direction != 'bidirect':\n",
    "                output = last_hidden[-1, :, :]\n",
    "            else:\n",
    "                output = paddle.concat(\n",
    "                    (last_hidden[-2, :, :], last_hidden[-1, :, :]), axis=1)\n",
    "        else:\n",
    "            # We exploit the `encoded_text` (the hidden state at the every time step for last layer)\n",
    "            # to create a single vector. We perform pooling on the encoded text.\n",
    "            # The output shape is `(batch_size, hidden_size*2)` if use bidirectional GRU, \n",
    "            # otherwise the output shape is `(batch_size, hidden_size*2)`.\n",
    "            if self._pooling_type == 'sum':\n",
    "                output = paddle.sum(encoded_text, axis=1)\n",
    "            elif self._pooling_type == 'max':\n",
    "                output = paddle.max(encoded_text, axis=1)\n",
    "            elif self._pooling_type == 'mean':\n",
    "                output = paddle.mean(encoded_text, axis=1)\n",
    "            else:\n",
    "                raise RuntimeError(\n",
    "                    \"Unexpected pooling type %s .\"\n",
    "                    \"Pooling type must be one of sum, max and mean.\" %\n",
    "                    self._pooling_type)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GRUModel(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 padding_idx=0,\n",
    "                 gru_hidden_size=128,\n",
    "                 direction='forward',\n",
    "                 gru_layers=1,\n",
    "                 dropout_rate=0.0,\n",
    "                 pooling_type=None,\n",
    "                 fc_hidden_size=96):\n",
    "        super().__init__()\n",
    "        # 词嵌入层\n",
    "        self.embedder = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=padding_idx)\n",
    "        # GRUb编码层\n",
    "        self.gru_encoder = GRUEncoder(\n",
    "            emb_dim,\n",
    "            gru_hidden_size,\n",
    "            num_layers=gru_layers,\n",
    "            direction=direction,\n",
    "            dropout=dropout_rate)\n",
    "        # 线性层\n",
    "        self.fc = nn.Linear(self.gru_encoder.get_output_dim() * 2,fc_hidden_size)\n",
    "        # 输出层\n",
    "        self.output_layer = nn.Linear(fc_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, query, title, query_seq_len, title_seq_len):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_query = self.embedder(query)\n",
    "        embedded_title = self.embedder(title)\n",
    "        # Shape: (batch_size, gru_hidden_size)\n",
    "        query_repr = self.gru_encoder(\n",
    "            embedded_query, sequence_length=query_seq_len)\n",
    "        title_repr = self.gru_encoder(\n",
    "            embedded_title, sequence_length=title_seq_len)\n",
    "        # Shape: (batch_size, 2*gru_hidden_size)\n",
    "        contacted = paddle.concat([query_repr, title_repr], axis=-1)\n",
    "        # Shape: (batch_size, fc_hidden_size)\n",
    "        fc_out = paddle.tanh(self.fc(contacted))\n",
    "        # Shape: (batch_size, num_classes)\n",
    "        logits = self.output_layer(fc_out)\n",
    "        # probs = F.softmax(logits, axis=-1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimNet(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 network,\n",
    "                 vocab_size,\n",
    "                 num_classes,\n",
    "                 emb_dim=128,\n",
    "                 pad_token_id=0):\n",
    "        super().__init__()\n",
    "        # 转换成小写\n",
    "        network = network.lower()\n",
    "        # gru网络\n",
    "        if network == 'gru':\n",
    "            self.model = GRUModel(\n",
    "                vocab_size,\n",
    "                num_classes,\n",
    "                emb_dim,\n",
    "                direction='forward',\n",
    "                padding_idx=pad_token_id)\n",
    "\n",
    "    def forward(self, query, title, query_seq_len=None, title_seq_len=None):\n",
    "        logits = self.model(query, title, query_seq_len, title_seq_len)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.3 训练配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 检测是否可以使用GPU，如果可以优先使用GPU\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\n",
    "if use_gpu:\n",
    "    paddle.set_device('gpu')\n",
    "    #print('gpu')\n",
    "batch_size=200   # batch size 的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 实例化SimilarityNet网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建SimNet网络，网络的主体采用GRUb网络\n",
    "network='gru'\n",
    "model = SimNet(\n",
    "        network=network,\n",
    "        vocab_size=len(vocab),\n",
    "        num_classes=len(train_ds.label_list))\n",
    "model = paddle.Model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 构建训练集，验证集和测试集的dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建训练集的dataloader\n",
    "train_loader = create_dataloader(\n",
    "        train_ds,\n",
    "        trans_fn=trans_fn,\n",
    "        batch_size=batch_size,\n",
    "        mode='train',\n",
    "        batchify_fn=batchify_fn)\n",
    "# 构建验证集的dataloader\n",
    "dev_loader = create_dataloader(\n",
    "        dev_ds,\n",
    "        trans_fn=trans_fn,\n",
    "        batch_size=batch_size,\n",
    "        mode='validation',\n",
    "        batchify_fn=batchify_fn)\n",
    "# # 构建测试集的dataloader\n",
    "# test_loader = create_dataloader(\n",
    "#         test_ds,\n",
    "#         trans_fn=trans_fn,\n",
    "#         batch_size=batch_size,\n",
    "#         mode='test',\n",
    "#         batchify_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 定义优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr=5e-4\n",
    "optimizer = paddle.optimizer.Adam(\n",
    "        parameters=model.parameters(), learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 实例化损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defines loss and metric.\n",
    "criterion = paddle.nn.CrossEntropyLoss()  # 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 实例化评估方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = paddle.metric.Accuracy()  # accuracy 评估方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 编译模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.prepare(optimizer, criterion, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.4 模型训练\n",
    "\n",
    "+ 模型训练使用paddle的高级API，使用fit函数就可以实现训练和模型保存，模型保存的位置为save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  return (isinstance(seq, collections.Sequence) and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/313 - loss: 0.6779 - acc: 0.5560 - 71ms/step\n",
      "step  20/313 - loss: 0.6775 - acc: 0.5620 - 54ms/step\n",
      "step  30/313 - loss: 0.6709 - acc: 0.5600 - 49ms/step\n",
      "step  40/313 - loss: 0.6698 - acc: 0.5779 - 46ms/step\n",
      "step  50/313 - loss: 0.6427 - acc: 0.5852 - 44ms/step\n",
      "step  60/313 - loss: 0.6441 - acc: 0.5972 - 43ms/step\n",
      "step  70/313 - loss: 0.6351 - acc: 0.6081 - 42ms/step\n",
      "step  80/313 - loss: 0.5961 - acc: 0.6187 - 42ms/step\n",
      "step  90/313 - loss: 0.5682 - acc: 0.6259 - 42ms/step\n",
      "step 100/313 - loss: 0.6386 - acc: 0.6305 - 41ms/step\n",
      "step 110/313 - loss: 0.5877 - acc: 0.6357 - 41ms/step\n",
      "step 120/313 - loss: 0.5476 - acc: 0.6404 - 41ms/step\n",
      "step 130/313 - loss: 0.5689 - acc: 0.6439 - 40ms/step\n",
      "step 140/313 - loss: 0.5621 - acc: 0.6487 - 40ms/step\n",
      "step 150/313 - loss: 0.5484 - acc: 0.6526 - 40ms/step\n",
      "step 160/313 - loss: 0.5292 - acc: 0.6560 - 40ms/step\n",
      "step 170/313 - loss: 0.5861 - acc: 0.6589 - 39ms/step\n",
      "step 180/313 - loss: 0.5605 - acc: 0.6619 - 39ms/step\n",
      "step 190/313 - loss: 0.5725 - acc: 0.6634 - 39ms/step\n",
      "step 200/313 - loss: 0.5240 - acc: 0.6660 - 39ms/step\n",
      "step 210/313 - loss: 0.5724 - acc: 0.6684 - 39ms/step\n",
      "step 220/313 - loss: 0.5520 - acc: 0.6701 - 39ms/step\n",
      "step 230/313 - loss: 0.5021 - acc: 0.6723 - 39ms/step\n",
      "step 240/313 - loss: 0.4600 - acc: 0.6744 - 39ms/step\n",
      "step 250/313 - loss: 0.5444 - acc: 0.6758 - 39ms/step\n",
      "step 260/313 - loss: 0.5799 - acc: 0.6770 - 39ms/step\n",
      "step 270/313 - loss: 0.5113 - acc: 0.6786 - 39ms/step\n",
      "step 280/313 - loss: 0.4968 - acc: 0.6802 - 39ms/step\n",
      "step 290/313 - loss: 0.5544 - acc: 0.6815 - 39ms/step\n",
      "step 300/313 - loss: 0.5375 - acc: 0.6823 - 39ms/step\n",
      "step 310/313 - loss: 0.5395 - acc: 0.6842 - 38ms/step\n",
      "step 313/313 - loss: 0.5030 - acc: 0.6843 - 38ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/0\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.4852 - acc: 0.7650 - 45ms/step\n",
      "step 20/40 - loss: 0.4746 - acc: 0.7752 - 37ms/step\n",
      "step 30/40 - loss: 0.4398 - acc: 0.7763 - 34ms/step\n",
      "step 40/40 - loss: 0.8953 - acc: 0.7793 - 29ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 2/10\n",
      "step  10/313 - loss: 0.4326 - acc: 0.7815 - 50ms/step\n",
      "step  20/313 - loss: 0.4775 - acc: 0.7855 - 43ms/step\n",
      "step  30/313 - loss: 0.5019 - acc: 0.7822 - 40ms/step\n",
      "step  40/313 - loss: 0.4167 - acc: 0.7849 - 39ms/step\n",
      "step  50/313 - loss: 0.5203 - acc: 0.7799 - 39ms/step\n",
      "step  60/313 - loss: 0.4360 - acc: 0.7805 - 39ms/step\n",
      "step  70/313 - loss: 0.4256 - acc: 0.7804 - 38ms/step\n",
      "step  80/313 - loss: 0.5098 - acc: 0.7797 - 38ms/step\n",
      "step  90/313 - loss: 0.4505 - acc: 0.7802 - 38ms/step\n",
      "step 100/313 - loss: 0.5386 - acc: 0.7798 - 38ms/step\n",
      "step 110/313 - loss: 0.3901 - acc: 0.7804 - 37ms/step\n",
      "step 120/313 - loss: 0.5151 - acc: 0.7810 - 37ms/step\n",
      "step 130/313 - loss: 0.4669 - acc: 0.7808 - 37ms/step\n",
      "step 140/313 - loss: 0.5065 - acc: 0.7811 - 37ms/step\n",
      "step 150/313 - loss: 0.4478 - acc: 0.7807 - 37ms/step\n",
      "step 160/313 - loss: 0.4828 - acc: 0.7816 - 37ms/step\n",
      "step 170/313 - loss: 0.4707 - acc: 0.7814 - 37ms/step\n",
      "step 180/313 - loss: 0.4508 - acc: 0.7816 - 37ms/step\n",
      "step 190/313 - loss: 0.4934 - acc: 0.7808 - 37ms/step\n",
      "step 200/313 - loss: 0.4777 - acc: 0.7797 - 37ms/step\n",
      "step 210/313 - loss: 0.5268 - acc: 0.7788 - 37ms/step\n",
      "step 220/313 - loss: 0.4566 - acc: 0.7787 - 37ms/step\n",
      "step 230/313 - loss: 0.4869 - acc: 0.7783 - 37ms/step\n",
      "step 240/313 - loss: 0.5303 - acc: 0.7784 - 37ms/step\n",
      "step 250/313 - loss: 0.5559 - acc: 0.7783 - 37ms/step\n",
      "step 260/313 - loss: 0.4952 - acc: 0.7775 - 37ms/step\n",
      "step 270/313 - loss: 0.4588 - acc: 0.7774 - 37ms/step\n",
      "step 280/313 - loss: 0.5450 - acc: 0.7772 - 37ms/step\n",
      "step 290/313 - loss: 0.4265 - acc: 0.7778 - 37ms/step\n",
      "step 300/313 - loss: 0.5427 - acc: 0.7781 - 37ms/step\n",
      "step 310/313 - loss: 0.5577 - acc: 0.7780 - 37ms/step\n",
      "step 313/313 - loss: 0.4183 - acc: 0.7780 - 36ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/1\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.3883 - acc: 0.8435 - 41ms/step\n",
      "step 20/40 - loss: 0.3899 - acc: 0.8440 - 34ms/step\n",
      "step 30/40 - loss: 0.3690 - acc: 0.8463 - 32ms/step\n",
      "step 40/40 - loss: 0.7862 - acc: 0.8472 - 28ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 3/10\n",
      "step  10/313 - loss: 0.4020 - acc: 0.8515 - 50ms/step\n",
      "step  20/313 - loss: 0.2510 - acc: 0.8470 - 43ms/step\n",
      "step  30/313 - loss: 0.4031 - acc: 0.8463 - 41ms/step\n",
      "step  40/313 - loss: 0.3762 - acc: 0.8464 - 39ms/step\n",
      "step  50/313 - loss: 0.3229 - acc: 0.8437 - 39ms/step\n",
      "step  60/313 - loss: 0.4479 - acc: 0.8409 - 38ms/step\n",
      "step  70/313 - loss: 0.3726 - acc: 0.8391 - 38ms/step\n",
      "step  80/313 - loss: 0.4101 - acc: 0.8391 - 38ms/step\n",
      "step  90/313 - loss: 0.3670 - acc: 0.8381 - 38ms/step\n",
      "step 100/313 - loss: 0.3189 - acc: 0.8377 - 38ms/step\n",
      "step 110/313 - loss: 0.3986 - acc: 0.8376 - 38ms/step\n",
      "step 120/313 - loss: 0.3410 - acc: 0.8371 - 37ms/step\n",
      "step 130/313 - loss: 0.3904 - acc: 0.8371 - 37ms/step\n",
      "step 140/313 - loss: 0.3792 - acc: 0.8365 - 37ms/step\n",
      "step 150/313 - loss: 0.4145 - acc: 0.8358 - 37ms/step\n",
      "step 160/313 - loss: 0.3486 - acc: 0.8357 - 37ms/step\n",
      "step 170/313 - loss: 0.3987 - acc: 0.8349 - 37ms/step\n",
      "step 180/313 - loss: 0.3979 - acc: 0.8339 - 37ms/step\n",
      "step 190/313 - loss: 0.4143 - acc: 0.8342 - 37ms/step\n",
      "step 200/313 - loss: 0.3647 - acc: 0.8338 - 37ms/step\n",
      "step 210/313 - loss: 0.4422 - acc: 0.8331 - 37ms/step\n",
      "step 220/313 - loss: 0.3823 - acc: 0.8324 - 37ms/step\n",
      "step 230/313 - loss: 0.4111 - acc: 0.8318 - 37ms/step\n",
      "step 240/313 - loss: 0.4629 - acc: 0.8319 - 37ms/step\n",
      "step 250/313 - loss: 0.3643 - acc: 0.8312 - 37ms/step\n",
      "step 260/313 - loss: 0.4079 - acc: 0.8308 - 37ms/step\n",
      "step 270/313 - loss: 0.4235 - acc: 0.8305 - 37ms/step\n",
      "step 280/313 - loss: 0.3720 - acc: 0.8304 - 37ms/step\n",
      "step 290/313 - loss: 0.4635 - acc: 0.8302 - 37ms/step\n",
      "step 300/313 - loss: 0.3411 - acc: 0.8298 - 37ms/step\n",
      "step 310/313 - loss: 0.3830 - acc: 0.8289 - 37ms/step\n",
      "step 313/313 - loss: 0.7811 - acc: 0.8287 - 37ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/2\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.2885 - acc: 0.8880 - 43ms/step\n",
      "step 20/40 - loss: 0.3043 - acc: 0.8885 - 36ms/step\n",
      "step 30/40 - loss: 0.2885 - acc: 0.8885 - 34ms/step\n",
      "step 40/40 - loss: 0.4380 - acc: 0.8894 - 29ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 4/10\n",
      "step  10/313 - loss: 0.2928 - acc: 0.8875 - 49ms/step\n",
      "step  20/313 - loss: 0.2682 - acc: 0.8855 - 44ms/step\n",
      "step  30/313 - loss: 0.3044 - acc: 0.8835 - 42ms/step\n",
      "step  40/313 - loss: 0.3253 - acc: 0.8832 - 40ms/step\n",
      "step  50/313 - loss: 0.3368 - acc: 0.8811 - 40ms/step\n",
      "step  60/313 - loss: 0.3091 - acc: 0.8802 - 39ms/step\n",
      "step  70/313 - loss: 0.2681 - acc: 0.8816 - 39ms/step\n",
      "step  80/313 - loss: 0.3478 - acc: 0.8801 - 39ms/step\n",
      "step  90/313 - loss: 0.2915 - acc: 0.8805 - 38ms/step\n",
      "step 100/313 - loss: 0.3208 - acc: 0.8796 - 38ms/step\n",
      "step 110/313 - loss: 0.2509 - acc: 0.8790 - 38ms/step\n",
      "step 120/313 - loss: 0.3732 - acc: 0.8788 - 38ms/step\n",
      "step 130/313 - loss: 0.2936 - acc: 0.8772 - 38ms/step\n",
      "step 140/313 - loss: 0.4708 - acc: 0.8749 - 38ms/step\n",
      "step 150/313 - loss: 0.3266 - acc: 0.8744 - 38ms/step\n",
      "step 160/313 - loss: 0.2815 - acc: 0.8739 - 38ms/step\n",
      "step 170/313 - loss: 0.2958 - acc: 0.8735 - 38ms/step\n",
      "step 180/313 - loss: 0.2897 - acc: 0.8727 - 38ms/step\n",
      "step 190/313 - loss: 0.2979 - acc: 0.8724 - 38ms/step\n",
      "step 200/313 - loss: 0.2620 - acc: 0.8718 - 38ms/step\n",
      "step 210/313 - loss: 0.2769 - acc: 0.8707 - 38ms/step\n",
      "step 220/313 - loss: 0.3142 - acc: 0.8705 - 38ms/step\n",
      "step 230/313 - loss: 0.3019 - acc: 0.8703 - 38ms/step\n",
      "step 240/313 - loss: 0.3372 - acc: 0.8700 - 38ms/step\n",
      "step 250/313 - loss: 0.3072 - acc: 0.8693 - 38ms/step\n",
      "step 260/313 - loss: 0.3214 - acc: 0.8692 - 38ms/step\n",
      "step 270/313 - loss: 0.3697 - acc: 0.8680 - 38ms/step\n",
      "step 280/313 - loss: 0.4037 - acc: 0.8675 - 38ms/step\n",
      "step 290/313 - loss: 0.4131 - acc: 0.8671 - 38ms/step\n",
      "step 300/313 - loss: 0.3185 - acc: 0.8665 - 38ms/step\n",
      "step 310/313 - loss: 0.3101 - acc: 0.8662 - 38ms/step\n",
      "step 313/313 - loss: 0.2654 - acc: 0.8661 - 37ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/3\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.2280 - acc: 0.9195 - 41ms/step\n",
      "step 20/40 - loss: 0.2821 - acc: 0.9120 - 35ms/step\n",
      "step 30/40 - loss: 0.2366 - acc: 0.9115 - 33ms/step\n",
      "step 40/40 - loss: 0.3791 - acc: 0.9096 - 28ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 5/10\n",
      "step  10/313 - loss: 0.2273 - acc: 0.9120 - 51ms/step\n",
      "step  20/313 - loss: 0.2184 - acc: 0.9075 - 44ms/step\n",
      "step  30/313 - loss: 0.1705 - acc: 0.9085 - 42ms/step\n",
      "step  40/313 - loss: 0.2454 - acc: 0.9080 - 40ms/step\n",
      "step  50/313 - loss: 0.2804 - acc: 0.9109 - 40ms/step\n",
      "step  60/313 - loss: 0.2295 - acc: 0.9094 - 39ms/step\n",
      "step  70/313 - loss: 0.2070 - acc: 0.9100 - 39ms/step\n",
      "step  80/313 - loss: 0.2257 - acc: 0.9090 - 38ms/step\n",
      "step  90/313 - loss: 0.2572 - acc: 0.9091 - 38ms/step\n",
      "step 100/313 - loss: 0.2887 - acc: 0.9078 - 38ms/step\n",
      "step 110/313 - loss: 0.2616 - acc: 0.9072 - 38ms/step\n",
      "step 120/313 - loss: 0.3199 - acc: 0.9058 - 38ms/step\n",
      "step 130/313 - loss: 0.1998 - acc: 0.9056 - 38ms/step\n",
      "step 140/313 - loss: 0.2293 - acc: 0.9050 - 37ms/step\n",
      "step 150/313 - loss: 0.2421 - acc: 0.9040 - 37ms/step\n",
      "step 160/313 - loss: 0.3262 - acc: 0.9026 - 37ms/step\n",
      "step 170/313 - loss: 0.2710 - acc: 0.9013 - 37ms/step\n",
      "step 180/313 - loss: 0.2383 - acc: 0.9012 - 37ms/step\n",
      "step 190/313 - loss: 0.3109 - acc: 0.9011 - 37ms/step\n",
      "step 200/313 - loss: 0.2651 - acc: 0.9001 - 37ms/step\n",
      "step 210/313 - loss: 0.2629 - acc: 0.8995 - 37ms/step\n",
      "step 220/313 - loss: 0.2654 - acc: 0.8987 - 37ms/step\n",
      "step 230/313 - loss: 0.2675 - acc: 0.8979 - 37ms/step\n",
      "step 240/313 - loss: 0.2811 - acc: 0.8974 - 37ms/step\n",
      "step 250/313 - loss: 0.2632 - acc: 0.8965 - 37ms/step\n",
      "step 260/313 - loss: 0.3469 - acc: 0.8958 - 37ms/step\n",
      "step 270/313 - loss: 0.2688 - acc: 0.8954 - 37ms/step\n",
      "step 280/313 - loss: 0.2639 - acc: 0.8947 - 37ms/step\n",
      "step 290/313 - loss: 0.3070 - acc: 0.8941 - 37ms/step\n",
      "step 300/313 - loss: 0.2988 - acc: 0.8939 - 37ms/step\n",
      "step 310/313 - loss: 0.3038 - acc: 0.8934 - 37ms/step\n",
      "step 313/313 - loss: 0.1511 - acc: 0.8934 - 37ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/4\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.1704 - acc: 0.9340 - 46ms/step\n",
      "step 20/40 - loss: 0.2443 - acc: 0.9275 - 38ms/step\n",
      "step 30/40 - loss: 0.1940 - acc: 0.9272 - 35ms/step\n",
      "step 40/40 - loss: 0.5411 - acc: 0.9268 - 29ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 6/10\n",
      "step  10/313 - loss: 0.1581 - acc: 0.9380 - 54ms/step\n",
      "step  20/313 - loss: 0.1754 - acc: 0.9335 - 47ms/step\n",
      "step  30/313 - loss: 0.2125 - acc: 0.9312 - 45ms/step\n",
      "step  40/313 - loss: 0.1296 - acc: 0.9294 - 43ms/step\n",
      "step  50/313 - loss: 0.1597 - acc: 0.9275 - 42ms/step\n",
      "step  60/313 - loss: 0.1716 - acc: 0.9258 - 42ms/step\n",
      "step  70/313 - loss: 0.2207 - acc: 0.9248 - 42ms/step\n",
      "step  80/313 - loss: 0.1764 - acc: 0.9261 - 42ms/step\n",
      "step  90/313 - loss: 0.1768 - acc: 0.9259 - 41ms/step\n",
      "step 100/313 - loss: 0.2047 - acc: 0.9258 - 41ms/step\n",
      "step 110/313 - loss: 0.2370 - acc: 0.9243 - 41ms/step\n",
      "step 120/313 - loss: 0.2340 - acc: 0.9238 - 41ms/step\n",
      "step 130/313 - loss: 0.1979 - acc: 0.9233 - 41ms/step\n",
      "step 140/313 - loss: 0.2778 - acc: 0.9223 - 41ms/step\n",
      "step 150/313 - loss: 0.2230 - acc: 0.9211 - 41ms/step\n",
      "step 160/313 - loss: 0.2481 - acc: 0.9201 - 41ms/step\n",
      "step 170/313 - loss: 0.2304 - acc: 0.9192 - 41ms/step\n",
      "step 180/313 - loss: 0.1882 - acc: 0.9191 - 40ms/step\n",
      "step 190/313 - loss: 0.2794 - acc: 0.9182 - 40ms/step\n",
      "step 200/313 - loss: 0.2881 - acc: 0.9172 - 40ms/step\n",
      "step 210/313 - loss: 0.2870 - acc: 0.9167 - 40ms/step\n",
      "step 220/313 - loss: 0.2170 - acc: 0.9165 - 40ms/step\n",
      "step 230/313 - loss: 0.2473 - acc: 0.9158 - 40ms/step\n",
      "step 240/313 - loss: 0.1799 - acc: 0.9149 - 40ms/step\n",
      "step 250/313 - loss: 0.2647 - acc: 0.9145 - 40ms/step\n",
      "step 260/313 - loss: 0.2410 - acc: 0.9142 - 40ms/step\n",
      "step 270/313 - loss: 0.2469 - acc: 0.9134 - 40ms/step\n",
      "step 280/313 - loss: 0.2352 - acc: 0.9131 - 40ms/step\n",
      "step 290/313 - loss: 0.1969 - acc: 0.9126 - 39ms/step\n",
      "step 300/313 - loss: 0.2011 - acc: 0.9119 - 39ms/step\n",
      "step 310/313 - loss: 0.2766 - acc: 0.9116 - 39ms/step\n",
      "step 313/313 - loss: 0.1941 - acc: 0.9113 - 39ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/5\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.1364 - acc: 0.9405 - 41ms/step\n",
      "step 20/40 - loss: 0.1778 - acc: 0.9390 - 34ms/step\n",
      "step 30/40 - loss: 0.1743 - acc: 0.9380 - 32ms/step\n",
      "step 40/40 - loss: 0.5655 - acc: 0.9368 - 28ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 7/10\n",
      "step  10/313 - loss: 0.1356 - acc: 0.9370 - 49ms/step\n",
      "step  20/313 - loss: 0.1708 - acc: 0.9410 - 44ms/step\n",
      "step  30/313 - loss: 0.1759 - acc: 0.9422 - 41ms/step\n",
      "step  40/313 - loss: 0.2352 - acc: 0.9420 - 40ms/step\n",
      "step  50/313 - loss: 0.1469 - acc: 0.9404 - 40ms/step\n",
      "step  60/313 - loss: 0.1239 - acc: 0.9413 - 40ms/step\n",
      "step  70/313 - loss: 0.1251 - acc: 0.9416 - 39ms/step\n",
      "step  80/313 - loss: 0.2329 - acc: 0.9407 - 39ms/step\n",
      "step  90/313 - loss: 0.1653 - acc: 0.9394 - 39ms/step\n",
      "step 100/313 - loss: 0.1765 - acc: 0.9384 - 39ms/step\n",
      "step 110/313 - loss: 0.1899 - acc: 0.9376 - 38ms/step\n",
      "step 120/313 - loss: 0.2282 - acc: 0.9367 - 38ms/step\n",
      "step 130/313 - loss: 0.1671 - acc: 0.9358 - 38ms/step\n",
      "step 140/313 - loss: 0.1461 - acc: 0.9350 - 38ms/step\n",
      "step 150/313 - loss: 0.1666 - acc: 0.9338 - 38ms/step\n",
      "step 160/313 - loss: 0.2081 - acc: 0.9325 - 38ms/step\n",
      "step 170/313 - loss: 0.1975 - acc: 0.9314 - 38ms/step\n",
      "step 180/313 - loss: 0.2556 - acc: 0.9302 - 38ms/step\n",
      "step 190/313 - loss: 0.1919 - acc: 0.9297 - 38ms/step\n",
      "step 200/313 - loss: 0.1781 - acc: 0.9294 - 38ms/step\n",
      "step 210/313 - loss: 0.2554 - acc: 0.9284 - 38ms/step\n",
      "step 220/313 - loss: 0.1822 - acc: 0.9281 - 38ms/step\n",
      "step 230/313 - loss: 0.1802 - acc: 0.9276 - 38ms/step\n",
      "step 240/313 - loss: 0.1953 - acc: 0.9273 - 38ms/step\n",
      "step 250/313 - loss: 0.1916 - acc: 0.9269 - 38ms/step\n",
      "step 260/313 - loss: 0.2343 - acc: 0.9263 - 38ms/step\n",
      "step 270/313 - loss: 0.2569 - acc: 0.9256 - 38ms/step\n",
      "step 280/313 - loss: 0.1868 - acc: 0.9248 - 37ms/step\n",
      "step 290/313 - loss: 0.2784 - acc: 0.9243 - 37ms/step\n",
      "step 300/313 - loss: 0.2176 - acc: 0.9238 - 37ms/step\n",
      "step 310/313 - loss: 0.2597 - acc: 0.9236 - 37ms/step\n",
      "step 313/313 - loss: 0.1152 - acc: 0.9237 - 37ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/6\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.1167 - acc: 0.9500 - 45ms/step\n",
      "step 20/40 - loss: 0.1898 - acc: 0.9425 - 37ms/step\n",
      "step 30/40 - loss: 0.1393 - acc: 0.9447 - 35ms/step\n",
      "step 40/40 - loss: 0.5776 - acc: 0.9442 - 30ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 8/10\n",
      "step  10/313 - loss: 0.1116 - acc: 0.9545 - 47ms/step\n",
      "step  20/313 - loss: 0.1921 - acc: 0.9493 - 41ms/step\n",
      "step  30/313 - loss: 0.1522 - acc: 0.9478 - 40ms/step\n",
      "step  40/313 - loss: 0.1188 - acc: 0.9480 - 39ms/step\n",
      "step  50/313 - loss: 0.1279 - acc: 0.9478 - 38ms/step\n",
      "step  60/313 - loss: 0.1761 - acc: 0.9484 - 38ms/step\n",
      "step  70/313 - loss: 0.2476 - acc: 0.9473 - 37ms/step\n",
      "step  80/313 - loss: 0.1823 - acc: 0.9474 - 37ms/step\n",
      "step  90/313 - loss: 0.1562 - acc: 0.9464 - 37ms/step\n",
      "step 100/313 - loss: 0.1696 - acc: 0.9451 - 37ms/step\n",
      "step 110/313 - loss: 0.1978 - acc: 0.9443 - 37ms/step\n",
      "step 120/313 - loss: 0.1863 - acc: 0.9445 - 37ms/step\n",
      "step 130/313 - loss: 0.1715 - acc: 0.9432 - 37ms/step\n",
      "step 140/313 - loss: 0.1826 - acc: 0.9425 - 37ms/step\n",
      "step 150/313 - loss: 0.2051 - acc: 0.9412 - 37ms/step\n",
      "step 160/313 - loss: 0.1343 - acc: 0.9406 - 37ms/step\n",
      "step 170/313 - loss: 0.1123 - acc: 0.9401 - 37ms/step\n",
      "step 180/313 - loss: 0.1128 - acc: 0.9400 - 37ms/step\n",
      "step 190/313 - loss: 0.2686 - acc: 0.9394 - 37ms/step\n",
      "step 200/313 - loss: 0.0916 - acc: 0.9395 - 37ms/step\n",
      "step 210/313 - loss: 0.1566 - acc: 0.9389 - 37ms/step\n",
      "step 220/313 - loss: 0.2082 - acc: 0.9380 - 37ms/step\n",
      "step 230/313 - loss: 0.1506 - acc: 0.9375 - 37ms/step\n",
      "step 240/313 - loss: 0.2126 - acc: 0.9372 - 37ms/step\n",
      "step 250/313 - loss: 0.2126 - acc: 0.9368 - 37ms/step\n",
      "step 260/313 - loss: 0.1755 - acc: 0.9362 - 37ms/step\n",
      "step 270/313 - loss: 0.2091 - acc: 0.9358 - 37ms/step\n",
      "step 280/313 - loss: 0.1718 - acc: 0.9357 - 37ms/step\n",
      "step 290/313 - loss: 0.2018 - acc: 0.9353 - 37ms/step\n",
      "step 300/313 - loss: 0.2005 - acc: 0.9347 - 37ms/step\n",
      "step 310/313 - loss: 0.2185 - acc: 0.9343 - 36ms/step\n",
      "step 313/313 - loss: 0.5939 - acc: 0.9341 - 36ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/7\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.0999 - acc: 0.9615 - 42ms/step\n",
      "step 20/40 - loss: 0.1513 - acc: 0.9550 - 35ms/step\n",
      "step 30/40 - loss: 0.1454 - acc: 0.9533 - 33ms/step\n",
      "step 40/40 - loss: 0.2175 - acc: 0.9535 - 28ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 9/10\n",
      "step  10/313 - loss: 0.1940 - acc: 0.9500 - 48ms/step\n",
      "step  20/313 - loss: 0.0704 - acc: 0.9540 - 42ms/step\n",
      "step  30/313 - loss: 0.1606 - acc: 0.9525 - 40ms/step\n",
      "step  40/313 - loss: 0.1312 - acc: 0.9520 - 39ms/step\n",
      "step  50/313 - loss: 0.1390 - acc: 0.9511 - 39ms/step\n",
      "step  60/313 - loss: 0.0732 - acc: 0.9513 - 38ms/step\n",
      "step  70/313 - loss: 0.1018 - acc: 0.9504 - 38ms/step\n",
      "step  80/313 - loss: 0.1107 - acc: 0.9496 - 38ms/step\n",
      "step  90/313 - loss: 0.1057 - acc: 0.9498 - 38ms/step\n",
      "step 100/313 - loss: 0.1056 - acc: 0.9495 - 38ms/step\n",
      "step 110/313 - loss: 0.1293 - acc: 0.9497 - 37ms/step\n",
      "step 120/313 - loss: 0.1279 - acc: 0.9486 - 37ms/step\n",
      "step 130/313 - loss: 0.1521 - acc: 0.9481 - 37ms/step\n",
      "step 140/313 - loss: 0.1171 - acc: 0.9473 - 37ms/step\n",
      "step 150/313 - loss: 0.1414 - acc: 0.9469 - 37ms/step\n",
      "step 160/313 - loss: 0.1528 - acc: 0.9463 - 37ms/step\n",
      "step 170/313 - loss: 0.2263 - acc: 0.9453 - 37ms/step\n",
      "step 180/313 - loss: 0.1872 - acc: 0.9444 - 37ms/step\n",
      "step 190/313 - loss: 0.1718 - acc: 0.9438 - 37ms/step\n",
      "step 200/313 - loss: 0.2349 - acc: 0.9435 - 37ms/step\n",
      "step 210/313 - loss: 0.1463 - acc: 0.9431 - 37ms/step\n",
      "step 220/313 - loss: 0.1457 - acc: 0.9427 - 37ms/step\n",
      "step 230/313 - loss: 0.1101 - acc: 0.9426 - 38ms/step\n",
      "step 240/313 - loss: 0.1558 - acc: 0.9421 - 38ms/step\n",
      "step 250/313 - loss: 0.1483 - acc: 0.9416 - 38ms/step\n",
      "step 260/313 - loss: 0.1467 - acc: 0.9416 - 38ms/step\n",
      "step 270/313 - loss: 0.1594 - acc: 0.9413 - 38ms/step\n",
      "step 280/313 - loss: 0.1795 - acc: 0.9411 - 38ms/step\n",
      "step 290/313 - loss: 0.1909 - acc: 0.9408 - 37ms/step\n",
      "step 300/313 - loss: 0.1435 - acc: 0.9405 - 37ms/step\n",
      "step 310/313 - loss: 0.1683 - acc: 0.9405 - 37ms/step\n",
      "step 313/313 - loss: 0.3363 - acc: 0.9405 - 37ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/8\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.0799 - acc: 0.9670 - 43ms/step\n",
      "step 20/40 - loss: 0.1404 - acc: 0.9595 - 36ms/step\n",
      "step 30/40 - loss: 0.1395 - acc: 0.9573 - 34ms/step\n",
      "step 40/40 - loss: 0.2501 - acc: 0.9578 - 29ms/step\n",
      "Eval samples: 7802\n",
      "Epoch 10/10\n",
      "step  10/313 - loss: 0.1660 - acc: 0.9520 - 49ms/step\n",
      "step  20/313 - loss: 0.1313 - acc: 0.9535 - 43ms/step\n",
      "step  30/313 - loss: 0.1238 - acc: 0.9538 - 41ms/step\n",
      "step  40/313 - loss: 0.1803 - acc: 0.9527 - 40ms/step\n",
      "step  50/313 - loss: 0.1018 - acc: 0.9531 - 40ms/step\n",
      "step  60/313 - loss: 0.1955 - acc: 0.9532 - 40ms/step\n",
      "step  70/313 - loss: 0.1262 - acc: 0.9519 - 39ms/step\n",
      "step  80/313 - loss: 0.1270 - acc: 0.9517 - 39ms/step\n",
      "step  90/313 - loss: 0.1346 - acc: 0.9514 - 38ms/step\n",
      "step 100/313 - loss: 0.0909 - acc: 0.9516 - 38ms/step\n",
      "step 110/313 - loss: 0.1378 - acc: 0.9520 - 38ms/step\n",
      "step 120/313 - loss: 0.1277 - acc: 0.9520 - 38ms/step\n",
      "step 130/313 - loss: 0.2406 - acc: 0.9515 - 38ms/step\n",
      "step 140/313 - loss: 0.1314 - acc: 0.9515 - 38ms/step\n",
      "step 150/313 - loss: 0.1605 - acc: 0.9508 - 38ms/step\n",
      "step 160/313 - loss: 0.1643 - acc: 0.9501 - 38ms/step\n",
      "step 170/313 - loss: 0.1569 - acc: 0.9497 - 38ms/step\n",
      "step 180/313 - loss: 0.1599 - acc: 0.9489 - 38ms/step\n",
      "step 190/313 - loss: 0.1829 - acc: 0.9487 - 38ms/step\n",
      "step 200/313 - loss: 0.1165 - acc: 0.9484 - 38ms/step\n",
      "step 210/313 - loss: 0.1941 - acc: 0.9475 - 38ms/step\n",
      "step 220/313 - loss: 0.1974 - acc: 0.9469 - 38ms/step\n",
      "step 230/313 - loss: 0.1035 - acc: 0.9469 - 38ms/step\n",
      "step 240/313 - loss: 0.1260 - acc: 0.9466 - 38ms/step\n",
      "step 250/313 - loss: 0.1233 - acc: 0.9469 - 38ms/step\n",
      "step 260/313 - loss: 0.0908 - acc: 0.9469 - 38ms/step\n",
      "step 270/313 - loss: 0.1533 - acc: 0.9467 - 38ms/step\n",
      "step 280/313 - loss: 0.1544 - acc: 0.9464 - 38ms/step\n",
      "step 290/313 - loss: 0.1601 - acc: 0.9464 - 38ms/step\n",
      "step 300/313 - loss: 0.1387 - acc: 0.9464 - 38ms/step\n",
      "step 310/313 - loss: 0.1235 - acc: 0.9462 - 37ms/step\n",
      "step 313/313 - loss: 0.0436 - acc: 0.9461 - 37ms/step\n",
      "save checkpoint at /home/aistudio/checkpoints/9\n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "step 10/40 - loss: 0.0886 - acc: 0.9710 - 42ms/step\n",
      "step 20/40 - loss: 0.1208 - acc: 0.9670 - 36ms/step\n",
      "step 30/40 - loss: 0.1234 - acc: 0.9665 - 34ms/step\n",
      "step 40/40 - loss: 0.1445 - acc: 0.9646 - 29ms/step\n",
      "Eval samples: 7802\n",
      "save checkpoint at /home/aistudio/checkpoints/final\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "save_dir='./checkpoints'\n",
    "# Starts training and evaluating.\n",
    "model.fit(\n",
    "        train_loader,\n",
    "        dev_loader,\n",
    "        epochs=epochs,\n",
    "        save_dir=save_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.5 模型评估\n",
    "\n",
    "+ 模型评估实用paddle高级API自带的evaluate函数就可以实现评估，评估的方式是accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\n",
      "step  10/122 - loss: 0.4908 - acc: 0.7703 - 24ms/step\n",
      "step  20/122 - loss: 0.4314 - acc: 0.7695 - 18ms/step\n",
      "step  30/122 - loss: 0.4691 - acc: 0.7760 - 16ms/step\n",
      "step  40/122 - loss: 0.5401 - acc: 0.7867 - 15ms/step\n",
      "step  50/122 - loss: 0.5670 - acc: 0.7866 - 15ms/step\n",
      "step  60/122 - loss: 0.4395 - acc: 0.7878 - 14ms/step\n",
      "step  70/122 - loss: 0.5325 - acc: 0.7895 - 14ms/step\n",
      "step  80/122 - loss: 0.5728 - acc: 0.7879 - 14ms/step\n",
      "step  90/122 - loss: 0.4095 - acc: 0.7905 - 13ms/step\n",
      "step 100/122 - loss: 0.4865 - acc: 0.7930 - 13ms/step\n",
      "step 110/122 - loss: 0.4447 - acc: 0.7923 - 13ms/step\n",
      "step 120/122 - loss: 0.5081 - acc: 0.7926 - 13ms/step\n",
      "step 122/122 - loss: 0.5163 - acc: 0.7918 - 13ms/step\n",
      "Eval samples: 7802\n",
      "Finally test acc: 0.79185\n"
     ]
    }
   ],
   "source": [
    "# Finally tests model.\n",
    "results = model.evaluate(dev_loader)\n",
    "print(\"Finally test acc: %.5f\" % results['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.6 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, data, label_map, batch_size=1, pad_token_id=0):\n",
    "    \"\"\"\n",
    "    预测数据的标签\n",
    "    \"\"\"\n",
    "    # 把数据分成若干个batch\n",
    "    batches = [\n",
    "        data[idx:idx + batch_size] for idx in range(0, len(data), batch_size)\n",
    "    ]\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=pad_token_id),  # query_ids\n",
    "        Pad(axis=0, pad_val=pad_token_id),  # title_ids\n",
    "        Stack(dtype=\"int64\"),  # query_seq_lens\n",
    "        Stack(dtype=\"int64\"),  # title_seq_lens\n",
    "    ): [data for data in fn(samples)]\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        query_ids, title_ids, query_seq_lens, title_seq_lens = batchify_fn(\n",
    "            batch)\n",
    "        query_ids = paddle.to_tensor(query_ids)\n",
    "        title_ids = paddle.to_tensor(title_ids)\n",
    "        query_seq_lens = paddle.to_tensor(query_seq_lens)\n",
    "        title_seq_lens = paddle.to_tensor(title_seq_lens)\n",
    "        logits = model(query_ids, title_ids, query_seq_lens, title_seq_lens)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 建立id到类别的映射，本实验有两个类别，编码为0，1。0类为dissimilar，1类为similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_map = {0: '0', 1: '1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "+ 加载训练的模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from ./checkpoints/final.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 实例化SimNet模型\n",
    "model = SimNet(network=network, vocab_size=len(vocab), num_classes=len(label_map))\n",
    "params_path='./checkpoints/final.pdparams'\n",
    "#  加载模型参数\n",
    "state_dict = paddle.load(params_path)\n",
    "model.set_dict(state_dict)\n",
    "print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_prediction_data(data, tokenizer):\n",
    "    examples = []\n",
    "    for query, title in data:\n",
    "        query_ids = tokenizer.encode(query)\n",
    "        title_ids = tokenizer.encode(title)\n",
    "        examples.append([query_ids, title_ids, len(query_ids), len(title_ids)])\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Firstly pre-processing prediction data  and then do predict.\n",
    "# 将data替换为我们给出的测试集test_forstu.tsv，输出预测结果result.tsv并提交。\n",
    "import pandas as pd\n",
    "data = [\n",
    "        ['世界上什么东西最小', '世界上什么东西最小？'],\n",
    "        ['光眼睛大就好看吗', '眼睛好看吗？'],\n",
    "        ['小蝌蚪找妈妈怎么样', '小蝌蚪找妈妈是谁画的'],\n",
    "    ]\n",
    "data = pd.read_csv('test_forstu.tsv', sep='\\t')\n",
    "data = data.values.tolist()\n",
    "examples = preprocess_prediction_data(data, tokenizer)\n",
    "\n",
    "results = predict(\n",
    "        model,\n",
    "        examples,\n",
    "        label_map=label_map,\n",
    "        batch_size=batch_size,\n",
    "        pad_token_id=vocab.token_to_idx.get('[PAD]', 0))\n",
    "\n",
    "list1 = []\n",
    "list2 = []\n",
    "list3 = []\n",
    "\n",
    "for idx, text in enumerate(data):\n",
    "    list1.append(text[0])\n",
    "    list2.append(text[1])\n",
    "    list3.append(results[idx])\n",
    "dict= {'text_a': list1, 'text_b': list2, 'label': list3}\n",
    "datas = pd.DataFrame(dict)\n",
    "datas.to_csv('result.csv',encoding='UTF-8',sep=\"\\t\")\n",
    "    #print('Data: {} \\t{}\\t Label: {}'.format(text[0],text[1], results[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.0.0b0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
