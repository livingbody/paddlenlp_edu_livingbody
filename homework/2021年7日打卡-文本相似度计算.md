# 2021年7日打卡营大作业

大家好，这里是2021年7日打卡营大作业，本次作业内容为**实现文本相似度任务**，通过课上所学知识，实现文本相似度任务的代码。**目前已经给出了基于SimilarityNet的相似度任务的实现代码，同学们可以基于本代码进行修改或者重写，实现更高的准确率**，其中数据集已经上传，如果有误操，可以联系qq群助教，获得数据集。

## 【作业内容】

按照本项目给出的模块完成代码并跑通。

## 【作业提交】
我们会给出一个数据集压缩包，包含`baidu_train.tsv`,`baidu_dev.tsv`,`vocab.txt`和`test_forstu.tsv`。其中`baidu_train.tsv`,`baidu_dev.tsv`,`vocab.txt`用来训练模型。最后大家根据`test_forstu.tsv`测试数据输出预测结果。命名为`result.tsv`的文件并提交。
>note:遇到问题请及时向qq群的助教反馈。

## 【评分标准】

**优秀学员：**

条件：
1. 完成每天的课后作业；
2. 大作业代码运行成功且有结果；
3. 准确率排名前26名。

奖品设置：
|等级|奖品|名额|
|---|---|---|
|一等奖|小度在家|1|
|二等奖|耳机 |5|
|三等奖|书 | 10|
|四等奖|健身包|10|
|鼓励奖|招财熊|170|

# 1. 任务介绍
## 1.1 任务内容

文本语义匹配是自然语言处理中一个重要的基础问题，NLP领域的很多任务都可以抽象为文本匹配任务。例如，信息检索可以归结为查询项和文档的匹配，问答系统可以归结为问题和候选答案的匹配，对话系统可以归结为对话和回复的匹配。语义匹配在搜索优化、推荐系统、快速检索排序、智能客服上都有广泛的应用。如何提升文本匹配的准确度，是自然语言处理领域的一个重要挑战。

+ 信息检索：在信息检索领域的很多应用中，都需要根据原文本来检索与其相似的其他文本，使用场景非常普遍。
+ 新闻推荐：通过用户刚刚浏览过的新闻标题，自动检索出其他的相似新闻，个性化地为用户做推荐，从而增强用户粘性，提升产品体验。
+ 智能客服：用户输入一个问题后，自动为用户检索出相似的问题和答案，节约人工客服的成本，提高效率。

## 1.2 什么是文本匹配？

让我们来看一个简单的例子，比较各候选句子哪句和原句语义更相近

原句：“车头如何放置车牌”

+ 比较句1：“前牌照怎么装”
+ 比较句2：“如何办理北京车牌”
+ 比较句3：“后牌照怎么装”

（1）比较句1与原句，虽然句式和语序等存在较大差异，但是所表述的含义几乎相同

（2）比较句2与原句，虽然存在“如何” 、“车牌”等共现词，但是所表述的含义完全不同

（3）比较句3与原句，二者讨论的都是如何放置车牌的问题，只不过一个是前牌照，另一个是后牌照。二者间存在一定的语义相关性。

所以语义相关性，句1大于句3，句3大于句2.这就是语义匹配。


## 1.3 短文本语义匹配网络

短文本语义匹配(SimilarityNet, SimNet)是一个计算短文本相似度的框架，可以根据用户输入的两个文本，计算出相似度得分。主要包括BOW、CNN、RNN、MMDNN等核心网络结构形式，提供语义相似度计算训练和预测框架，适用于信息检索、新闻推荐、智能客服等多个应用场景，帮助企业解决语义匹配问题。

SimNet模型结构如图所示，包括输入层、表示层以及匹配层。

<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/953b573d100b46fe84f026c99a3a2664fbecbe680fa748eda6337548a2f4b4af" width=50%></center>
<center><br>图 SimilarityNet 框架 </br></center>
<br></br>


## 1.4 实验设计

本任务给出一个文本相似度任务实现方法，模型框架结构图如下图所示，其中query和title是数据集经过处理后的待匹配的文本，然后经过分词处理，编码成id，经过SimilarityNet处理，得到输出，训练的损失函数使用的是交叉熵损失。

<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/df8c743f9b3742e4a99ae6b066c133c9b119f7499e3b4addb5ce755ec3faae7e" width='600px'></center>
<center><br>图 SimilarityNet实验流程 </br></center>
<br></br>


# 2. 实验详细实现
文本相似度实验流程包含如下6个步骤：
1. **数据读取**：根据网络接收的数据格式，完成相应的预处理操作，保证模型正常读取；
2. **模型构建**：设计基于GRU的文本相似度模型，判断两句话是否是相似；
3. **训练配置**：实例化模型，选择模型计算资源（CPU或者GPU），指定模型迭代的优化算法；
4. **模型训练与保存**：执行多轮训练不断调整参数，以达到较好的效果，保存模型；
5. **模型评估**：训练好的模型在测试集合上测试；
6. **模型预测**：加载训练好的模型，并进行预测；

## 2.1数据读取

本实验共计需要读取四份数据: 训练集 **train.tsv**、验证集 **dev.tsv**、测试集 **test.tsv** 和 词汇表 **vocab.txt**。加载数据的代码如下：

+ 加载第三方库，paddle和paddlenlp相关的库


```python
import math
import numpy as np
import os
import collections
from functools import partial
import random
import time
import inspect
import importlib
from tqdm import tqdm

import paddle
import paddle.nn as nn
import paddle.nn.functional as F
from paddle.io import IterableDataset
from paddle.utils.download import get_path_from_url
```

    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      def convert_to_list(value, n, name, dtype=np.int):


+ 本实验需要依赖与paddlenlp，aistudio上的paddlenlp版本过低，所以需要首先升级paddlenlp


```python
!pip install paddlenlp --upgrade
```

    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting paddlenlp
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)
    [K     |████████████████████████████████| 430kB 12.6MB/s eta 0:00:01
    [?25hCollecting multiprocess (from paddlenlp)
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/db/20/458ac043a57322365ac2ed86a911bf7598fc2e49bccb3f94ea810fbb6b9b/multiprocess-0.70.11.1-py37-none-any.whl (108kB)
    [K     |████████████████████████████████| 112kB 52.1MB/s eta 0:00:01
    [?25hRequirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)
    Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)
    Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)
    Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)
    Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)
    Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)
    Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)
    Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)
    Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)
    Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)
    Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)
    Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)
    Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)
    Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)
    Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)
    Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)
    Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)
    Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)
    Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)
    Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)
    Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)
    Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)
    Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)
    Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)
    Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)
    Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)
    Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)
    Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)
    Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)
    Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < "3.8" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)
    Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)
    Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)
    Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)
    Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)
    Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)
    Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)
    Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)
    Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)
    Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)
    Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)
    Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)
    Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)
    Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)
    Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)
    Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < "3.8"->pre-commit->visualdl->paddlenlp) (0.6.0)
    Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < "3.8"->pre-commit->visualdl->paddlenlp) (7.2.0)
    Installing collected packages: multiprocess, paddlenlp
      Found existing installation: paddlenlp 2.0.0rc7
        Uninstalling paddlenlp-2.0.0rc7:
          Successfully uninstalled paddlenlp-2.0.0rc7
    Successfully installed multiprocess-0.70.11.1 paddlenlp-2.0.2


+ 导入paddlenlp相关的包


```python
import paddlenlp as ppnlp
from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab
# from utils import convert_example
from paddlenlp.datasets import MapDataset
from paddle.dataset.common import md5file
from paddlenlp.datasets import DatasetBuilder
```


```python
def convert_example(example, tokenizer, is_test=False):
    """
    为序列分类任务从序列生成模型输入
    """
    
    query, title = example["query"], example["title"]
    # query id生成
    query_ids = np.array(tokenizer.encode(query), dtype="int64")
    query_seq_len = np.array(len(query_ids), dtype="int64")

    # title id 生成
    title_ids = np.array(tokenizer.encode(title), dtype="int64")
    title_seq_len = np.array(len(title_ids), dtype="int64")
    
    # 训练模式
    if not is_test:
        label = np.array(example["label"], dtype="int64")
        return query_ids, title_ids, query_seq_len, title_seq_len, label
    else:
        # 测试模式
        return query_ids, title_ids, query_seq_len, title_seq_len
```


```python
class BAIDUData(DatasetBuilder):

    SPLITS = {
        'train':os.path.join( 'baidu_train.tsv'),
        'dev': os.path.join('baidu_dev.tsv'),
    }

    def _get_data(self, mode, **kwargs):
        filename = self.SPLITS[mode]
        return filename

    def _read(self, filename):
        """读取数据"""
        with open(filename, 'r', encoding='utf-8') as f:
            head = None
            for line in f:
                data = line.strip().split("\t")
                if not head:
                    head = data
                else:
                    query, title, label = data
                    yield {"query": query, "title": title, "label": label}

    def get_labels(self):
        return ["0", "1"]
```


```python
def load_dataset(name=None,
                 data_files=None,
                 splits=None,
                 lazy=None,
                 **kwargs):
   
    reader_cls = BAIDUData
    print(reader_cls)
    if not name:
        reader_instance = reader_cls(lazy=lazy, **kwargs)
    else:
        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)

    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)
    return datasets
```

+ 加载词汇表


```python

vocab_path="vocab.txt"

vocab = Vocab.load_vocabulary(vocab_path, unk_token='[UNK]', pad_token='[PAD]')
```

+ 加载训练集，验证集，测试集


```python
# Loads dataset.
train_ds, dev_ds = load_dataset(splits=["train", "dev"])
```

    <class '__main__.BAIDUData'>


    2021-06-08 19:08:47,165 - INFO - unique_endpoints {''}


+ 创建dataloader


```python
def create_dataloader(dataset,
                      trans_fn=None,
                      mode='train',
                      batch_size=1, #1
                      batchify_fn=None):
    if trans_fn:
        dataset = dataset.map(trans_fn)

    shuffle = True if mode == 'train' else False
    if mode == "train":
        sampler = paddle.io.DistributedBatchSampler(
            dataset=dataset, batch_size=batch_size, shuffle=True)
    else:
        sampler = paddle.io.BatchSampler(
            dataset=dataset, batch_size=batch_size, shuffle=shuffle)
    dataloader = paddle.io.DataLoader(
        dataset,
        batch_sampler=sampler,
        return_list=True,
        collate_fn=batchify_fn)
    return dataloader
```


```python
# Reads data and generates mini-batches.
batchify_fn = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # query_ids
        Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # title_ids
        Stack(dtype="int64"),  # query_seq_lens
        Stack(dtype="int64"),  # title_seq_lens
        Stack(dtype="int64")  # label
    ): [data for data in fn(samples)]
tokenizer = ppnlp.data.JiebaTokenizer(vocab)

trans_fn = partial(convert_example, tokenizer=tokenizer, is_test=False)
```

## 2.2 模型构建

本节实验构建了基于GRU的SimilarityNet网络结构，如图所示。模型得输入是query和title，然后分别经过embedding层，GRU层，把GRU输出得向量拼接（concat操作），最后经过FC(全连接)层。
<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/645678222ebb4c87bcf8b53e4e4ecbdc64340506839e498c8a3ed12fe57c0686" width='500px'></center>
<center><br>图 基于GRU的SimNet网络结构 </br></center>
<br></br>


```python
class GRUEncoder(nn.Layer):

    def __init__(self,
                 input_size,
                 hidden_size,
                 num_layers=1,
                 direction="forward",
                 dropout=0.0,
                 pooling_type=None,
                 **kwargs):
        super().__init__()
        # 输入的大小
        self._input_size = input_size
        # 隐藏层的大小
        self._hidden_size = hidden_size
        self._direction = direction
        self._pooling_type = pooling_type
        # 实例化GRU层
        self.gru_layer = nn.GRU(input_size=input_size,
                                hidden_size=hidden_size,
                                num_layers=num_layers,
                                direction=direction,
                                dropout=dropout,
                                **kwargs)

    def get_input_dim(self):
        """
        Returns the dimension of the vector input for each element in the sequence input
        to a `GRUEncoder`. This is not the shape of the input tensor, but the
        last element of that shape.
        """
        return self._input_size

    def get_output_dim(self):
        """
        Returns the dimension of the final vector output by this `GRUEncoder`.  This is not
        the shape of the returned tensor, but the last element of that shape.
        """
        if self._direction == "bidirect":
            return self._hidden_size * 2
        else:
            return self._hidden_size

    def forward(self, inputs, sequence_length):
        
        encoded_text, last_hidden = self.gru_layer(
            inputs, sequence_length=sequence_length)
        if not self._pooling_type:
            # We exploit the `last_hidden` (the hidden state at the last time step for every layer)
            # to create a single vector.
            # If gru is not bidirection, then output is the hidden state of the last time step 
            # at last layer. Output is shape of `(batch_size, hidden_size)`.
            # If gru is bidirection, then output is concatenation of the forward and backward hidden state 
            # of the last time step at last layer. Output is shape of `(batch_size, hidden_size*2)`.
            if self._direction != 'bidirect':
                output = last_hidden[-1, :, :]
            else:
                output = paddle.concat(
                    (last_hidden[-2, :, :], last_hidden[-1, :, :]), axis=1)
        else:
            # We exploit the `encoded_text` (the hidden state at the every time step for last layer)
            # to create a single vector. We perform pooling on the encoded text.
            # The output shape is `(batch_size, hidden_size*2)` if use bidirectional GRU, 
            # otherwise the output shape is `(batch_size, hidden_size*2)`.
            if self._pooling_type == 'sum':
                output = paddle.sum(encoded_text, axis=1)
            elif self._pooling_type == 'max':
                output = paddle.max(encoded_text, axis=1)
            elif self._pooling_type == 'mean':
                output = paddle.mean(encoded_text, axis=1)
            else:
                raise RuntimeError(
                    "Unexpected pooling type %s ."
                    "Pooling type must be one of sum, max and mean." %
                    self._pooling_type)
        return output
```


```python
class GRUModel(nn.Layer):
    def __init__(self,
                 vocab_size,
                 num_classes,
                 emb_dim=128,
                 padding_idx=0,
                 gru_hidden_size=128,
                 direction='forward',
                 gru_layers=1,
                 dropout_rate=0.0,
                 pooling_type=None,
                 fc_hidden_size=96):
        super().__init__()
        # 词嵌入层
        self.embedder = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=emb_dim,
            padding_idx=padding_idx)
        # GRUb编码层
        self.gru_encoder = GRUEncoder(
            emb_dim,
            gru_hidden_size,
            num_layers=gru_layers,
            direction=direction,
            dropout=dropout_rate)
        # 线性层
        self.fc = nn.Linear(self.gru_encoder.get_output_dim() * 2,fc_hidden_size)
        # 输出层
        self.output_layer = nn.Linear(fc_hidden_size, num_classes)

    def forward(self, query, title, query_seq_len, title_seq_len):
        # Shape: (batch_size, num_tokens, embedding_dim)
        embedded_query = self.embedder(query)
        embedded_title = self.embedder(title)
        # Shape: (batch_size, gru_hidden_size)
        query_repr = self.gru_encoder(
            embedded_query, sequence_length=query_seq_len)
        title_repr = self.gru_encoder(
            embedded_title, sequence_length=title_seq_len)
        # Shape: (batch_size, 2*gru_hidden_size)
        contacted = paddle.concat([query_repr, title_repr], axis=-1)
        # Shape: (batch_size, fc_hidden_size)
        fc_out = paddle.tanh(self.fc(contacted))
        # Shape: (batch_size, num_classes)
        logits = self.output_layer(fc_out)
        # probs = F.softmax(logits, axis=-1)

        return logits
```


```python
class SimNet(nn.Layer):
    def __init__(self,
                 network,
                 vocab_size,
                 num_classes,
                 emb_dim=128,
                 pad_token_id=0):
        super().__init__()
        # 转换成小写
        network = network.lower()
        # gru网络
        if network == 'gru':
            self.model = GRUModel(
                vocab_size,
                num_classes,
                emb_dim,
                direction='forward',
                padding_idx=pad_token_id)

    def forward(self, query, title, query_seq_len=None, title_seq_len=None):
        logits = self.model(query, title, query_seq_len, title_seq_len)
        return logits
```

## 2.3 训练配置


```python
# 检测是否可以使用GPU，如果可以优先使用GPU
use_gpu = True if paddle.get_device().startswith("gpu") else False
if use_gpu:
    paddle.set_device('gpu')
    #print('gpu')
batch_size=200   # batch size 的大小
```

+ 实例化SimilarityNet网络


```python
# 构建SimNet网络，网络的主体采用GRUb网络
network='gru'
model = SimNet(
        network=network,
        vocab_size=len(vocab),
        num_classes=len(train_ds.label_list))
model = paddle.Model(model)
```

+ 构建训练集，验证集和测试集的dataloader


```python
# 构建训练集的dataloader
train_loader = create_dataloader(
        train_ds,
        trans_fn=trans_fn,
        batch_size=batch_size,
        mode='train',
        batchify_fn=batchify_fn)
# 构建验证集的dataloader
dev_loader = create_dataloader(
        dev_ds,
        trans_fn=trans_fn,
        batch_size=batch_size,
        mode='validation',
        batchify_fn=batchify_fn)
# # 构建测试集的dataloader
# test_loader = create_dataloader(
#         test_ds,
#         trans_fn=trans_fn,
#         batch_size=batch_size,
#         mode='test',
#         batchify_fn=batchify_fn)
```

+ 定义优化器


```python
lr=5e-4
optimizer = paddle.optimizer.Adam(
        parameters=model.parameters(), learning_rate=lr)
```

+ 实例化损失函数


```python
# Defines loss and metric.
criterion = paddle.nn.CrossEntropyLoss()  # 交叉熵损失函数
```

+ 实例化评估方式


```python
metric = paddle.metric.Accuracy()  # accuracy 评估方式
```

+ 编译模型


```python
model.prepare(optimizer, criterion, metric)
```

## 2.4 模型训练

+ 模型训练使用paddle的高级API，使用fit函数就可以实现训练和模型保存，模型保存的位置为save_dir


```python
epochs=10
save_dir='./checkpoints'
# Starts training and evaluating.
model.fit(
        train_loader,
        dev_loader,
        epochs=epochs,
        save_dir=save_dir
        )
```

    The loss value printed in the log is the current step, and the metric is the average value of previous step.
    Epoch 1/10


    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
      return (isinstance(seq, collections.Sequence) and


    step  10/313 - loss: 0.6779 - acc: 0.5560 - 71ms/step
    step  20/313 - loss: 0.6775 - acc: 0.5620 - 54ms/step
    step  30/313 - loss: 0.6709 - acc: 0.5600 - 49ms/step
    step  40/313 - loss: 0.6698 - acc: 0.5779 - 46ms/step
    step  50/313 - loss: 0.6427 - acc: 0.5852 - 44ms/step
    step  60/313 - loss: 0.6441 - acc: 0.5972 - 43ms/step
    step  70/313 - loss: 0.6351 - acc: 0.6081 - 42ms/step
    step  80/313 - loss: 0.5961 - acc: 0.6187 - 42ms/step
    step  90/313 - loss: 0.5682 - acc: 0.6259 - 42ms/step
    step 100/313 - loss: 0.6386 - acc: 0.6305 - 41ms/step
    step 110/313 - loss: 0.5877 - acc: 0.6357 - 41ms/step
    step 120/313 - loss: 0.5476 - acc: 0.6404 - 41ms/step
    step 130/313 - loss: 0.5689 - acc: 0.6439 - 40ms/step
    step 140/313 - loss: 0.5621 - acc: 0.6487 - 40ms/step
    step 150/313 - loss: 0.5484 - acc: 0.6526 - 40ms/step
    step 160/313 - loss: 0.5292 - acc: 0.6560 - 40ms/step
    step 170/313 - loss: 0.5861 - acc: 0.6589 - 39ms/step
    step 180/313 - loss: 0.5605 - acc: 0.6619 - 39ms/step
    step 190/313 - loss: 0.5725 - acc: 0.6634 - 39ms/step
    step 200/313 - loss: 0.5240 - acc: 0.6660 - 39ms/step
    step 210/313 - loss: 0.5724 - acc: 0.6684 - 39ms/step
    step 220/313 - loss: 0.5520 - acc: 0.6701 - 39ms/step
    step 230/313 - loss: 0.5021 - acc: 0.6723 - 39ms/step
    step 240/313 - loss: 0.4600 - acc: 0.6744 - 39ms/step
    step 250/313 - loss: 0.5444 - acc: 0.6758 - 39ms/step
    step 260/313 - loss: 0.5799 - acc: 0.6770 - 39ms/step
    step 270/313 - loss: 0.5113 - acc: 0.6786 - 39ms/step
    step 280/313 - loss: 0.4968 - acc: 0.6802 - 39ms/step
    step 290/313 - loss: 0.5544 - acc: 0.6815 - 39ms/step
    step 300/313 - loss: 0.5375 - acc: 0.6823 - 39ms/step
    step 310/313 - loss: 0.5395 - acc: 0.6842 - 38ms/step
    step 313/313 - loss: 0.5030 - acc: 0.6843 - 38ms/step
    save checkpoint at /home/aistudio/checkpoints/0
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.4852 - acc: 0.7650 - 45ms/step
    step 20/40 - loss: 0.4746 - acc: 0.7752 - 37ms/step
    step 30/40 - loss: 0.4398 - acc: 0.7763 - 34ms/step
    step 40/40 - loss: 0.8953 - acc: 0.7793 - 29ms/step
    Eval samples: 7802
    Epoch 2/10
    step  10/313 - loss: 0.4326 - acc: 0.7815 - 50ms/step
    step  20/313 - loss: 0.4775 - acc: 0.7855 - 43ms/step
    step  30/313 - loss: 0.5019 - acc: 0.7822 - 40ms/step
    step  40/313 - loss: 0.4167 - acc: 0.7849 - 39ms/step
    step  50/313 - loss: 0.5203 - acc: 0.7799 - 39ms/step
    step  60/313 - loss: 0.4360 - acc: 0.7805 - 39ms/step
    step  70/313 - loss: 0.4256 - acc: 0.7804 - 38ms/step
    step  80/313 - loss: 0.5098 - acc: 0.7797 - 38ms/step
    step  90/313 - loss: 0.4505 - acc: 0.7802 - 38ms/step
    step 100/313 - loss: 0.5386 - acc: 0.7798 - 38ms/step
    step 110/313 - loss: 0.3901 - acc: 0.7804 - 37ms/step
    step 120/313 - loss: 0.5151 - acc: 0.7810 - 37ms/step
    step 130/313 - loss: 0.4669 - acc: 0.7808 - 37ms/step
    step 140/313 - loss: 0.5065 - acc: 0.7811 - 37ms/step
    step 150/313 - loss: 0.4478 - acc: 0.7807 - 37ms/step
    step 160/313 - loss: 0.4828 - acc: 0.7816 - 37ms/step
    step 170/313 - loss: 0.4707 - acc: 0.7814 - 37ms/step
    step 180/313 - loss: 0.4508 - acc: 0.7816 - 37ms/step
    step 190/313 - loss: 0.4934 - acc: 0.7808 - 37ms/step
    step 200/313 - loss: 0.4777 - acc: 0.7797 - 37ms/step
    step 210/313 - loss: 0.5268 - acc: 0.7788 - 37ms/step
    step 220/313 - loss: 0.4566 - acc: 0.7787 - 37ms/step
    step 230/313 - loss: 0.4869 - acc: 0.7783 - 37ms/step
    step 240/313 - loss: 0.5303 - acc: 0.7784 - 37ms/step
    step 250/313 - loss: 0.5559 - acc: 0.7783 - 37ms/step
    step 260/313 - loss: 0.4952 - acc: 0.7775 - 37ms/step
    step 270/313 - loss: 0.4588 - acc: 0.7774 - 37ms/step
    step 280/313 - loss: 0.5450 - acc: 0.7772 - 37ms/step
    step 290/313 - loss: 0.4265 - acc: 0.7778 - 37ms/step
    step 300/313 - loss: 0.5427 - acc: 0.7781 - 37ms/step
    step 310/313 - loss: 0.5577 - acc: 0.7780 - 37ms/step
    step 313/313 - loss: 0.4183 - acc: 0.7780 - 36ms/step
    save checkpoint at /home/aistudio/checkpoints/1
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.3883 - acc: 0.8435 - 41ms/step
    step 20/40 - loss: 0.3899 - acc: 0.8440 - 34ms/step
    step 30/40 - loss: 0.3690 - acc: 0.8463 - 32ms/step
    step 40/40 - loss: 0.7862 - acc: 0.8472 - 28ms/step
    Eval samples: 7802
    Epoch 3/10
    step  10/313 - loss: 0.4020 - acc: 0.8515 - 50ms/step
    step  20/313 - loss: 0.2510 - acc: 0.8470 - 43ms/step
    step  30/313 - loss: 0.4031 - acc: 0.8463 - 41ms/step
    step  40/313 - loss: 0.3762 - acc: 0.8464 - 39ms/step
    step  50/313 - loss: 0.3229 - acc: 0.8437 - 39ms/step
    step  60/313 - loss: 0.4479 - acc: 0.8409 - 38ms/step
    step  70/313 - loss: 0.3726 - acc: 0.8391 - 38ms/step
    step  80/313 - loss: 0.4101 - acc: 0.8391 - 38ms/step
    step  90/313 - loss: 0.3670 - acc: 0.8381 - 38ms/step
    step 100/313 - loss: 0.3189 - acc: 0.8377 - 38ms/step
    step 110/313 - loss: 0.3986 - acc: 0.8376 - 38ms/step
    step 120/313 - loss: 0.3410 - acc: 0.8371 - 37ms/step
    step 130/313 - loss: 0.3904 - acc: 0.8371 - 37ms/step
    step 140/313 - loss: 0.3792 - acc: 0.8365 - 37ms/step
    step 150/313 - loss: 0.4145 - acc: 0.8358 - 37ms/step
    step 160/313 - loss: 0.3486 - acc: 0.8357 - 37ms/step
    step 170/313 - loss: 0.3987 - acc: 0.8349 - 37ms/step
    step 180/313 - loss: 0.3979 - acc: 0.8339 - 37ms/step
    step 190/313 - loss: 0.4143 - acc: 0.8342 - 37ms/step
    step 200/313 - loss: 0.3647 - acc: 0.8338 - 37ms/step
    step 210/313 - loss: 0.4422 - acc: 0.8331 - 37ms/step
    step 220/313 - loss: 0.3823 - acc: 0.8324 - 37ms/step
    step 230/313 - loss: 0.4111 - acc: 0.8318 - 37ms/step
    step 240/313 - loss: 0.4629 - acc: 0.8319 - 37ms/step
    step 250/313 - loss: 0.3643 - acc: 0.8312 - 37ms/step
    step 260/313 - loss: 0.4079 - acc: 0.8308 - 37ms/step
    step 270/313 - loss: 0.4235 - acc: 0.8305 - 37ms/step
    step 280/313 - loss: 0.3720 - acc: 0.8304 - 37ms/step
    step 290/313 - loss: 0.4635 - acc: 0.8302 - 37ms/step
    step 300/313 - loss: 0.3411 - acc: 0.8298 - 37ms/step
    step 310/313 - loss: 0.3830 - acc: 0.8289 - 37ms/step
    step 313/313 - loss: 0.7811 - acc: 0.8287 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/2
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.2885 - acc: 0.8880 - 43ms/step
    step 20/40 - loss: 0.3043 - acc: 0.8885 - 36ms/step
    step 30/40 - loss: 0.2885 - acc: 0.8885 - 34ms/step
    step 40/40 - loss: 0.4380 - acc: 0.8894 - 29ms/step
    Eval samples: 7802
    Epoch 4/10
    step  10/313 - loss: 0.2928 - acc: 0.8875 - 49ms/step
    step  20/313 - loss: 0.2682 - acc: 0.8855 - 44ms/step
    step  30/313 - loss: 0.3044 - acc: 0.8835 - 42ms/step
    step  40/313 - loss: 0.3253 - acc: 0.8832 - 40ms/step
    step  50/313 - loss: 0.3368 - acc: 0.8811 - 40ms/step
    step  60/313 - loss: 0.3091 - acc: 0.8802 - 39ms/step
    step  70/313 - loss: 0.2681 - acc: 0.8816 - 39ms/step
    step  80/313 - loss: 0.3478 - acc: 0.8801 - 39ms/step
    step  90/313 - loss: 0.2915 - acc: 0.8805 - 38ms/step
    step 100/313 - loss: 0.3208 - acc: 0.8796 - 38ms/step
    step 110/313 - loss: 0.2509 - acc: 0.8790 - 38ms/step
    step 120/313 - loss: 0.3732 - acc: 0.8788 - 38ms/step
    step 130/313 - loss: 0.2936 - acc: 0.8772 - 38ms/step
    step 140/313 - loss: 0.4708 - acc: 0.8749 - 38ms/step
    step 150/313 - loss: 0.3266 - acc: 0.8744 - 38ms/step
    step 160/313 - loss: 0.2815 - acc: 0.8739 - 38ms/step
    step 170/313 - loss: 0.2958 - acc: 0.8735 - 38ms/step
    step 180/313 - loss: 0.2897 - acc: 0.8727 - 38ms/step
    step 190/313 - loss: 0.2979 - acc: 0.8724 - 38ms/step
    step 200/313 - loss: 0.2620 - acc: 0.8718 - 38ms/step
    step 210/313 - loss: 0.2769 - acc: 0.8707 - 38ms/step
    step 220/313 - loss: 0.3142 - acc: 0.8705 - 38ms/step
    step 230/313 - loss: 0.3019 - acc: 0.8703 - 38ms/step
    step 240/313 - loss: 0.3372 - acc: 0.8700 - 38ms/step
    step 250/313 - loss: 0.3072 - acc: 0.8693 - 38ms/step
    step 260/313 - loss: 0.3214 - acc: 0.8692 - 38ms/step
    step 270/313 - loss: 0.3697 - acc: 0.8680 - 38ms/step
    step 280/313 - loss: 0.4037 - acc: 0.8675 - 38ms/step
    step 290/313 - loss: 0.4131 - acc: 0.8671 - 38ms/step
    step 300/313 - loss: 0.3185 - acc: 0.8665 - 38ms/step
    step 310/313 - loss: 0.3101 - acc: 0.8662 - 38ms/step
    step 313/313 - loss: 0.2654 - acc: 0.8661 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/3
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.2280 - acc: 0.9195 - 41ms/step
    step 20/40 - loss: 0.2821 - acc: 0.9120 - 35ms/step
    step 30/40 - loss: 0.2366 - acc: 0.9115 - 33ms/step
    step 40/40 - loss: 0.3791 - acc: 0.9096 - 28ms/step
    Eval samples: 7802
    Epoch 5/10
    step  10/313 - loss: 0.2273 - acc: 0.9120 - 51ms/step
    step  20/313 - loss: 0.2184 - acc: 0.9075 - 44ms/step
    step  30/313 - loss: 0.1705 - acc: 0.9085 - 42ms/step
    step  40/313 - loss: 0.2454 - acc: 0.9080 - 40ms/step
    step  50/313 - loss: 0.2804 - acc: 0.9109 - 40ms/step
    step  60/313 - loss: 0.2295 - acc: 0.9094 - 39ms/step
    step  70/313 - loss: 0.2070 - acc: 0.9100 - 39ms/step
    step  80/313 - loss: 0.2257 - acc: 0.9090 - 38ms/step
    step  90/313 - loss: 0.2572 - acc: 0.9091 - 38ms/step
    step 100/313 - loss: 0.2887 - acc: 0.9078 - 38ms/step
    step 110/313 - loss: 0.2616 - acc: 0.9072 - 38ms/step
    step 120/313 - loss: 0.3199 - acc: 0.9058 - 38ms/step
    step 130/313 - loss: 0.1998 - acc: 0.9056 - 38ms/step
    step 140/313 - loss: 0.2293 - acc: 0.9050 - 37ms/step
    step 150/313 - loss: 0.2421 - acc: 0.9040 - 37ms/step
    step 160/313 - loss: 0.3262 - acc: 0.9026 - 37ms/step
    step 170/313 - loss: 0.2710 - acc: 0.9013 - 37ms/step
    step 180/313 - loss: 0.2383 - acc: 0.9012 - 37ms/step
    step 190/313 - loss: 0.3109 - acc: 0.9011 - 37ms/step
    step 200/313 - loss: 0.2651 - acc: 0.9001 - 37ms/step
    step 210/313 - loss: 0.2629 - acc: 0.8995 - 37ms/step
    step 220/313 - loss: 0.2654 - acc: 0.8987 - 37ms/step
    step 230/313 - loss: 0.2675 - acc: 0.8979 - 37ms/step
    step 240/313 - loss: 0.2811 - acc: 0.8974 - 37ms/step
    step 250/313 - loss: 0.2632 - acc: 0.8965 - 37ms/step
    step 260/313 - loss: 0.3469 - acc: 0.8958 - 37ms/step
    step 270/313 - loss: 0.2688 - acc: 0.8954 - 37ms/step
    step 280/313 - loss: 0.2639 - acc: 0.8947 - 37ms/step
    step 290/313 - loss: 0.3070 - acc: 0.8941 - 37ms/step
    step 300/313 - loss: 0.2988 - acc: 0.8939 - 37ms/step
    step 310/313 - loss: 0.3038 - acc: 0.8934 - 37ms/step
    step 313/313 - loss: 0.1511 - acc: 0.8934 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/4
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.1704 - acc: 0.9340 - 46ms/step
    step 20/40 - loss: 0.2443 - acc: 0.9275 - 38ms/step
    step 30/40 - loss: 0.1940 - acc: 0.9272 - 35ms/step
    step 40/40 - loss: 0.5411 - acc: 0.9268 - 29ms/step
    Eval samples: 7802
    Epoch 6/10
    step  10/313 - loss: 0.1581 - acc: 0.9380 - 54ms/step
    step  20/313 - loss: 0.1754 - acc: 0.9335 - 47ms/step
    step  30/313 - loss: 0.2125 - acc: 0.9312 - 45ms/step
    step  40/313 - loss: 0.1296 - acc: 0.9294 - 43ms/step
    step  50/313 - loss: 0.1597 - acc: 0.9275 - 42ms/step
    step  60/313 - loss: 0.1716 - acc: 0.9258 - 42ms/step
    step  70/313 - loss: 0.2207 - acc: 0.9248 - 42ms/step
    step  80/313 - loss: 0.1764 - acc: 0.9261 - 42ms/step
    step  90/313 - loss: 0.1768 - acc: 0.9259 - 41ms/step
    step 100/313 - loss: 0.2047 - acc: 0.9258 - 41ms/step
    step 110/313 - loss: 0.2370 - acc: 0.9243 - 41ms/step
    step 120/313 - loss: 0.2340 - acc: 0.9238 - 41ms/step
    step 130/313 - loss: 0.1979 - acc: 0.9233 - 41ms/step
    step 140/313 - loss: 0.2778 - acc: 0.9223 - 41ms/step
    step 150/313 - loss: 0.2230 - acc: 0.9211 - 41ms/step
    step 160/313 - loss: 0.2481 - acc: 0.9201 - 41ms/step
    step 170/313 - loss: 0.2304 - acc: 0.9192 - 41ms/step
    step 180/313 - loss: 0.1882 - acc: 0.9191 - 40ms/step
    step 190/313 - loss: 0.2794 - acc: 0.9182 - 40ms/step
    step 200/313 - loss: 0.2881 - acc: 0.9172 - 40ms/step
    step 210/313 - loss: 0.2870 - acc: 0.9167 - 40ms/step
    step 220/313 - loss: 0.2170 - acc: 0.9165 - 40ms/step
    step 230/313 - loss: 0.2473 - acc: 0.9158 - 40ms/step
    step 240/313 - loss: 0.1799 - acc: 0.9149 - 40ms/step
    step 250/313 - loss: 0.2647 - acc: 0.9145 - 40ms/step
    step 260/313 - loss: 0.2410 - acc: 0.9142 - 40ms/step
    step 270/313 - loss: 0.2469 - acc: 0.9134 - 40ms/step
    step 280/313 - loss: 0.2352 - acc: 0.9131 - 40ms/step
    step 290/313 - loss: 0.1969 - acc: 0.9126 - 39ms/step
    step 300/313 - loss: 0.2011 - acc: 0.9119 - 39ms/step
    step 310/313 - loss: 0.2766 - acc: 0.9116 - 39ms/step
    step 313/313 - loss: 0.1941 - acc: 0.9113 - 39ms/step
    save checkpoint at /home/aistudio/checkpoints/5
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.1364 - acc: 0.9405 - 41ms/step
    step 20/40 - loss: 0.1778 - acc: 0.9390 - 34ms/step
    step 30/40 - loss: 0.1743 - acc: 0.9380 - 32ms/step
    step 40/40 - loss: 0.5655 - acc: 0.9368 - 28ms/step
    Eval samples: 7802
    Epoch 7/10
    step  10/313 - loss: 0.1356 - acc: 0.9370 - 49ms/step
    step  20/313 - loss: 0.1708 - acc: 0.9410 - 44ms/step
    step  30/313 - loss: 0.1759 - acc: 0.9422 - 41ms/step
    step  40/313 - loss: 0.2352 - acc: 0.9420 - 40ms/step
    step  50/313 - loss: 0.1469 - acc: 0.9404 - 40ms/step
    step  60/313 - loss: 0.1239 - acc: 0.9413 - 40ms/step
    step  70/313 - loss: 0.1251 - acc: 0.9416 - 39ms/step
    step  80/313 - loss: 0.2329 - acc: 0.9407 - 39ms/step
    step  90/313 - loss: 0.1653 - acc: 0.9394 - 39ms/step
    step 100/313 - loss: 0.1765 - acc: 0.9384 - 39ms/step
    step 110/313 - loss: 0.1899 - acc: 0.9376 - 38ms/step
    step 120/313 - loss: 0.2282 - acc: 0.9367 - 38ms/step
    step 130/313 - loss: 0.1671 - acc: 0.9358 - 38ms/step
    step 140/313 - loss: 0.1461 - acc: 0.9350 - 38ms/step
    step 150/313 - loss: 0.1666 - acc: 0.9338 - 38ms/step
    step 160/313 - loss: 0.2081 - acc: 0.9325 - 38ms/step
    step 170/313 - loss: 0.1975 - acc: 0.9314 - 38ms/step
    step 180/313 - loss: 0.2556 - acc: 0.9302 - 38ms/step
    step 190/313 - loss: 0.1919 - acc: 0.9297 - 38ms/step
    step 200/313 - loss: 0.1781 - acc: 0.9294 - 38ms/step
    step 210/313 - loss: 0.2554 - acc: 0.9284 - 38ms/step
    step 220/313 - loss: 0.1822 - acc: 0.9281 - 38ms/step
    step 230/313 - loss: 0.1802 - acc: 0.9276 - 38ms/step
    step 240/313 - loss: 0.1953 - acc: 0.9273 - 38ms/step
    step 250/313 - loss: 0.1916 - acc: 0.9269 - 38ms/step
    step 260/313 - loss: 0.2343 - acc: 0.9263 - 38ms/step
    step 270/313 - loss: 0.2569 - acc: 0.9256 - 38ms/step
    step 280/313 - loss: 0.1868 - acc: 0.9248 - 37ms/step
    step 290/313 - loss: 0.2784 - acc: 0.9243 - 37ms/step
    step 300/313 - loss: 0.2176 - acc: 0.9238 - 37ms/step
    step 310/313 - loss: 0.2597 - acc: 0.9236 - 37ms/step
    step 313/313 - loss: 0.1152 - acc: 0.9237 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/6
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.1167 - acc: 0.9500 - 45ms/step
    step 20/40 - loss: 0.1898 - acc: 0.9425 - 37ms/step
    step 30/40 - loss: 0.1393 - acc: 0.9447 - 35ms/step
    step 40/40 - loss: 0.5776 - acc: 0.9442 - 30ms/step
    Eval samples: 7802
    Epoch 8/10
    step  10/313 - loss: 0.1116 - acc: 0.9545 - 47ms/step
    step  20/313 - loss: 0.1921 - acc: 0.9493 - 41ms/step
    step  30/313 - loss: 0.1522 - acc: 0.9478 - 40ms/step
    step  40/313 - loss: 0.1188 - acc: 0.9480 - 39ms/step
    step  50/313 - loss: 0.1279 - acc: 0.9478 - 38ms/step
    step  60/313 - loss: 0.1761 - acc: 0.9484 - 38ms/step
    step  70/313 - loss: 0.2476 - acc: 0.9473 - 37ms/step
    step  80/313 - loss: 0.1823 - acc: 0.9474 - 37ms/step
    step  90/313 - loss: 0.1562 - acc: 0.9464 - 37ms/step
    step 100/313 - loss: 0.1696 - acc: 0.9451 - 37ms/step
    step 110/313 - loss: 0.1978 - acc: 0.9443 - 37ms/step
    step 120/313 - loss: 0.1863 - acc: 0.9445 - 37ms/step
    step 130/313 - loss: 0.1715 - acc: 0.9432 - 37ms/step
    step 140/313 - loss: 0.1826 - acc: 0.9425 - 37ms/step
    step 150/313 - loss: 0.2051 - acc: 0.9412 - 37ms/step
    step 160/313 - loss: 0.1343 - acc: 0.9406 - 37ms/step
    step 170/313 - loss: 0.1123 - acc: 0.9401 - 37ms/step
    step 180/313 - loss: 0.1128 - acc: 0.9400 - 37ms/step
    step 190/313 - loss: 0.2686 - acc: 0.9394 - 37ms/step
    step 200/313 - loss: 0.0916 - acc: 0.9395 - 37ms/step
    step 210/313 - loss: 0.1566 - acc: 0.9389 - 37ms/step
    step 220/313 - loss: 0.2082 - acc: 0.9380 - 37ms/step
    step 230/313 - loss: 0.1506 - acc: 0.9375 - 37ms/step
    step 240/313 - loss: 0.2126 - acc: 0.9372 - 37ms/step
    step 250/313 - loss: 0.2126 - acc: 0.9368 - 37ms/step
    step 260/313 - loss: 0.1755 - acc: 0.9362 - 37ms/step
    step 270/313 - loss: 0.2091 - acc: 0.9358 - 37ms/step
    step 280/313 - loss: 0.1718 - acc: 0.9357 - 37ms/step
    step 290/313 - loss: 0.2018 - acc: 0.9353 - 37ms/step
    step 300/313 - loss: 0.2005 - acc: 0.9347 - 37ms/step
    step 310/313 - loss: 0.2185 - acc: 0.9343 - 36ms/step
    step 313/313 - loss: 0.5939 - acc: 0.9341 - 36ms/step
    save checkpoint at /home/aistudio/checkpoints/7
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.0999 - acc: 0.9615 - 42ms/step
    step 20/40 - loss: 0.1513 - acc: 0.9550 - 35ms/step
    step 30/40 - loss: 0.1454 - acc: 0.9533 - 33ms/step
    step 40/40 - loss: 0.2175 - acc: 0.9535 - 28ms/step
    Eval samples: 7802
    Epoch 9/10
    step  10/313 - loss: 0.1940 - acc: 0.9500 - 48ms/step
    step  20/313 - loss: 0.0704 - acc: 0.9540 - 42ms/step
    step  30/313 - loss: 0.1606 - acc: 0.9525 - 40ms/step
    step  40/313 - loss: 0.1312 - acc: 0.9520 - 39ms/step
    step  50/313 - loss: 0.1390 - acc: 0.9511 - 39ms/step
    step  60/313 - loss: 0.0732 - acc: 0.9513 - 38ms/step
    step  70/313 - loss: 0.1018 - acc: 0.9504 - 38ms/step
    step  80/313 - loss: 0.1107 - acc: 0.9496 - 38ms/step
    step  90/313 - loss: 0.1057 - acc: 0.9498 - 38ms/step
    step 100/313 - loss: 0.1056 - acc: 0.9495 - 38ms/step
    step 110/313 - loss: 0.1293 - acc: 0.9497 - 37ms/step
    step 120/313 - loss: 0.1279 - acc: 0.9486 - 37ms/step
    step 130/313 - loss: 0.1521 - acc: 0.9481 - 37ms/step
    step 140/313 - loss: 0.1171 - acc: 0.9473 - 37ms/step
    step 150/313 - loss: 0.1414 - acc: 0.9469 - 37ms/step
    step 160/313 - loss: 0.1528 - acc: 0.9463 - 37ms/step
    step 170/313 - loss: 0.2263 - acc: 0.9453 - 37ms/step
    step 180/313 - loss: 0.1872 - acc: 0.9444 - 37ms/step
    step 190/313 - loss: 0.1718 - acc: 0.9438 - 37ms/step
    step 200/313 - loss: 0.2349 - acc: 0.9435 - 37ms/step
    step 210/313 - loss: 0.1463 - acc: 0.9431 - 37ms/step
    step 220/313 - loss: 0.1457 - acc: 0.9427 - 37ms/step
    step 230/313 - loss: 0.1101 - acc: 0.9426 - 38ms/step
    step 240/313 - loss: 0.1558 - acc: 0.9421 - 38ms/step
    step 250/313 - loss: 0.1483 - acc: 0.9416 - 38ms/step
    step 260/313 - loss: 0.1467 - acc: 0.9416 - 38ms/step
    step 270/313 - loss: 0.1594 - acc: 0.9413 - 38ms/step
    step 280/313 - loss: 0.1795 - acc: 0.9411 - 38ms/step
    step 290/313 - loss: 0.1909 - acc: 0.9408 - 37ms/step
    step 300/313 - loss: 0.1435 - acc: 0.9405 - 37ms/step
    step 310/313 - loss: 0.1683 - acc: 0.9405 - 37ms/step
    step 313/313 - loss: 0.3363 - acc: 0.9405 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/8
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.0799 - acc: 0.9670 - 43ms/step
    step 20/40 - loss: 0.1404 - acc: 0.9595 - 36ms/step
    step 30/40 - loss: 0.1395 - acc: 0.9573 - 34ms/step
    step 40/40 - loss: 0.2501 - acc: 0.9578 - 29ms/step
    Eval samples: 7802
    Epoch 10/10
    step  10/313 - loss: 0.1660 - acc: 0.9520 - 49ms/step
    step  20/313 - loss: 0.1313 - acc: 0.9535 - 43ms/step
    step  30/313 - loss: 0.1238 - acc: 0.9538 - 41ms/step
    step  40/313 - loss: 0.1803 - acc: 0.9527 - 40ms/step
    step  50/313 - loss: 0.1018 - acc: 0.9531 - 40ms/step
    step  60/313 - loss: 0.1955 - acc: 0.9532 - 40ms/step
    step  70/313 - loss: 0.1262 - acc: 0.9519 - 39ms/step
    step  80/313 - loss: 0.1270 - acc: 0.9517 - 39ms/step
    step  90/313 - loss: 0.1346 - acc: 0.9514 - 38ms/step
    step 100/313 - loss: 0.0909 - acc: 0.9516 - 38ms/step
    step 110/313 - loss: 0.1378 - acc: 0.9520 - 38ms/step
    step 120/313 - loss: 0.1277 - acc: 0.9520 - 38ms/step
    step 130/313 - loss: 0.2406 - acc: 0.9515 - 38ms/step
    step 140/313 - loss: 0.1314 - acc: 0.9515 - 38ms/step
    step 150/313 - loss: 0.1605 - acc: 0.9508 - 38ms/step
    step 160/313 - loss: 0.1643 - acc: 0.9501 - 38ms/step
    step 170/313 - loss: 0.1569 - acc: 0.9497 - 38ms/step
    step 180/313 - loss: 0.1599 - acc: 0.9489 - 38ms/step
    step 190/313 - loss: 0.1829 - acc: 0.9487 - 38ms/step
    step 200/313 - loss: 0.1165 - acc: 0.9484 - 38ms/step
    step 210/313 - loss: 0.1941 - acc: 0.9475 - 38ms/step
    step 220/313 - loss: 0.1974 - acc: 0.9469 - 38ms/step
    step 230/313 - loss: 0.1035 - acc: 0.9469 - 38ms/step
    step 240/313 - loss: 0.1260 - acc: 0.9466 - 38ms/step
    step 250/313 - loss: 0.1233 - acc: 0.9469 - 38ms/step
    step 260/313 - loss: 0.0908 - acc: 0.9469 - 38ms/step
    step 270/313 - loss: 0.1533 - acc: 0.9467 - 38ms/step
    step 280/313 - loss: 0.1544 - acc: 0.9464 - 38ms/step
    step 290/313 - loss: 0.1601 - acc: 0.9464 - 38ms/step
    step 300/313 - loss: 0.1387 - acc: 0.9464 - 38ms/step
    step 310/313 - loss: 0.1235 - acc: 0.9462 - 37ms/step
    step 313/313 - loss: 0.0436 - acc: 0.9461 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/9
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.0886 - acc: 0.9710 - 42ms/step
    step 20/40 - loss: 0.1208 - acc: 0.9670 - 36ms/step
    step 30/40 - loss: 0.1234 - acc: 0.9665 - 34ms/step
    step 40/40 - loss: 0.1445 - acc: 0.9646 - 29ms/step
    Eval samples: 7802
    save checkpoint at /home/aistudio/checkpoints/final


## 2.5 模型评估

+ 模型评估实用paddle高级API自带的evaluate函数就可以实现评估，评估的方式是accuracy


```python
# Finally tests model.
results = model.evaluate(dev_loader)
print("Finally test acc: %.5f" % results['acc'])
```

    Eval begin...
    step  10/122 - loss: 0.4908 - acc: 0.7703 - 24ms/step
    step  20/122 - loss: 0.4314 - acc: 0.7695 - 18ms/step
    step  30/122 - loss: 0.4691 - acc: 0.7760 - 16ms/step
    step  40/122 - loss: 0.5401 - acc: 0.7867 - 15ms/step
    step  50/122 - loss: 0.5670 - acc: 0.7866 - 15ms/step
    step  60/122 - loss: 0.4395 - acc: 0.7878 - 14ms/step
    step  70/122 - loss: 0.5325 - acc: 0.7895 - 14ms/step
    step  80/122 - loss: 0.5728 - acc: 0.7879 - 14ms/step
    step  90/122 - loss: 0.4095 - acc: 0.7905 - 13ms/step
    step 100/122 - loss: 0.4865 - acc: 0.7930 - 13ms/step
    step 110/122 - loss: 0.4447 - acc: 0.7923 - 13ms/step
    step 120/122 - loss: 0.5081 - acc: 0.7926 - 13ms/step
    step 122/122 - loss: 0.5163 - acc: 0.7918 - 13ms/step
    Eval samples: 7802
    Finally test acc: 0.79185


## 2.6 模型预测


```python
def predict(model, data, label_map, batch_size=1, pad_token_id=0):
    """
    预测数据的标签
    """
    # 把数据分成若干个batch
    batches = [
        data[idx:idx + batch_size] for idx in range(0, len(data), batch_size)
    ]

    batchify_fn = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=pad_token_id),  # query_ids
        Pad(axis=0, pad_val=pad_token_id),  # title_ids
        Stack(dtype="int64"),  # query_seq_lens
        Stack(dtype="int64"),  # title_seq_lens
    ): [data for data in fn(samples)]

    results = []
    model.eval()
    for batch in batches:
        query_ids, title_ids, query_seq_lens, title_seq_lens = batchify_fn(
            batch)
        query_ids = paddle.to_tensor(query_ids)
        title_ids = paddle.to_tensor(title_ids)
        query_seq_lens = paddle.to_tensor(query_seq_lens)
        title_seq_lens = paddle.to_tensor(title_seq_lens)
        logits = model(query_ids, title_ids, query_seq_lens, title_seq_lens)
        probs = F.softmax(logits, axis=1)
        idx = paddle.argmax(probs, axis=1).numpy()
        idx = idx.tolist()
        labels = [label_map[i] for i in idx]
        results.extend(labels)
    return results
```

+ 建立id到类别的映射，本实验有两个类别，编码为0，1。0类为dissimilar，1类为similar


```python
label_map = {0: '0', 1: '1'}
```

+ 加载训练的模型的参数


```python
# 实例化SimNet模型
model = SimNet(network=network, vocab_size=len(vocab), num_classes=len(label_map))
params_path='./checkpoints/final.pdparams'
#  加载模型参数
state_dict = paddle.load(params_path)
model.set_dict(state_dict)
print("Loaded parameters from %s" % params_path)
```

    Loaded parameters from ./checkpoints/final.pdparams



```python
def preprocess_prediction_data(data, tokenizer):
    examples = []
    for query, title in data:
        query_ids = tokenizer.encode(query)
        title_ids = tokenizer.encode(title)
        examples.append([query_ids, title_ids, len(query_ids), len(title_ids)])
    return examples
```


```python
# Firstly pre-processing prediction data  and then do predict.
# 将data替换为我们给出的测试集test_forstu.tsv，输出预测结果result.tsv并提交。
import pandas as pd
data = [
        ['世界上什么东西最小', '世界上什么东西最小？'],
        ['光眼睛大就好看吗', '眼睛好看吗？'],
        ['小蝌蚪找妈妈怎么样', '小蝌蚪找妈妈是谁画的'],
    ]
data = pd.read_csv('test_forstu.tsv', sep='\t')
data = data.values.tolist()
examples = preprocess_prediction_data(data, tokenizer)

results = predict(
        model,
        examples,
        label_map=label_map,
        batch_size=batch_size,
        pad_token_id=vocab.token_to_idx.get('[PAD]', 0))

list1 = []
list2 = []
list3 = []

for idx, text in enumerate(data):
    list1.append(text[0])
    list2.append(text[1])
    list3.append(results[idx])
dict= {'text_a': list1, 'text_b': list2, 'label': list3}
datas = pd.DataFrame(dict)
datas.to_csv('result.csv',encoding='UTF-8',sep="\t")
    #print('Data: {} \t{}\t Label: {}'.format(text[0],text[1], results[idx]))
```


```python

```
