# 2021å¹´7æ—¥æ‰“å¡è¥å¤§ä½œä¸š

å¤§å®¶å¥½ï¼Œè¿™é‡Œæ˜¯2021å¹´7æ—¥æ‰“å¡è¥å¤§ä½œä¸šï¼Œæœ¬æ¬¡ä½œä¸šå†…å®¹ä¸º**å®ç°æ–‡æœ¬ç›¸ä¼¼åº¦ä»»åŠ¡**ï¼Œé€šè¿‡è¯¾ä¸Šæ‰€å­¦çŸ¥è¯†ï¼Œå®ç°æ–‡æœ¬ç›¸ä¼¼åº¦ä»»åŠ¡çš„ä»£ç ã€‚**ç›®å‰å·²ç»ç»™å‡ºäº†åŸºäºSimilarityNetçš„ç›¸ä¼¼åº¦ä»»åŠ¡çš„å®ç°ä»£ç ï¼ŒåŒå­¦ä»¬å¯ä»¥åŸºäºæœ¬ä»£ç è¿›è¡Œä¿®æ”¹æˆ–è€…é‡å†™ï¼Œå®ç°æ›´é«˜çš„å‡†ç¡®ç‡**ï¼Œå…¶ä¸­æ•°æ®é›†å·²ç»ä¸Šä¼ ï¼Œå¦‚æœæœ‰è¯¯æ“ï¼Œå¯ä»¥è”ç³»qqç¾¤åŠ©æ•™ï¼Œè·å¾—æ•°æ®é›†ã€‚

## ã€ä½œä¸šå†…å®¹ã€‘

æŒ‰ç…§æœ¬é¡¹ç›®ç»™å‡ºçš„æ¨¡å—å®Œæˆä»£ç å¹¶è·‘é€šã€‚

## ã€ä½œä¸šæäº¤ã€‘
æˆ‘ä»¬ä¼šç»™å‡ºä¸€ä¸ªæ•°æ®é›†å‹ç¼©åŒ…ï¼ŒåŒ…å«`baidu_train.tsv`,`baidu_dev.tsv`,`vocab.txt`å’Œ`test_forstu.tsv`ã€‚å…¶ä¸­`baidu_train.tsv`,`baidu_dev.tsv`,`vocab.txt`ç”¨æ¥è®­ç»ƒæ¨¡å‹ã€‚æœ€åå¤§å®¶æ ¹æ®`test_forstu.tsv`æµ‹è¯•æ•°æ®è¾“å‡ºé¢„æµ‹ç»“æœã€‚å‘½åä¸º`result.tsv`çš„æ–‡ä»¶å¹¶æäº¤ã€‚
>note:é‡åˆ°é—®é¢˜è¯·åŠæ—¶å‘qqç¾¤çš„åŠ©æ•™åé¦ˆã€‚

## ã€è¯„åˆ†æ ‡å‡†ã€‘

**ä¼˜ç§€å­¦å‘˜ï¼š**

æ¡ä»¶ï¼š
1. å®Œæˆæ¯å¤©çš„è¯¾åä½œä¸šï¼›
2. å¤§ä½œä¸šä»£ç è¿è¡ŒæˆåŠŸä¸”æœ‰ç»“æœï¼›
3. å‡†ç¡®ç‡æ’åå‰26åã€‚

å¥–å“è®¾ç½®ï¼š
|ç­‰çº§|å¥–å“|åé¢|
|---|---|---|
|ä¸€ç­‰å¥–|å°åº¦åœ¨å®¶|1|
|äºŒç­‰å¥–|è€³æœº |5|
|ä¸‰ç­‰å¥–|ä¹¦ | 10|
|å››ç­‰å¥–|å¥èº«åŒ…|10|
|é¼“åŠ±å¥–|æ‹›è´¢ç†Š|170|

# 1. ä»»åŠ¡ä»‹ç»
## 1.1 ä»»åŠ¡å†…å®¹

æ–‡æœ¬è¯­ä¹‰åŒ¹é…æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¸­ä¸€ä¸ªé‡è¦çš„åŸºç¡€é—®é¢˜ï¼ŒNLPé¢†åŸŸçš„å¾ˆå¤šä»»åŠ¡éƒ½å¯ä»¥æŠ½è±¡ä¸ºæ–‡æœ¬åŒ¹é…ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œä¿¡æ¯æ£€ç´¢å¯ä»¥å½’ç»“ä¸ºæŸ¥è¯¢é¡¹å’Œæ–‡æ¡£çš„åŒ¹é…ï¼Œé—®ç­”ç³»ç»Ÿå¯ä»¥å½’ç»“ä¸ºé—®é¢˜å’Œå€™é€‰ç­”æ¡ˆçš„åŒ¹é…ï¼Œå¯¹è¯ç³»ç»Ÿå¯ä»¥å½’ç»“ä¸ºå¯¹è¯å’Œå›å¤çš„åŒ¹é…ã€‚è¯­ä¹‰åŒ¹é…åœ¨æœç´¢ä¼˜åŒ–ã€æ¨èç³»ç»Ÿã€å¿«é€Ÿæ£€ç´¢æ’åºã€æ™ºèƒ½å®¢æœä¸Šéƒ½æœ‰å¹¿æ³›çš„åº”ç”¨ã€‚å¦‚ä½•æå‡æ–‡æœ¬åŒ¹é…çš„å‡†ç¡®åº¦ï¼Œæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„ä¸€ä¸ªé‡è¦æŒ‘æˆ˜ã€‚

+ ä¿¡æ¯æ£€ç´¢ï¼šåœ¨ä¿¡æ¯æ£€ç´¢é¢†åŸŸçš„å¾ˆå¤šåº”ç”¨ä¸­ï¼Œéƒ½éœ€è¦æ ¹æ®åŸæ–‡æœ¬æ¥æ£€ç´¢ä¸å…¶ç›¸ä¼¼çš„å…¶ä»–æ–‡æœ¬ï¼Œä½¿ç”¨åœºæ™¯éå¸¸æ™®éã€‚
+ æ–°é—»æ¨èï¼šé€šè¿‡ç”¨æˆ·åˆšåˆšæµè§ˆè¿‡çš„æ–°é—»æ ‡é¢˜ï¼Œè‡ªåŠ¨æ£€ç´¢å‡ºå…¶ä»–çš„ç›¸ä¼¼æ–°é—»ï¼Œä¸ªæ€§åŒ–åœ°ä¸ºç”¨æˆ·åšæ¨èï¼Œä»è€Œå¢å¼ºç”¨æˆ·ç²˜æ€§ï¼Œæå‡äº§å“ä½“éªŒã€‚
+ æ™ºèƒ½å®¢æœï¼šç”¨æˆ·è¾“å…¥ä¸€ä¸ªé—®é¢˜åï¼Œè‡ªåŠ¨ä¸ºç”¨æˆ·æ£€ç´¢å‡ºç›¸ä¼¼çš„é—®é¢˜å’Œç­”æ¡ˆï¼ŒèŠ‚çº¦äººå·¥å®¢æœçš„æˆæœ¬ï¼Œæé«˜æ•ˆç‡ã€‚

## 1.2 ä»€ä¹ˆæ˜¯æ–‡æœ¬åŒ¹é…ï¼Ÿ

è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªç®€å•çš„ä¾‹å­ï¼Œæ¯”è¾ƒå„å€™é€‰å¥å­å“ªå¥å’ŒåŸå¥è¯­ä¹‰æ›´ç›¸è¿‘

åŸå¥ï¼šâ€œè½¦å¤´å¦‚ä½•æ”¾ç½®è½¦ç‰Œâ€

+ æ¯”è¾ƒå¥1ï¼šâ€œå‰ç‰Œç…§æ€ä¹ˆè£…â€
+ æ¯”è¾ƒå¥2ï¼šâ€œå¦‚ä½•åŠç†åŒ—äº¬è½¦ç‰Œâ€
+ æ¯”è¾ƒå¥3ï¼šâ€œåç‰Œç…§æ€ä¹ˆè£…â€

ï¼ˆ1ï¼‰æ¯”è¾ƒå¥1ä¸åŸå¥ï¼Œè™½ç„¶å¥å¼å’Œè¯­åºç­‰å­˜åœ¨è¾ƒå¤§å·®å¼‚ï¼Œä½†æ˜¯æ‰€è¡¨è¿°çš„å«ä¹‰å‡ ä¹ç›¸åŒ

ï¼ˆ2ï¼‰æ¯”è¾ƒå¥2ä¸åŸå¥ï¼Œè™½ç„¶å­˜åœ¨â€œå¦‚ä½•â€ ã€â€œè½¦ç‰Œâ€ç­‰å…±ç°è¯ï¼Œä½†æ˜¯æ‰€è¡¨è¿°çš„å«ä¹‰å®Œå…¨ä¸åŒ

ï¼ˆ3ï¼‰æ¯”è¾ƒå¥3ä¸åŸå¥ï¼ŒäºŒè€…è®¨è®ºçš„éƒ½æ˜¯å¦‚ä½•æ”¾ç½®è½¦ç‰Œçš„é—®é¢˜ï¼Œåªä¸è¿‡ä¸€ä¸ªæ˜¯å‰ç‰Œç…§ï¼Œå¦ä¸€ä¸ªæ˜¯åç‰Œç…§ã€‚äºŒè€…é—´å­˜åœ¨ä¸€å®šçš„è¯­ä¹‰ç›¸å…³æ€§ã€‚

æ‰€ä»¥è¯­ä¹‰ç›¸å…³æ€§ï¼Œå¥1å¤§äºå¥3ï¼Œå¥3å¤§äºå¥2.è¿™å°±æ˜¯è¯­ä¹‰åŒ¹é…ã€‚


## 1.3 çŸ­æ–‡æœ¬è¯­ä¹‰åŒ¹é…ç½‘ç»œ

çŸ­æ–‡æœ¬è¯­ä¹‰åŒ¹é…(SimilarityNet, SimNet)æ˜¯ä¸€ä¸ªè®¡ç®—çŸ­æ–‡æœ¬ç›¸ä¼¼åº¦çš„æ¡†æ¶ï¼Œå¯ä»¥æ ¹æ®ç”¨æˆ·è¾“å…¥çš„ä¸¤ä¸ªæ–‡æœ¬ï¼Œè®¡ç®—å‡ºç›¸ä¼¼åº¦å¾—åˆ†ã€‚ä¸»è¦åŒ…æ‹¬BOWã€CNNã€RNNã€MMDNNç­‰æ ¸å¿ƒç½‘ç»œç»“æ„å½¢å¼ï¼Œæä¾›è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—è®­ç»ƒå’Œé¢„æµ‹æ¡†æ¶ï¼Œé€‚ç”¨äºä¿¡æ¯æ£€ç´¢ã€æ–°é—»æ¨èã€æ™ºèƒ½å®¢æœç­‰å¤šä¸ªåº”ç”¨åœºæ™¯ï¼Œå¸®åŠ©ä¼ä¸šè§£å†³è¯­ä¹‰åŒ¹é…é—®é¢˜ã€‚

SimNetæ¨¡å‹ç»“æ„å¦‚å›¾æ‰€ç¤ºï¼ŒåŒ…æ‹¬è¾“å…¥å±‚ã€è¡¨ç¤ºå±‚ä»¥åŠåŒ¹é…å±‚ã€‚

<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/953b573d100b46fe84f026c99a3a2664fbecbe680fa748eda6337548a2f4b4af" width=50%></center>
<center><br>å›¾ SimilarityNet æ¡†æ¶ </br></center>
<br></br>


## 1.4 å®éªŒè®¾è®¡

æœ¬ä»»åŠ¡ç»™å‡ºä¸€ä¸ªæ–‡æœ¬ç›¸ä¼¼åº¦ä»»åŠ¡å®ç°æ–¹æ³•ï¼Œæ¨¡å‹æ¡†æ¶ç»“æ„å›¾å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå…¶ä¸­queryå’Œtitleæ˜¯æ•°æ®é›†ç»è¿‡å¤„ç†åçš„å¾…åŒ¹é…çš„æ–‡æœ¬ï¼Œç„¶åç»è¿‡åˆ†è¯å¤„ç†ï¼Œç¼–ç æˆidï¼Œç»è¿‡SimilarityNetå¤„ç†ï¼Œå¾—åˆ°è¾“å‡ºï¼Œè®­ç»ƒçš„æŸå¤±å‡½æ•°ä½¿ç”¨çš„æ˜¯äº¤å‰ç†µæŸå¤±ã€‚

<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/df8c743f9b3742e4a99ae6b066c133c9b119f7499e3b4addb5ce755ec3faae7e" width='600px'></center>
<center><br>å›¾ SimilarityNetå®éªŒæµç¨‹ </br></center>
<br></br>


# 2. å®éªŒè¯¦ç»†å®ç°
æ–‡æœ¬ç›¸ä¼¼åº¦å®éªŒæµç¨‹åŒ…å«å¦‚ä¸‹6ä¸ªæ­¥éª¤ï¼š
1. **æ•°æ®è¯»å–**ï¼šæ ¹æ®ç½‘ç»œæ¥æ”¶çš„æ•°æ®æ ¼å¼ï¼Œå®Œæˆç›¸åº”çš„é¢„å¤„ç†æ“ä½œï¼Œä¿è¯æ¨¡å‹æ­£å¸¸è¯»å–ï¼›
2. **æ¨¡å‹æ„å»º**ï¼šè®¾è®¡åŸºäºGRUçš„æ–‡æœ¬ç›¸ä¼¼åº¦æ¨¡å‹ï¼Œåˆ¤æ–­ä¸¤å¥è¯æ˜¯å¦æ˜¯ç›¸ä¼¼ï¼›
3. **è®­ç»ƒé…ç½®**ï¼šå®ä¾‹åŒ–æ¨¡å‹ï¼Œé€‰æ‹©æ¨¡å‹è®¡ç®—èµ„æºï¼ˆCPUæˆ–è€…GPUï¼‰ï¼ŒæŒ‡å®šæ¨¡å‹è¿­ä»£çš„ä¼˜åŒ–ç®—æ³•ï¼›
4. **æ¨¡å‹è®­ç»ƒä¸ä¿å­˜**ï¼šæ‰§è¡Œå¤šè½®è®­ç»ƒä¸æ–­è°ƒæ•´å‚æ•°ï¼Œä»¥è¾¾åˆ°è¾ƒå¥½çš„æ•ˆæœï¼Œä¿å­˜æ¨¡å‹ï¼›
5. **æ¨¡å‹è¯„ä¼°**ï¼šè®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†åˆä¸Šæµ‹è¯•ï¼›
6. **æ¨¡å‹é¢„æµ‹**ï¼šåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå¹¶è¿›è¡Œé¢„æµ‹ï¼›

## 2.1æ•°æ®è¯»å–

æœ¬å®éªŒå…±è®¡éœ€è¦è¯»å–å››ä»½æ•°æ®: è®­ç»ƒé›† **train.tsv**ã€éªŒè¯é›† **dev.tsv**ã€æµ‹è¯•é›† **test.tsv** å’Œ è¯æ±‡è¡¨ **vocab.txt**ã€‚åŠ è½½æ•°æ®çš„ä»£ç å¦‚ä¸‹ï¼š

+ åŠ è½½ç¬¬ä¸‰æ–¹åº“ï¼Œpaddleå’Œpaddlenlpç›¸å…³çš„åº“


```python
import math
import numpy as np
import os
import collections
from functools import partial
import random
import time
import inspect
import importlib
from tqdm import tqdm

import paddle
import paddle.nn as nn
import paddle.nn.functional as F
from paddle.io import IterableDataset
from paddle.utils.download import get_path_from_url
```

    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
    Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
      def convert_to_list(value, n, name, dtype=np.int):


+ æœ¬å®éªŒéœ€è¦ä¾èµ–ä¸paddlenlpï¼Œaistudioä¸Šçš„paddlenlpç‰ˆæœ¬è¿‡ä½ï¼Œæ‰€ä»¥éœ€è¦é¦–å…ˆå‡çº§paddlenlp


```python
!pip install paddlenlp --upgrade
```

    Looking in indexes: https://mirror.baidu.com/pypi/simple/
    Collecting paddlenlp
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/b1/e9/128dfc1371db3fc2fa883d8ef27ab6b21e3876e76750a43f58cf3c24e707/paddlenlp-2.0.2-py3-none-any.whl (426kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 430kB 12.6MB/s eta 0:00:01
    [?25hCollecting multiprocess (from paddlenlp)
    [?25l  Downloading https://mirror.baidu.com/pypi/packages/db/20/458ac043a57322365ac2ed86a911bf7598fc2e49bccb3f94ea810fbb6b9b/multiprocess-0.70.11.1-py37-none-any.whl (108kB)
    [K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112kB 52.1MB/s eta 0:00:01
    [?25hRequirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)
    Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)
    Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)
    Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)
    Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)
    Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)
    Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)
    Requirement already satisfied, skipping upgrade: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.20.3)
    Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)
    Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)
    Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)
    Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)
    Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)
    Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)
    Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)
    Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)
    Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)
    Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)
    Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)
    Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)
    Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)
    Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)
    Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)
    Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)
    Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)
    Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)
    Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)
    Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)
    Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)
    Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < "3.8" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.23)
    Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)
    Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)
    Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)
    Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)
    Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)
    Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)
    Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)
    Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)
    Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)
    Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)
    Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)
    Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)
    Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)
    Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)
    Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)
    Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < "3.8"->pre-commit->visualdl->paddlenlp) (0.6.0)
    Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < "3.8"->pre-commit->visualdl->paddlenlp) (7.2.0)
    Installing collected packages: multiprocess, paddlenlp
      Found existing installation: paddlenlp 2.0.0rc7
        Uninstalling paddlenlp-2.0.0rc7:
          Successfully uninstalled paddlenlp-2.0.0rc7
    Successfully installed multiprocess-0.70.11.1 paddlenlp-2.0.2


+ å¯¼å…¥paddlenlpç›¸å…³çš„åŒ…


```python
import paddlenlp as ppnlp
from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab
# from utils import convert_example
from paddlenlp.datasets import MapDataset
from paddle.dataset.common import md5file
from paddlenlp.datasets import DatasetBuilder
```


```python
def convert_example(example, tokenizer, is_test=False):
    """
    ä¸ºåºåˆ—åˆ†ç±»ä»»åŠ¡ä»åºåˆ—ç”Ÿæˆæ¨¡å‹è¾“å…¥
    """
    
    query, title = example["query"], example["title"]
    # query idç”Ÿæˆ
    query_ids = np.array(tokenizer.encode(query), dtype="int64")
    query_seq_len = np.array(len(query_ids), dtype="int64")

    # title id ç”Ÿæˆ
    title_ids = np.array(tokenizer.encode(title), dtype="int64")
    title_seq_len = np.array(len(title_ids), dtype="int64")
    
    # è®­ç»ƒæ¨¡å¼
    if not is_test:
        label = np.array(example["label"], dtype="int64")
        return query_ids, title_ids, query_seq_len, title_seq_len, label
    else:
        # æµ‹è¯•æ¨¡å¼
        return query_ids, title_ids, query_seq_len, title_seq_len
```


```python
class BAIDUData(DatasetBuilder):

    SPLITS = {
        'train':os.path.join( 'baidu_train.tsv'),
        'dev': os.path.join('baidu_dev.tsv'),
    }

    def _get_data(self, mode, **kwargs):
        filename = self.SPLITS[mode]
        return filename

    def _read(self, filename):
        """è¯»å–æ•°æ®"""
        with open(filename, 'r', encoding='utf-8') as f:
            head = None
            for line in f:
                data = line.strip().split("\t")
                if not head:
                    head = data
                else:
                    query, title, label = data
                    yield {"query": query, "title": title, "label": label}

    def get_labels(self):
        return ["0", "1"]
```


```python
def load_dataset(name=None,
                 data_files=None,
                 splits=None,
                 lazy=None,
                 **kwargs):
   
    reader_cls = BAIDUData
    print(reader_cls)
    if not name:
        reader_instance = reader_cls(lazy=lazy, **kwargs)
    else:
        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)

    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)
    return datasets
```

+ åŠ è½½è¯æ±‡è¡¨


```python

vocab_path="vocab.txt"

vocab = Vocab.load_vocabulary(vocab_path, unk_token='[UNK]', pad_token='[PAD]')
```

+ åŠ è½½è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†


```python
# Loads dataset.
train_ds, dev_ds = load_dataset(splits=["train", "dev"])
```

    <class '__main__.BAIDUData'>


    2021-06-08 19:08:47,165 - INFO - unique_endpoints {''}


+ åˆ›å»ºdataloader


```python
def create_dataloader(dataset,
                      trans_fn=None,
                      mode='train',
                      batch_size=1, #1
                      batchify_fn=None):
    if trans_fn:
        dataset = dataset.map(trans_fn)

    shuffle = True if mode == 'train' else False
    if mode == "train":
        sampler = paddle.io.DistributedBatchSampler(
            dataset=dataset, batch_size=batch_size, shuffle=True)
    else:
        sampler = paddle.io.BatchSampler(
            dataset=dataset, batch_size=batch_size, shuffle=shuffle)
    dataloader = paddle.io.DataLoader(
        dataset,
        batch_sampler=sampler,
        return_list=True,
        collate_fn=batchify_fn)
    return dataloader
```


```python
# Reads data and generates mini-batches.
batchify_fn = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # query_ids
        Pad(axis=0, pad_val=vocab.token_to_idx.get('[PAD]', 0)),  # title_ids
        Stack(dtype="int64"),  # query_seq_lens
        Stack(dtype="int64"),  # title_seq_lens
        Stack(dtype="int64")  # label
    ): [data for data in fn(samples)]
tokenizer = ppnlp.data.JiebaTokenizer(vocab)

trans_fn = partial(convert_example, tokenizer=tokenizer, is_test=False)
```

## 2.2 æ¨¡å‹æ„å»º

æœ¬èŠ‚å®éªŒæ„å»ºäº†åŸºäºGRUçš„SimilarityNetç½‘ç»œç»“æ„ï¼Œå¦‚å›¾æ‰€ç¤ºã€‚æ¨¡å‹å¾—è¾“å…¥æ˜¯queryå’Œtitleï¼Œç„¶ååˆ†åˆ«ç»è¿‡embeddingå±‚ï¼ŒGRUå±‚ï¼ŒæŠŠGRUè¾“å‡ºå¾—å‘é‡æ‹¼æ¥ï¼ˆconcatæ“ä½œï¼‰ï¼Œæœ€åç»è¿‡FC(å…¨è¿æ¥)å±‚ã€‚
<br></br>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/645678222ebb4c87bcf8b53e4e4ecbdc64340506839e498c8a3ed12fe57c0686" width='500px'></center>
<center><br>å›¾ åŸºäºGRUçš„SimNetç½‘ç»œç»“æ„ </br></center>
<br></br>


```python
class GRUEncoder(nn.Layer):

    def __init__(self,
                 input_size,
                 hidden_size,
                 num_layers=1,
                 direction="forward",
                 dropout=0.0,
                 pooling_type=None,
                 **kwargs):
        super().__init__()
        # è¾“å…¥çš„å¤§å°
        self._input_size = input_size
        # éšè—å±‚çš„å¤§å°
        self._hidden_size = hidden_size
        self._direction = direction
        self._pooling_type = pooling_type
        # å®ä¾‹åŒ–GRUå±‚
        self.gru_layer = nn.GRU(input_size=input_size,
                                hidden_size=hidden_size,
                                num_layers=num_layers,
                                direction=direction,
                                dropout=dropout,
                                **kwargs)

    def get_input_dim(self):
        """
        Returns the dimension of the vector input for each element in the sequence input
        to a `GRUEncoder`. This is not the shape of the input tensor, but the
        last element of that shape.
        """
        return self._input_size

    def get_output_dim(self):
        """
        Returns the dimension of the final vector output by this `GRUEncoder`.  This is not
        the shape of the returned tensor, but the last element of that shape.
        """
        if self._direction == "bidirect":
            return self._hidden_size * 2
        else:
            return self._hidden_size

    def forward(self, inputs, sequence_length):
        
        encoded_text, last_hidden = self.gru_layer(
            inputs, sequence_length=sequence_length)
        if not self._pooling_type:
            # We exploit the `last_hidden` (the hidden state at the last time step for every layer)
            # to create a single vector.
            # If gru is not bidirection, then output is the hidden state of the last time step 
            # at last layer. Output is shape of `(batch_size, hidden_size)`.
            # If gru is bidirection, then output is concatenation of the forward and backward hidden state 
            # of the last time step at last layer. Output is shape of `(batch_size, hidden_size*2)`.
            if self._direction != 'bidirect':
                output = last_hidden[-1, :, :]
            else:
                output = paddle.concat(
                    (last_hidden[-2, :, :], last_hidden[-1, :, :]), axis=1)
        else:
            # We exploit the `encoded_text` (the hidden state at the every time step for last layer)
            # to create a single vector. We perform pooling on the encoded text.
            # The output shape is `(batch_size, hidden_size*2)` if use bidirectional GRU, 
            # otherwise the output shape is `(batch_size, hidden_size*2)`.
            if self._pooling_type == 'sum':
                output = paddle.sum(encoded_text, axis=1)
            elif self._pooling_type == 'max':
                output = paddle.max(encoded_text, axis=1)
            elif self._pooling_type == 'mean':
                output = paddle.mean(encoded_text, axis=1)
            else:
                raise RuntimeError(
                    "Unexpected pooling type %s ."
                    "Pooling type must be one of sum, max and mean." %
                    self._pooling_type)
        return output
```


```python
class GRUModel(nn.Layer):
    def __init__(self,
                 vocab_size,
                 num_classes,
                 emb_dim=128,
                 padding_idx=0,
                 gru_hidden_size=128,
                 direction='forward',
                 gru_layers=1,
                 dropout_rate=0.0,
                 pooling_type=None,
                 fc_hidden_size=96):
        super().__init__()
        # è¯åµŒå…¥å±‚
        self.embedder = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=emb_dim,
            padding_idx=padding_idx)
        # GRUbç¼–ç å±‚
        self.gru_encoder = GRUEncoder(
            emb_dim,
            gru_hidden_size,
            num_layers=gru_layers,
            direction=direction,
            dropout=dropout_rate)
        # çº¿æ€§å±‚
        self.fc = nn.Linear(self.gru_encoder.get_output_dim() * 2,fc_hidden_size)
        # è¾“å‡ºå±‚
        self.output_layer = nn.Linear(fc_hidden_size, num_classes)

    def forward(self, query, title, query_seq_len, title_seq_len):
        # Shape: (batch_size, num_tokens, embedding_dim)
        embedded_query = self.embedder(query)
        embedded_title = self.embedder(title)
        # Shape: (batch_size, gru_hidden_size)
        query_repr = self.gru_encoder(
            embedded_query, sequence_length=query_seq_len)
        title_repr = self.gru_encoder(
            embedded_title, sequence_length=title_seq_len)
        # Shape: (batch_size, 2*gru_hidden_size)
        contacted = paddle.concat([query_repr, title_repr], axis=-1)
        # Shape: (batch_size, fc_hidden_size)
        fc_out = paddle.tanh(self.fc(contacted))
        # Shape: (batch_size, num_classes)
        logits = self.output_layer(fc_out)
        # probs = F.softmax(logits, axis=-1)

        return logits
```


```python
class SimNet(nn.Layer):
    def __init__(self,
                 network,
                 vocab_size,
                 num_classes,
                 emb_dim=128,
                 pad_token_id=0):
        super().__init__()
        # è½¬æ¢æˆå°å†™
        network = network.lower()
        # gruç½‘ç»œ
        if network == 'gru':
            self.model = GRUModel(
                vocab_size,
                num_classes,
                emb_dim,
                direction='forward',
                padding_idx=pad_token_id)

    def forward(self, query, title, query_seq_len=None, title_seq_len=None):
        logits = self.model(query, title, query_seq_len, title_seq_len)
        return logits
```

## 2.3 è®­ç»ƒé…ç½®


```python
# æ£€æµ‹æ˜¯å¦å¯ä»¥ä½¿ç”¨GPUï¼Œå¦‚æœå¯ä»¥ä¼˜å…ˆä½¿ç”¨GPU
use_gpu = True if paddle.get_device().startswith("gpu") else False
if use_gpu:
    paddle.set_device('gpu')
    #print('gpu')
batch_size=200   # batch size çš„å¤§å°
```

+ å®ä¾‹åŒ–SimilarityNetç½‘ç»œ


```python
# æ„å»ºSimNetç½‘ç»œï¼Œç½‘ç»œçš„ä¸»ä½“é‡‡ç”¨GRUbç½‘ç»œ
network='gru'
model = SimNet(
        network=network,
        vocab_size=len(vocab),
        num_classes=len(train_ds.label_list))
model = paddle.Model(model)
```

+ æ„å»ºè®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„dataloader


```python
# æ„å»ºè®­ç»ƒé›†çš„dataloader
train_loader = create_dataloader(
        train_ds,
        trans_fn=trans_fn,
        batch_size=batch_size,
        mode='train',
        batchify_fn=batchify_fn)
# æ„å»ºéªŒè¯é›†çš„dataloader
dev_loader = create_dataloader(
        dev_ds,
        trans_fn=trans_fn,
        batch_size=batch_size,
        mode='validation',
        batchify_fn=batchify_fn)
# # æ„å»ºæµ‹è¯•é›†çš„dataloader
# test_loader = create_dataloader(
#         test_ds,
#         trans_fn=trans_fn,
#         batch_size=batch_size,
#         mode='test',
#         batchify_fn=batchify_fn)
```

+ å®šä¹‰ä¼˜åŒ–å™¨


```python
lr=5e-4
optimizer = paddle.optimizer.Adam(
        parameters=model.parameters(), learning_rate=lr)
```

+ å®ä¾‹åŒ–æŸå¤±å‡½æ•°


```python
# Defines loss and metric.
criterion = paddle.nn.CrossEntropyLoss()  # äº¤å‰ç†µæŸå¤±å‡½æ•°
```

+ å®ä¾‹åŒ–è¯„ä¼°æ–¹å¼


```python
metric = paddle.metric.Accuracy()  # accuracy è¯„ä¼°æ–¹å¼
```

+ ç¼–è¯‘æ¨¡å‹


```python
model.prepare(optimizer, criterion, metric)
```

## 2.4 æ¨¡å‹è®­ç»ƒ

+ æ¨¡å‹è®­ç»ƒä½¿ç”¨paddleçš„é«˜çº§APIï¼Œä½¿ç”¨fitå‡½æ•°å°±å¯ä»¥å®ç°è®­ç»ƒå’Œæ¨¡å‹ä¿å­˜ï¼Œæ¨¡å‹ä¿å­˜çš„ä½ç½®ä¸ºsave_dir


```python
epochs=10
save_dir='./checkpoints'
# Starts training and evaluating.
model.fit(
        train_loader,
        dev_loader,
        epochs=epochs,
        save_dir=save_dir
        )
```

    The loss value printed in the log is the current step, and the metric is the average value of previous step.
    Epoch 1/10


    /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/layers/utils.py:77: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
      return (isinstance(seq, collections.Sequence) and


    step  10/313 - loss: 0.6779 - acc: 0.5560 - 71ms/step
    step  20/313 - loss: 0.6775 - acc: 0.5620 - 54ms/step
    step  30/313 - loss: 0.6709 - acc: 0.5600 - 49ms/step
    step  40/313 - loss: 0.6698 - acc: 0.5779 - 46ms/step
    step  50/313 - loss: 0.6427 - acc: 0.5852 - 44ms/step
    step  60/313 - loss: 0.6441 - acc: 0.5972 - 43ms/step
    step  70/313 - loss: 0.6351 - acc: 0.6081 - 42ms/step
    step  80/313 - loss: 0.5961 - acc: 0.6187 - 42ms/step
    step  90/313 - loss: 0.5682 - acc: 0.6259 - 42ms/step
    step 100/313 - loss: 0.6386 - acc: 0.6305 - 41ms/step
    step 110/313 - loss: 0.5877 - acc: 0.6357 - 41ms/step
    step 120/313 - loss: 0.5476 - acc: 0.6404 - 41ms/step
    step 130/313 - loss: 0.5689 - acc: 0.6439 - 40ms/step
    step 140/313 - loss: 0.5621 - acc: 0.6487 - 40ms/step
    step 150/313 - loss: 0.5484 - acc: 0.6526 - 40ms/step
    step 160/313 - loss: 0.5292 - acc: 0.6560 - 40ms/step
    step 170/313 - loss: 0.5861 - acc: 0.6589 - 39ms/step
    step 180/313 - loss: 0.5605 - acc: 0.6619 - 39ms/step
    step 190/313 - loss: 0.5725 - acc: 0.6634 - 39ms/step
    step 200/313 - loss: 0.5240 - acc: 0.6660 - 39ms/step
    step 210/313 - loss: 0.5724 - acc: 0.6684 - 39ms/step
    step 220/313 - loss: 0.5520 - acc: 0.6701 - 39ms/step
    step 230/313 - loss: 0.5021 - acc: 0.6723 - 39ms/step
    step 240/313 - loss: 0.4600 - acc: 0.6744 - 39ms/step
    step 250/313 - loss: 0.5444 - acc: 0.6758 - 39ms/step
    step 260/313 - loss: 0.5799 - acc: 0.6770 - 39ms/step
    step 270/313 - loss: 0.5113 - acc: 0.6786 - 39ms/step
    step 280/313 - loss: 0.4968 - acc: 0.6802 - 39ms/step
    step 290/313 - loss: 0.5544 - acc: 0.6815 - 39ms/step
    step 300/313 - loss: 0.5375 - acc: 0.6823 - 39ms/step
    step 310/313 - loss: 0.5395 - acc: 0.6842 - 38ms/step
    step 313/313 - loss: 0.5030 - acc: 0.6843 - 38ms/step
    save checkpoint at /home/aistudio/checkpoints/0
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.4852 - acc: 0.7650 - 45ms/step
    step 20/40 - loss: 0.4746 - acc: 0.7752 - 37ms/step
    step 30/40 - loss: 0.4398 - acc: 0.7763 - 34ms/step
    step 40/40 - loss: 0.8953 - acc: 0.7793 - 29ms/step
    Eval samples: 7802
    Epoch 2/10
    step  10/313 - loss: 0.4326 - acc: 0.7815 - 50ms/step
    step  20/313 - loss: 0.4775 - acc: 0.7855 - 43ms/step
    step  30/313 - loss: 0.5019 - acc: 0.7822 - 40ms/step
    step  40/313 - loss: 0.4167 - acc: 0.7849 - 39ms/step
    step  50/313 - loss: 0.5203 - acc: 0.7799 - 39ms/step
    step  60/313 - loss: 0.4360 - acc: 0.7805 - 39ms/step
    step  70/313 - loss: 0.4256 - acc: 0.7804 - 38ms/step
    step  80/313 - loss: 0.5098 - acc: 0.7797 - 38ms/step
    step  90/313 - loss: 0.4505 - acc: 0.7802 - 38ms/step
    step 100/313 - loss: 0.5386 - acc: 0.7798 - 38ms/step
    step 110/313 - loss: 0.3901 - acc: 0.7804 - 37ms/step
    step 120/313 - loss: 0.5151 - acc: 0.7810 - 37ms/step
    step 130/313 - loss: 0.4669 - acc: 0.7808 - 37ms/step
    step 140/313 - loss: 0.5065 - acc: 0.7811 - 37ms/step
    step 150/313 - loss: 0.4478 - acc: 0.7807 - 37ms/step
    step 160/313 - loss: 0.4828 - acc: 0.7816 - 37ms/step
    step 170/313 - loss: 0.4707 - acc: 0.7814 - 37ms/step
    step 180/313 - loss: 0.4508 - acc: 0.7816 - 37ms/step
    step 190/313 - loss: 0.4934 - acc: 0.7808 - 37ms/step
    step 200/313 - loss: 0.4777 - acc: 0.7797 - 37ms/step
    step 210/313 - loss: 0.5268 - acc: 0.7788 - 37ms/step
    step 220/313 - loss: 0.4566 - acc: 0.7787 - 37ms/step
    step 230/313 - loss: 0.4869 - acc: 0.7783 - 37ms/step
    step 240/313 - loss: 0.5303 - acc: 0.7784 - 37ms/step
    step 250/313 - loss: 0.5559 - acc: 0.7783 - 37ms/step
    step 260/313 - loss: 0.4952 - acc: 0.7775 - 37ms/step
    step 270/313 - loss: 0.4588 - acc: 0.7774 - 37ms/step
    step 280/313 - loss: 0.5450 - acc: 0.7772 - 37ms/step
    step 290/313 - loss: 0.4265 - acc: 0.7778 - 37ms/step
    step 300/313 - loss: 0.5427 - acc: 0.7781 - 37ms/step
    step 310/313 - loss: 0.5577 - acc: 0.7780 - 37ms/step
    step 313/313 - loss: 0.4183 - acc: 0.7780 - 36ms/step
    save checkpoint at /home/aistudio/checkpoints/1
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.3883 - acc: 0.8435 - 41ms/step
    step 20/40 - loss: 0.3899 - acc: 0.8440 - 34ms/step
    step 30/40 - loss: 0.3690 - acc: 0.8463 - 32ms/step
    step 40/40 - loss: 0.7862 - acc: 0.8472 - 28ms/step
    Eval samples: 7802
    Epoch 3/10
    step  10/313 - loss: 0.4020 - acc: 0.8515 - 50ms/step
    step  20/313 - loss: 0.2510 - acc: 0.8470 - 43ms/step
    step  30/313 - loss: 0.4031 - acc: 0.8463 - 41ms/step
    step  40/313 - loss: 0.3762 - acc: 0.8464 - 39ms/step
    step  50/313 - loss: 0.3229 - acc: 0.8437 - 39ms/step
    step  60/313 - loss: 0.4479 - acc: 0.8409 - 38ms/step
    step  70/313 - loss: 0.3726 - acc: 0.8391 - 38ms/step
    step  80/313 - loss: 0.4101 - acc: 0.8391 - 38ms/step
    step  90/313 - loss: 0.3670 - acc: 0.8381 - 38ms/step
    step 100/313 - loss: 0.3189 - acc: 0.8377 - 38ms/step
    step 110/313 - loss: 0.3986 - acc: 0.8376 - 38ms/step
    step 120/313 - loss: 0.3410 - acc: 0.8371 - 37ms/step
    step 130/313 - loss: 0.3904 - acc: 0.8371 - 37ms/step
    step 140/313 - loss: 0.3792 - acc: 0.8365 - 37ms/step
    step 150/313 - loss: 0.4145 - acc: 0.8358 - 37ms/step
    step 160/313 - loss: 0.3486 - acc: 0.8357 - 37ms/step
    step 170/313 - loss: 0.3987 - acc: 0.8349 - 37ms/step
    step 180/313 - loss: 0.3979 - acc: 0.8339 - 37ms/step
    step 190/313 - loss: 0.4143 - acc: 0.8342 - 37ms/step
    step 200/313 - loss: 0.3647 - acc: 0.8338 - 37ms/step
    step 210/313 - loss: 0.4422 - acc: 0.8331 - 37ms/step
    step 220/313 - loss: 0.3823 - acc: 0.8324 - 37ms/step
    step 230/313 - loss: 0.4111 - acc: 0.8318 - 37ms/step
    step 240/313 - loss: 0.4629 - acc: 0.8319 - 37ms/step
    step 250/313 - loss: 0.3643 - acc: 0.8312 - 37ms/step
    step 260/313 - loss: 0.4079 - acc: 0.8308 - 37ms/step
    step 270/313 - loss: 0.4235 - acc: 0.8305 - 37ms/step
    step 280/313 - loss: 0.3720 - acc: 0.8304 - 37ms/step
    step 290/313 - loss: 0.4635 - acc: 0.8302 - 37ms/step
    step 300/313 - loss: 0.3411 - acc: 0.8298 - 37ms/step
    step 310/313 - loss: 0.3830 - acc: 0.8289 - 37ms/step
    step 313/313 - loss: 0.7811 - acc: 0.8287 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/2
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.2885 - acc: 0.8880 - 43ms/step
    step 20/40 - loss: 0.3043 - acc: 0.8885 - 36ms/step
    step 30/40 - loss: 0.2885 - acc: 0.8885 - 34ms/step
    step 40/40 - loss: 0.4380 - acc: 0.8894 - 29ms/step
    Eval samples: 7802
    Epoch 4/10
    step  10/313 - loss: 0.2928 - acc: 0.8875 - 49ms/step
    step  20/313 - loss: 0.2682 - acc: 0.8855 - 44ms/step
    step  30/313 - loss: 0.3044 - acc: 0.8835 - 42ms/step
    step  40/313 - loss: 0.3253 - acc: 0.8832 - 40ms/step
    step  50/313 - loss: 0.3368 - acc: 0.8811 - 40ms/step
    step  60/313 - loss: 0.3091 - acc: 0.8802 - 39ms/step
    step  70/313 - loss: 0.2681 - acc: 0.8816 - 39ms/step
    step  80/313 - loss: 0.3478 - acc: 0.8801 - 39ms/step
    step  90/313 - loss: 0.2915 - acc: 0.8805 - 38ms/step
    step 100/313 - loss: 0.3208 - acc: 0.8796 - 38ms/step
    step 110/313 - loss: 0.2509 - acc: 0.8790 - 38ms/step
    step 120/313 - loss: 0.3732 - acc: 0.8788 - 38ms/step
    step 130/313 - loss: 0.2936 - acc: 0.8772 - 38ms/step
    step 140/313 - loss: 0.4708 - acc: 0.8749 - 38ms/step
    step 150/313 - loss: 0.3266 - acc: 0.8744 - 38ms/step
    step 160/313 - loss: 0.2815 - acc: 0.8739 - 38ms/step
    step 170/313 - loss: 0.2958 - acc: 0.8735 - 38ms/step
    step 180/313 - loss: 0.2897 - acc: 0.8727 - 38ms/step
    step 190/313 - loss: 0.2979 - acc: 0.8724 - 38ms/step
    step 200/313 - loss: 0.2620 - acc: 0.8718 - 38ms/step
    step 210/313 - loss: 0.2769 - acc: 0.8707 - 38ms/step
    step 220/313 - loss: 0.3142 - acc: 0.8705 - 38ms/step
    step 230/313 - loss: 0.3019 - acc: 0.8703 - 38ms/step
    step 240/313 - loss: 0.3372 - acc: 0.8700 - 38ms/step
    step 250/313 - loss: 0.3072 - acc: 0.8693 - 38ms/step
    step 260/313 - loss: 0.3214 - acc: 0.8692 - 38ms/step
    step 270/313 - loss: 0.3697 - acc: 0.8680 - 38ms/step
    step 280/313 - loss: 0.4037 - acc: 0.8675 - 38ms/step
    step 290/313 - loss: 0.4131 - acc: 0.8671 - 38ms/step
    step 300/313 - loss: 0.3185 - acc: 0.8665 - 38ms/step
    step 310/313 - loss: 0.3101 - acc: 0.8662 - 38ms/step
    step 313/313 - loss: 0.2654 - acc: 0.8661 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/3
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.2280 - acc: 0.9195 - 41ms/step
    step 20/40 - loss: 0.2821 - acc: 0.9120 - 35ms/step
    step 30/40 - loss: 0.2366 - acc: 0.9115 - 33ms/step
    step 40/40 - loss: 0.3791 - acc: 0.9096 - 28ms/step
    Eval samples: 7802
    Epoch 5/10
    step  10/313 - loss: 0.2273 - acc: 0.9120 - 51ms/step
    step  20/313 - loss: 0.2184 - acc: 0.9075 - 44ms/step
    step  30/313 - loss: 0.1705 - acc: 0.9085 - 42ms/step
    step  40/313 - loss: 0.2454 - acc: 0.9080 - 40ms/step
    step  50/313 - loss: 0.2804 - acc: 0.9109 - 40ms/step
    step  60/313 - loss: 0.2295 - acc: 0.9094 - 39ms/step
    step  70/313 - loss: 0.2070 - acc: 0.9100 - 39ms/step
    step  80/313 - loss: 0.2257 - acc: 0.9090 - 38ms/step
    step  90/313 - loss: 0.2572 - acc: 0.9091 - 38ms/step
    step 100/313 - loss: 0.2887 - acc: 0.9078 - 38ms/step
    step 110/313 - loss: 0.2616 - acc: 0.9072 - 38ms/step
    step 120/313 - loss: 0.3199 - acc: 0.9058 - 38ms/step
    step 130/313 - loss: 0.1998 - acc: 0.9056 - 38ms/step
    step 140/313 - loss: 0.2293 - acc: 0.9050 - 37ms/step
    step 150/313 - loss: 0.2421 - acc: 0.9040 - 37ms/step
    step 160/313 - loss: 0.3262 - acc: 0.9026 - 37ms/step
    step 170/313 - loss: 0.2710 - acc: 0.9013 - 37ms/step
    step 180/313 - loss: 0.2383 - acc: 0.9012 - 37ms/step
    step 190/313 - loss: 0.3109 - acc: 0.9011 - 37ms/step
    step 200/313 - loss: 0.2651 - acc: 0.9001 - 37ms/step
    step 210/313 - loss: 0.2629 - acc: 0.8995 - 37ms/step
    step 220/313 - loss: 0.2654 - acc: 0.8987 - 37ms/step
    step 230/313 - loss: 0.2675 - acc: 0.8979 - 37ms/step
    step 240/313 - loss: 0.2811 - acc: 0.8974 - 37ms/step
    step 250/313 - loss: 0.2632 - acc: 0.8965 - 37ms/step
    step 260/313 - loss: 0.3469 - acc: 0.8958 - 37ms/step
    step 270/313 - loss: 0.2688 - acc: 0.8954 - 37ms/step
    step 280/313 - loss: 0.2639 - acc: 0.8947 - 37ms/step
    step 290/313 - loss: 0.3070 - acc: 0.8941 - 37ms/step
    step 300/313 - loss: 0.2988 - acc: 0.8939 - 37ms/step
    step 310/313 - loss: 0.3038 - acc: 0.8934 - 37ms/step
    step 313/313 - loss: 0.1511 - acc: 0.8934 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/4
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.1704 - acc: 0.9340 - 46ms/step
    step 20/40 - loss: 0.2443 - acc: 0.9275 - 38ms/step
    step 30/40 - loss: 0.1940 - acc: 0.9272 - 35ms/step
    step 40/40 - loss: 0.5411 - acc: 0.9268 - 29ms/step
    Eval samples: 7802
    Epoch 6/10
    step  10/313 - loss: 0.1581 - acc: 0.9380 - 54ms/step
    step  20/313 - loss: 0.1754 - acc: 0.9335 - 47ms/step
    step  30/313 - loss: 0.2125 - acc: 0.9312 - 45ms/step
    step  40/313 - loss: 0.1296 - acc: 0.9294 - 43ms/step
    step  50/313 - loss: 0.1597 - acc: 0.9275 - 42ms/step
    step  60/313 - loss: 0.1716 - acc: 0.9258 - 42ms/step
    step  70/313 - loss: 0.2207 - acc: 0.9248 - 42ms/step
    step  80/313 - loss: 0.1764 - acc: 0.9261 - 42ms/step
    step  90/313 - loss: 0.1768 - acc: 0.9259 - 41ms/step
    step 100/313 - loss: 0.2047 - acc: 0.9258 - 41ms/step
    step 110/313 - loss: 0.2370 - acc: 0.9243 - 41ms/step
    step 120/313 - loss: 0.2340 - acc: 0.9238 - 41ms/step
    step 130/313 - loss: 0.1979 - acc: 0.9233 - 41ms/step
    step 140/313 - loss: 0.2778 - acc: 0.9223 - 41ms/step
    step 150/313 - loss: 0.2230 - acc: 0.9211 - 41ms/step
    step 160/313 - loss: 0.2481 - acc: 0.9201 - 41ms/step
    step 170/313 - loss: 0.2304 - acc: 0.9192 - 41ms/step
    step 180/313 - loss: 0.1882 - acc: 0.9191 - 40ms/step
    step 190/313 - loss: 0.2794 - acc: 0.9182 - 40ms/step
    step 200/313 - loss: 0.2881 - acc: 0.9172 - 40ms/step
    step 210/313 - loss: 0.2870 - acc: 0.9167 - 40ms/step
    step 220/313 - loss: 0.2170 - acc: 0.9165 - 40ms/step
    step 230/313 - loss: 0.2473 - acc: 0.9158 - 40ms/step
    step 240/313 - loss: 0.1799 - acc: 0.9149 - 40ms/step
    step 250/313 - loss: 0.2647 - acc: 0.9145 - 40ms/step
    step 260/313 - loss: 0.2410 - acc: 0.9142 - 40ms/step
    step 270/313 - loss: 0.2469 - acc: 0.9134 - 40ms/step
    step 280/313 - loss: 0.2352 - acc: 0.9131 - 40ms/step
    step 290/313 - loss: 0.1969 - acc: 0.9126 - 39ms/step
    step 300/313 - loss: 0.2011 - acc: 0.9119 - 39ms/step
    step 310/313 - loss: 0.2766 - acc: 0.9116 - 39ms/step
    step 313/313 - loss: 0.1941 - acc: 0.9113 - 39ms/step
    save checkpoint at /home/aistudio/checkpoints/5
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.1364 - acc: 0.9405 - 41ms/step
    step 20/40 - loss: 0.1778 - acc: 0.9390 - 34ms/step
    step 30/40 - loss: 0.1743 - acc: 0.9380 - 32ms/step
    step 40/40 - loss: 0.5655 - acc: 0.9368 - 28ms/step
    Eval samples: 7802
    Epoch 7/10
    step  10/313 - loss: 0.1356 - acc: 0.9370 - 49ms/step
    step  20/313 - loss: 0.1708 - acc: 0.9410 - 44ms/step
    step  30/313 - loss: 0.1759 - acc: 0.9422 - 41ms/step
    step  40/313 - loss: 0.2352 - acc: 0.9420 - 40ms/step
    step  50/313 - loss: 0.1469 - acc: 0.9404 - 40ms/step
    step  60/313 - loss: 0.1239 - acc: 0.9413 - 40ms/step
    step  70/313 - loss: 0.1251 - acc: 0.9416 - 39ms/step
    step  80/313 - loss: 0.2329 - acc: 0.9407 - 39ms/step
    step  90/313 - loss: 0.1653 - acc: 0.9394 - 39ms/step
    step 100/313 - loss: 0.1765 - acc: 0.9384 - 39ms/step
    step 110/313 - loss: 0.1899 - acc: 0.9376 - 38ms/step
    step 120/313 - loss: 0.2282 - acc: 0.9367 - 38ms/step
    step 130/313 - loss: 0.1671 - acc: 0.9358 - 38ms/step
    step 140/313 - loss: 0.1461 - acc: 0.9350 - 38ms/step
    step 150/313 - loss: 0.1666 - acc: 0.9338 - 38ms/step
    step 160/313 - loss: 0.2081 - acc: 0.9325 - 38ms/step
    step 170/313 - loss: 0.1975 - acc: 0.9314 - 38ms/step
    step 180/313 - loss: 0.2556 - acc: 0.9302 - 38ms/step
    step 190/313 - loss: 0.1919 - acc: 0.9297 - 38ms/step
    step 200/313 - loss: 0.1781 - acc: 0.9294 - 38ms/step
    step 210/313 - loss: 0.2554 - acc: 0.9284 - 38ms/step
    step 220/313 - loss: 0.1822 - acc: 0.9281 - 38ms/step
    step 230/313 - loss: 0.1802 - acc: 0.9276 - 38ms/step
    step 240/313 - loss: 0.1953 - acc: 0.9273 - 38ms/step
    step 250/313 - loss: 0.1916 - acc: 0.9269 - 38ms/step
    step 260/313 - loss: 0.2343 - acc: 0.9263 - 38ms/step
    step 270/313 - loss: 0.2569 - acc: 0.9256 - 38ms/step
    step 280/313 - loss: 0.1868 - acc: 0.9248 - 37ms/step
    step 290/313 - loss: 0.2784 - acc: 0.9243 - 37ms/step
    step 300/313 - loss: 0.2176 - acc: 0.9238 - 37ms/step
    step 310/313 - loss: 0.2597 - acc: 0.9236 - 37ms/step
    step 313/313 - loss: 0.1152 - acc: 0.9237 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/6
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.1167 - acc: 0.9500 - 45ms/step
    step 20/40 - loss: 0.1898 - acc: 0.9425 - 37ms/step
    step 30/40 - loss: 0.1393 - acc: 0.9447 - 35ms/step
    step 40/40 - loss: 0.5776 - acc: 0.9442 - 30ms/step
    Eval samples: 7802
    Epoch 8/10
    step  10/313 - loss: 0.1116 - acc: 0.9545 - 47ms/step
    step  20/313 - loss: 0.1921 - acc: 0.9493 - 41ms/step
    step  30/313 - loss: 0.1522 - acc: 0.9478 - 40ms/step
    step  40/313 - loss: 0.1188 - acc: 0.9480 - 39ms/step
    step  50/313 - loss: 0.1279 - acc: 0.9478 - 38ms/step
    step  60/313 - loss: 0.1761 - acc: 0.9484 - 38ms/step
    step  70/313 - loss: 0.2476 - acc: 0.9473 - 37ms/step
    step  80/313 - loss: 0.1823 - acc: 0.9474 - 37ms/step
    step  90/313 - loss: 0.1562 - acc: 0.9464 - 37ms/step
    step 100/313 - loss: 0.1696 - acc: 0.9451 - 37ms/step
    step 110/313 - loss: 0.1978 - acc: 0.9443 - 37ms/step
    step 120/313 - loss: 0.1863 - acc: 0.9445 - 37ms/step
    step 130/313 - loss: 0.1715 - acc: 0.9432 - 37ms/step
    step 140/313 - loss: 0.1826 - acc: 0.9425 - 37ms/step
    step 150/313 - loss: 0.2051 - acc: 0.9412 - 37ms/step
    step 160/313 - loss: 0.1343 - acc: 0.9406 - 37ms/step
    step 170/313 - loss: 0.1123 - acc: 0.9401 - 37ms/step
    step 180/313 - loss: 0.1128 - acc: 0.9400 - 37ms/step
    step 190/313 - loss: 0.2686 - acc: 0.9394 - 37ms/step
    step 200/313 - loss: 0.0916 - acc: 0.9395 - 37ms/step
    step 210/313 - loss: 0.1566 - acc: 0.9389 - 37ms/step
    step 220/313 - loss: 0.2082 - acc: 0.9380 - 37ms/step
    step 230/313 - loss: 0.1506 - acc: 0.9375 - 37ms/step
    step 240/313 - loss: 0.2126 - acc: 0.9372 - 37ms/step
    step 250/313 - loss: 0.2126 - acc: 0.9368 - 37ms/step
    step 260/313 - loss: 0.1755 - acc: 0.9362 - 37ms/step
    step 270/313 - loss: 0.2091 - acc: 0.9358 - 37ms/step
    step 280/313 - loss: 0.1718 - acc: 0.9357 - 37ms/step
    step 290/313 - loss: 0.2018 - acc: 0.9353 - 37ms/step
    step 300/313 - loss: 0.2005 - acc: 0.9347 - 37ms/step
    step 310/313 - loss: 0.2185 - acc: 0.9343 - 36ms/step
    step 313/313 - loss: 0.5939 - acc: 0.9341 - 36ms/step
    save checkpoint at /home/aistudio/checkpoints/7
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.0999 - acc: 0.9615 - 42ms/step
    step 20/40 - loss: 0.1513 - acc: 0.9550 - 35ms/step
    step 30/40 - loss: 0.1454 - acc: 0.9533 - 33ms/step
    step 40/40 - loss: 0.2175 - acc: 0.9535 - 28ms/step
    Eval samples: 7802
    Epoch 9/10
    step  10/313 - loss: 0.1940 - acc: 0.9500 - 48ms/step
    step  20/313 - loss: 0.0704 - acc: 0.9540 - 42ms/step
    step  30/313 - loss: 0.1606 - acc: 0.9525 - 40ms/step
    step  40/313 - loss: 0.1312 - acc: 0.9520 - 39ms/step
    step  50/313 - loss: 0.1390 - acc: 0.9511 - 39ms/step
    step  60/313 - loss: 0.0732 - acc: 0.9513 - 38ms/step
    step  70/313 - loss: 0.1018 - acc: 0.9504 - 38ms/step
    step  80/313 - loss: 0.1107 - acc: 0.9496 - 38ms/step
    step  90/313 - loss: 0.1057 - acc: 0.9498 - 38ms/step
    step 100/313 - loss: 0.1056 - acc: 0.9495 - 38ms/step
    step 110/313 - loss: 0.1293 - acc: 0.9497 - 37ms/step
    step 120/313 - loss: 0.1279 - acc: 0.9486 - 37ms/step
    step 130/313 - loss: 0.1521 - acc: 0.9481 - 37ms/step
    step 140/313 - loss: 0.1171 - acc: 0.9473 - 37ms/step
    step 150/313 - loss: 0.1414 - acc: 0.9469 - 37ms/step
    step 160/313 - loss: 0.1528 - acc: 0.9463 - 37ms/step
    step 170/313 - loss: 0.2263 - acc: 0.9453 - 37ms/step
    step 180/313 - loss: 0.1872 - acc: 0.9444 - 37ms/step
    step 190/313 - loss: 0.1718 - acc: 0.9438 - 37ms/step
    step 200/313 - loss: 0.2349 - acc: 0.9435 - 37ms/step
    step 210/313 - loss: 0.1463 - acc: 0.9431 - 37ms/step
    step 220/313 - loss: 0.1457 - acc: 0.9427 - 37ms/step
    step 230/313 - loss: 0.1101 - acc: 0.9426 - 38ms/step
    step 240/313 - loss: 0.1558 - acc: 0.9421 - 38ms/step
    step 250/313 - loss: 0.1483 - acc: 0.9416 - 38ms/step
    step 260/313 - loss: 0.1467 - acc: 0.9416 - 38ms/step
    step 270/313 - loss: 0.1594 - acc: 0.9413 - 38ms/step
    step 280/313 - loss: 0.1795 - acc: 0.9411 - 38ms/step
    step 290/313 - loss: 0.1909 - acc: 0.9408 - 37ms/step
    step 300/313 - loss: 0.1435 - acc: 0.9405 - 37ms/step
    step 310/313 - loss: 0.1683 - acc: 0.9405 - 37ms/step
    step 313/313 - loss: 0.3363 - acc: 0.9405 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/8
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.0799 - acc: 0.9670 - 43ms/step
    step 20/40 - loss: 0.1404 - acc: 0.9595 - 36ms/step
    step 30/40 - loss: 0.1395 - acc: 0.9573 - 34ms/step
    step 40/40 - loss: 0.2501 - acc: 0.9578 - 29ms/step
    Eval samples: 7802
    Epoch 10/10
    step  10/313 - loss: 0.1660 - acc: 0.9520 - 49ms/step
    step  20/313 - loss: 0.1313 - acc: 0.9535 - 43ms/step
    step  30/313 - loss: 0.1238 - acc: 0.9538 - 41ms/step
    step  40/313 - loss: 0.1803 - acc: 0.9527 - 40ms/step
    step  50/313 - loss: 0.1018 - acc: 0.9531 - 40ms/step
    step  60/313 - loss: 0.1955 - acc: 0.9532 - 40ms/step
    step  70/313 - loss: 0.1262 - acc: 0.9519 - 39ms/step
    step  80/313 - loss: 0.1270 - acc: 0.9517 - 39ms/step
    step  90/313 - loss: 0.1346 - acc: 0.9514 - 38ms/step
    step 100/313 - loss: 0.0909 - acc: 0.9516 - 38ms/step
    step 110/313 - loss: 0.1378 - acc: 0.9520 - 38ms/step
    step 120/313 - loss: 0.1277 - acc: 0.9520 - 38ms/step
    step 130/313 - loss: 0.2406 - acc: 0.9515 - 38ms/step
    step 140/313 - loss: 0.1314 - acc: 0.9515 - 38ms/step
    step 150/313 - loss: 0.1605 - acc: 0.9508 - 38ms/step
    step 160/313 - loss: 0.1643 - acc: 0.9501 - 38ms/step
    step 170/313 - loss: 0.1569 - acc: 0.9497 - 38ms/step
    step 180/313 - loss: 0.1599 - acc: 0.9489 - 38ms/step
    step 190/313 - loss: 0.1829 - acc: 0.9487 - 38ms/step
    step 200/313 - loss: 0.1165 - acc: 0.9484 - 38ms/step
    step 210/313 - loss: 0.1941 - acc: 0.9475 - 38ms/step
    step 220/313 - loss: 0.1974 - acc: 0.9469 - 38ms/step
    step 230/313 - loss: 0.1035 - acc: 0.9469 - 38ms/step
    step 240/313 - loss: 0.1260 - acc: 0.9466 - 38ms/step
    step 250/313 - loss: 0.1233 - acc: 0.9469 - 38ms/step
    step 260/313 - loss: 0.0908 - acc: 0.9469 - 38ms/step
    step 270/313 - loss: 0.1533 - acc: 0.9467 - 38ms/step
    step 280/313 - loss: 0.1544 - acc: 0.9464 - 38ms/step
    step 290/313 - loss: 0.1601 - acc: 0.9464 - 38ms/step
    step 300/313 - loss: 0.1387 - acc: 0.9464 - 38ms/step
    step 310/313 - loss: 0.1235 - acc: 0.9462 - 37ms/step
    step 313/313 - loss: 0.0436 - acc: 0.9461 - 37ms/step
    save checkpoint at /home/aistudio/checkpoints/9
    Eval begin...
    The loss value printed in the log is the current batch, and the metric is the average value of previous step.
    step 10/40 - loss: 0.0886 - acc: 0.9710 - 42ms/step
    step 20/40 - loss: 0.1208 - acc: 0.9670 - 36ms/step
    step 30/40 - loss: 0.1234 - acc: 0.9665 - 34ms/step
    step 40/40 - loss: 0.1445 - acc: 0.9646 - 29ms/step
    Eval samples: 7802
    save checkpoint at /home/aistudio/checkpoints/final


## 2.5 æ¨¡å‹è¯„ä¼°

+ æ¨¡å‹è¯„ä¼°å®ç”¨paddleé«˜çº§APIè‡ªå¸¦çš„evaluateå‡½æ•°å°±å¯ä»¥å®ç°è¯„ä¼°ï¼Œè¯„ä¼°çš„æ–¹å¼æ˜¯accuracy


```python
# Finally tests model.
results = model.evaluate(dev_loader)
print("Finally test acc: %.5f" % results['acc'])
```

    Eval begin...
    step  10/122 - loss: 0.4908 - acc: 0.7703 - 24ms/step
    step  20/122 - loss: 0.4314 - acc: 0.7695 - 18ms/step
    step  30/122 - loss: 0.4691 - acc: 0.7760 - 16ms/step
    step  40/122 - loss: 0.5401 - acc: 0.7867 - 15ms/step
    step  50/122 - loss: 0.5670 - acc: 0.7866 - 15ms/step
    step  60/122 - loss: 0.4395 - acc: 0.7878 - 14ms/step
    step  70/122 - loss: 0.5325 - acc: 0.7895 - 14ms/step
    step  80/122 - loss: 0.5728 - acc: 0.7879 - 14ms/step
    step  90/122 - loss: 0.4095 - acc: 0.7905 - 13ms/step
    step 100/122 - loss: 0.4865 - acc: 0.7930 - 13ms/step
    step 110/122 - loss: 0.4447 - acc: 0.7923 - 13ms/step
    step 120/122 - loss: 0.5081 - acc: 0.7926 - 13ms/step
    step 122/122 - loss: 0.5163 - acc: 0.7918 - 13ms/step
    Eval samples: 7802
    Finally test acc: 0.79185


## 2.6 æ¨¡å‹é¢„æµ‹


```python
def predict(model, data, label_map, batch_size=1, pad_token_id=0):
    """
    é¢„æµ‹æ•°æ®çš„æ ‡ç­¾
    """
    # æŠŠæ•°æ®åˆ†æˆè‹¥å¹²ä¸ªbatch
    batches = [
        data[idx:idx + batch_size] for idx in range(0, len(data), batch_size)
    ]

    batchify_fn = lambda samples, fn=Tuple(
        Pad(axis=0, pad_val=pad_token_id),  # query_ids
        Pad(axis=0, pad_val=pad_token_id),  # title_ids
        Stack(dtype="int64"),  # query_seq_lens
        Stack(dtype="int64"),  # title_seq_lens
    ): [data for data in fn(samples)]

    results = []
    model.eval()
    for batch in batches:
        query_ids, title_ids, query_seq_lens, title_seq_lens = batchify_fn(
            batch)
        query_ids = paddle.to_tensor(query_ids)
        title_ids = paddle.to_tensor(title_ids)
        query_seq_lens = paddle.to_tensor(query_seq_lens)
        title_seq_lens = paddle.to_tensor(title_seq_lens)
        logits = model(query_ids, title_ids, query_seq_lens, title_seq_lens)
        probs = F.softmax(logits, axis=1)
        idx = paddle.argmax(probs, axis=1).numpy()
        idx = idx.tolist()
        labels = [label_map[i] for i in idx]
        results.extend(labels)
    return results
```

+ å»ºç«‹idåˆ°ç±»åˆ«çš„æ˜ å°„ï¼Œæœ¬å®éªŒæœ‰ä¸¤ä¸ªç±»åˆ«ï¼Œç¼–ç ä¸º0ï¼Œ1ã€‚0ç±»ä¸ºdissimilarï¼Œ1ç±»ä¸ºsimilar


```python
label_map = {0: '0', 1: '1'}
```

+ åŠ è½½è®­ç»ƒçš„æ¨¡å‹çš„å‚æ•°


```python
# å®ä¾‹åŒ–SimNetæ¨¡å‹
model = SimNet(network=network, vocab_size=len(vocab), num_classes=len(label_map))
params_path='./checkpoints/final.pdparams'
#  åŠ è½½æ¨¡å‹å‚æ•°
state_dict = paddle.load(params_path)
model.set_dict(state_dict)
print("Loaded parameters from %s" % params_path)
```

    Loaded parameters from ./checkpoints/final.pdparams



```python
def preprocess_prediction_data(data, tokenizer):
    examples = []
    for query, title in data:
        query_ids = tokenizer.encode(query)
        title_ids = tokenizer.encode(title)
        examples.append([query_ids, title_ids, len(query_ids), len(title_ids)])
    return examples
```


```python
# Firstly pre-processing prediction data  and then do predict.
# å°†dataæ›¿æ¢ä¸ºæˆ‘ä»¬ç»™å‡ºçš„æµ‹è¯•é›†test_forstu.tsvï¼Œè¾“å‡ºé¢„æµ‹ç»“æœresult.tsvå¹¶æäº¤ã€‚
import pandas as pd
data = [
        ['ä¸–ç•Œä¸Šä»€ä¹ˆä¸œè¥¿æœ€å°', 'ä¸–ç•Œä¸Šä»€ä¹ˆä¸œè¥¿æœ€å°ï¼Ÿ'],
        ['å…‰çœ¼ç›å¤§å°±å¥½çœ‹å—', 'çœ¼ç›å¥½çœ‹å—ï¼Ÿ'],
        ['å°èŒèšªæ‰¾å¦ˆå¦ˆæ€ä¹ˆæ ·', 'å°èŒèšªæ‰¾å¦ˆå¦ˆæ˜¯è°ç”»çš„'],
    ]
data = pd.read_csv('test_forstu.tsv', sep='\t')
data = data.values.tolist()
examples = preprocess_prediction_data(data, tokenizer)

results = predict(
        model,
        examples,
        label_map=label_map,
        batch_size=batch_size,
        pad_token_id=vocab.token_to_idx.get('[PAD]', 0))

list1 = []
list2 = []
list3 = []

for idx, text in enumerate(data):
    list1.append(text[0])
    list2.append(text[1])
    list3.append(results[idx])
dict= {'text_a': list1, 'text_b': list2, 'label': list3}
datas = pd.DataFrame(dict)
datas.to_csv('result.csv',encoding='UTF-8',sep="\t")
    #print('Data: {} \t{}\t Label: {}'.format(text[0],text[1], results[idx]))
```


```python

```
